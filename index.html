<!DOCTYPE html>
<html>
<head>
<title>Paper collected by Wang</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<style type="text/css">


/* BODY
=============================================================================*/

body {
  font-family: Helvetica, arial, freesans, clean, sans-serif;
  font-size: 14px;
  line-height: 1.6;
  color: #333;
  background-color: #fff;
  padding: 20px;
  max-width: 960px;
  margin: 0 auto;
}

span#pid {
  color:red;
  
}
span#filename{
  font-style: oblique;
}

span#title{
  font-family: Times New Roman, freesans, clean, sans-serif;
  font-style: italic;
  font-size: 20px;
  border:1px solid #B50;
}
span#abs{
  font-family: Times New Roman, freesans, clean, sans-serif;
  font-style: oblique;
  font-size: 18px;
}
</style>
</head>
<body>

</p></br></br><div id='section'>Paperid: <span id='pid'>1, <a href='https://arxiv.org/pdf/2512.15693.pdf' target='_blank'>https://arxiv.org/pdf/2512.15693.pdf</a></span>   <span><a href='https://github.com/JoeLeelyf/Skyra' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yifei Li, Wenzhao Zheng, Yanran Zhang, Runze Sun, Yu Zheng, Lei Chen, Jie Zhou, Jiwen Lu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.15693">Skyra: AI-Generated Video Detection via Grounded Artifact Reasoning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The misuse of AI-driven video generation technologies has raised serious social concerns, highlighting the urgent need for reliable AI-generated video detectors. However, most existing methods are limited to binary classification and lack the necessary explanations for human interpretation. In this paper, we present Skyra, a specialized multimodal large language model (MLLM) that identifies human-perceivable visual artifacts in AI-generated videos and leverages them as grounded evidence for both detection and explanation. To support this objective, we construct ViF-CoT-4K for Supervised Fine-Tuning (SFT), which represents the first large-scale AI-generated video artifact dataset with fine-grained human annotations. We then develop a two-stage training strategy that systematically enhances our model's spatio-temporal artifact perception, explanation capability, and detection accuracy. To comprehensively evaluate Skyra, we introduce ViF-Bench, a benchmark comprising 3K high-quality samples generated by over ten state-of-the-art video generators. Extensive experiments demonstrate that Skyra surpasses existing methods across multiple benchmarks, while our evaluation yields valuable insights for advancing explainable AI-generated video detection.
<div id='section'>Paperid: <span id='pid'>2, <a href='https://arxiv.org/pdf/2512.13281.pdf' target='_blank'>https://arxiv.org/pdf/2512.13281.pdf</a></span>   <span><a href='https://github.com/video-reality-test/video-reality-test,' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/video-reality-test/video-reality-test' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiaqi Wang, Weijia Wu, Yi Zhan, Rui Zhao, Ming Hu, James Cheng, Wei Liu, Philip Torr, Kevin Qinghong Lin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.13281">Video Reality Test: Can AI-Generated ASMR Videos fool VLMs and Humans?</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in video generation have produced vivid content that are often indistinguishable from real videos, making AI-generated video detection an emerging societal challenge. Prior AIGC detection benchmarks mostly evaluate video without audio, target broad narrative domains, and focus on classification solely. Yet it remains unclear whether state-of-the-art video generation models can produce immersive, audio-paired videos that reliably deceive humans and VLMs. To this end, we introduce Video Reality Test, an ASMR-sourced video benchmark suite for testing perceptual realism under tight audio-visual coupling, featuring the following dimensions: (i) Immersive ASMR video-audio sources. Built on carefully curated real ASMR videos, the benchmark targets fine-grained action-object interactions with diversity across objects, actions, and backgrounds. (ii) Peer-Review evaluation. An adversarial creator-reviewer protocol where video generation models act as creators aiming to fool reviewers, while VLMs serve as reviewers seeking to identify fakeness. Our experimental findings show: The best creator Veo3.1-Fast even fools most VLMs: the strongest reviewer (Gemini 2.5-Pro) achieves only 56% accuracy (random 50%), far below that of human experts (81.25%). Adding audio improves real-fake discrimination, yet superficial cues such as watermarks can still significantly mislead models. These findings delineate the current boundary of video generation realism and expose limitations of VLMs in perceptual fidelity and audio-visual consistency. Our code is available at https://github.com/video-reality-test/video-reality-test.
<div id='section'>Paperid: <span id='pid'>3, <a href='https://arxiv.org/pdf/2512.12982.pdf' target='_blank'>https://arxiv.org/pdf/2512.12982.pdf</a></span>   <span><a href='https://github.com/UltraCapture/GAPL' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ziheng Qin, Yuheng Ji, Renshuai Tao, Yuxuan Tian, Yuyang Liu, Yipu Wang, Xiaolong Zheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.12982">Scaling Up AI-Generated Image Detection via Generator-Aware Prototypes</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The pursuit of a universal AI-generated image (AIGI) detector often relies on aggregating data from numerous generators to improve generalization. However, this paper identifies a paradoxical phenomenon we term the Benefit then Conflict dilemma, where detector performance stagnates and eventually degrades as source diversity expands. Our systematic analysis, diagnoses this failure by identifying two core issues: severe data-level heterogeneity, which causes the feature distributions of real and synthetic images to increasingly overlap, and a critical model-level bottleneck from fixed, pretrained encoders that cannot adapt to the rising complexity. To address these challenges, we propose Generator-Aware Prototype Learning (GAPL), a framework that constrain representation with a structured learning paradigm. GAPL learns a compact set of canonical forgery prototypes to create a unified, low-variance feature space, effectively countering data heterogeneity.To resolve the model bottleneck, it employs a two-stage training scheme with Low-Rank Adaptation, enhancing its discriminative power while preserving valuable pretrained knowledge. This approach establishes a more robust and generalizable decision boundary. Through extensive experiments, we demonstrate that GAPL achieves state-of-the-art performance, showing superior detection accuracy across a wide variety of GAN and diffusion-based generators. Code is available at https://github.com/UltraCapture/GAPL
<div id='section'>Paperid: <span id='pid'>4, <a href='https://arxiv.org/pdf/2512.04969.pdf' target='_blank'>https://arxiv.org/pdf/2512.04969.pdf</a></span>   <span><a href='https://github.com/nahyeonkaty/mold' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>NaHyeon Park, Kunhee Kim, Junsuk Choe, Hyunjung Shim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.04969">Rethinking the Use of Vision Transformers for AI-Generated Image Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Rich feature representations derived from CLIP-ViT have been widely utilized in AI-generated image detection. While most existing methods primarily leverage features from the final layer, we systematically analyze the contributions of layer-wise features to this task. Our study reveals that earlier layers provide more localized and generalizable features, often surpassing the performance of final-layer features in detection tasks. Moreover, we find that different layers capture distinct aspects of the data, each contributing uniquely to AI-generated image detection. Motivated by these findings, we introduce a novel adaptive method, termed MoLD, which dynamically integrates features from multiple ViT layers using a gating-based mechanism. Extensive experiments on both GAN- and diffusion-generated images demonstrate that MoLD significantly improves detection performance, enhances generalization across diverse generative models, and exhibits robustness in real-world scenarios. Finally, we illustrate the scalability and versatility of our approach by successfully applying it to other pre-trained ViTs, such as DINOv2.
<div id='section'>Paperid: <span id='pid'>5, <a href='https://arxiv.org/pdf/2511.12511.pdf' target='_blank'>https://arxiv.org/pdf/2511.12511.pdf</a></span>   <span><a href='https://github.com/JiaLiangShen/Dino-Detect-for-blur-robust-AIGC-Detection' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jialiang Shen, Jiyang Zheng, Yunqi Xue, Huajie Chen, Yu Yao, Hui Kang, Ruiqi Liu, Helin Gong, Yang Yang, Dadong Wang, Tongliang Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.12511">DINO-Detect: A Simple yet Effective Framework for Blur-Robust AI-Generated Image Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With growing concerns over image authenticity and digital safety, the field of AI-generated image (AIGI) detection has progressed rapidly. Yet, most AIGI detectors still struggle under real-world degradations, particularly motion blur, which frequently occurs in handheld photography, fast motion, and compressed video. Such blur distorts fine textures and suppresses high-frequency artifacts, causing severe performance drops in real-world settings. We address this limitation with a blur-robust AIGI detection framework based on teacher-student knowledge distillation. A high-capacity teacher (DINOv3), trained on clean (i.e., sharp) images, provides stable and semantically rich representations that serve as a reference for learning. By freezing the teacher to maintain its generalization ability, we distill its feature and logit responses from sharp images to a student trained on blurred counterparts, enabling the student to produce consistent representations under motion degradation. Extensive experiments benchmarks show that our method achieves state-of-the-art performance under both motion-blurred and clean conditions, demonstrating improved generalization and real-world applicability. Source codes will be released at: https://github.com/JiaLiangShen/Dino-Detect-for-blur-robust-AIGC-Detection.
<div id='section'>Paperid: <span id='pid'>6, <a href='https://arxiv.org/pdf/2511.12363.pdf' target='_blank'>https://arxiv.org/pdf/2511.12363.pdf</a></span>   <span><a href='https://github.com/RewardBench/XAIGID-RewardBench' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Michael Yang, Shijian Deng, William T. Doan, Kai Wang, Tianyu Yang, Harsh Singh, Yapeng Tian
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.12363">Explainable AI-Generated Image Detection RewardBench</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Conventional, classification-based AI-generated image detection methods cannot explain why an image is considered real or AI-generated in a way a human expert would, which reduces the trustworthiness and persuasiveness of these detection tools for real-world applications. Leveraging Multimodal Large Language Models (MLLMs) has recently become a trending solution to this issue. Further, to evaluate the quality of generated explanations, a common approach is to adopt an "MLLM as a judge" methodology to evaluate explanations generated by other MLLMs. However, how well those MLLMs perform when judging explanations for AI-generated image detection generated by themselves or other MLLMs has not been well studied. We therefore propose \textbf{XAIGID-RewardBench}, the first benchmark designed to evaluate the ability of current MLLMs to judge the quality of explanations about whether an image is real or AI-generated. The benchmark consists of approximately 3,000 annotated triplets sourced from various image generation models and MLLMs as policy models (detectors) to assess the capabilities of current MLLMs as reward models (judges). Our results show that the current best reward model scored 88.76\% on this benchmark (while human inter-annotator agreement reaches 98.30\%), demonstrating that a visible gap remains between the reasoning abilities of today's MLLMs and human-level performance. In addition, we provide an analysis of common pitfalls that these models frequently encounter. Code and benchmark are available at https://github.com/RewardBench/XAIGID-RewardBench.
<div id='section'>Paperid: <span id='pid'>7, <a href='https://arxiv.org/pdf/2510.14230.pdf' target='_blank'>https://arxiv.org/pdf/2510.14230.pdf</a></span>   <span><a href='https://github.com/hongsong-wang/LOTA' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/hongsong-wang/LOTA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hongsong Wang, Renxi Cheng, Yang Zhang, Chaolei Han, Jie Gui
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.14230">LOTA: Bit-Planes Guided AI-Generated Image Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The rapid advancement of GAN and Diffusion models makes it more difficult to distinguish AI-generated images from real ones. Recent studies often use image-based reconstruction errors as an important feature for determining whether an image is AI-generated. However, these approaches typically incur high computational costs and also fail to capture intrinsic noisy features present in the raw images. To solve these problems, we innovatively refine error extraction by using bit-plane-based image processing, as lower bit planes indeed represent noise patterns in images. We introduce an effective bit-planes guided noisy image generation and exploit various image normalization strategies, including scaling and thresholding. Then, to amplify the noise signal for easier AI-generated image detection, we design a maximum gradient patch selection that applies multi-directional gradients to compute the noise score and selects the region with the highest score. Finally, we propose a lightweight and effective classification head and explore two different structures: noise-based classifier and noise-guided classifier. Extensive experiments on the GenImage benchmark demonstrate the outstanding performance of our method, which achieves an average accuracy of \textbf{98.9\%} (\textbf{11.9}\%~$\uparrow$) and shows excellent cross-generator generalization capability. Particularly, our method achieves an accuracy of over 98.2\% from GAN to Diffusion and over 99.2\% from Diffusion to GAN. Moreover, it performs error extraction at the millisecond level, nearly a hundred times faster than existing methods. The code is at https://github.com/hongsong-wang/LOTA.
<div id='section'>Paperid: <span id='pid'>8, <a href='https://arxiv.org/pdf/2510.08073.pdf' target='_blank'>https://arxiv.org/pdf/2510.08073.pdf</a></span>   <span><a href='https://github.com/ZSHsh98/NSG-VD' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Shuhai Zhang, ZiHao Lian, Jiahao Yang, Daiyuan Li, Guoxuan Pang, Feng Liu, Bo Han, Shutao Li, Mingkui Tan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.08073">Physics-Driven Spatiotemporal Modeling for AI-Generated Video Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>AI-generated videos have achieved near-perfect visual realism (e.g., Sora), urgently necessitating reliable detection mechanisms. However, detecting such videos faces significant challenges in modeling high-dimensional spatiotemporal dynamics and identifying subtle anomalies that violate physical laws. In this paper, we propose a physics-driven AI-generated video detection paradigm based on probability flow conservation principles. Specifically, we propose a statistic called Normalized Spatiotemporal Gradient (NSG), which quantifies the ratio of spatial probability gradients to temporal density changes, explicitly capturing deviations from natural video dynamics. Leveraging pre-trained diffusion models, we develop an NSG estimator through spatial gradients approximation and motion-aware temporal modeling without complex motion decomposition while preserving physical constraints. Building on this, we propose an NSG-based video detection method (NSG-VD) that computes the Maximum Mean Discrepancy (MMD) between NSG features of the test and real videos as a detection metric. Last, we derive an upper bound of NSG feature distances between real and generated videos, proving that generated videos exhibit amplified discrepancies due to distributional shifts. Extensive experiments confirm that NSG-VD outperforms state-of-the-art baselines by 16.00% in Recall and 10.75% in F1-Score, validating the superior performance of NSG-VD. The source code is available at https://github.com/ZSHsh98/NSG-VD.
<div id='section'>Paperid: <span id='pid'>9, <a href='https://arxiv.org/pdf/2508.00701.pdf' target='_blank'>https://arxiv.org/pdf/2508.00701.pdf</a></span>   <span><a href='https://github.com/Zig-HS/D3' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Chende Zheng, Ruiqi suo, Chenhao Lin, Zhengyu Zhao, Le Yang, Shuai Liu, Minghui Yang, Cong Wang, Chao Shen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.00701">D3: Training-Free AI-Generated Video Detection Using Second-Order Features</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The evolution of video generation techniques, such as Sora, has made it increasingly easy to produce high-fidelity AI-generated videos, raising public concern over the dissemination of synthetic content. However, existing detection methodologies remain limited by their insufficient exploration of temporal artifacts in synthetic videos. To bridge this gap, we establish a theoretical framework through second-order dynamical analysis under Newtonian mechanics, subsequently extending the Second-order Central Difference features tailored for temporal artifact detection. Building on this theoretical foundation, we reveal a fundamental divergence in second-order feature distributions between real and AI-generated videos. Concretely, we propose Detection by Difference of Differences (D3), a novel training-free detection method that leverages the above second-order temporal discrepancies. We validate the superiority of our D3 on 4 open-source datasets (Gen-Video, VideoPhy, EvalCrafter, VidProM), 40 subsets in total. For example, on GenVideo, D3 outperforms the previous best method by 10.39% (absolute) mean Average Precision. Additional experiments on time cost and post-processing operations demonstrate D3's exceptional computational efficiency and strong robust performance. Our code is available at https://github.com/Zig-HS/D3.
<div id='section'>Paperid: <span id='pid'>10, <a href='https://arxiv.org/pdf/2507.05162.pdf' target='_blank'>https://arxiv.org/pdf/2507.05162.pdf</a></span>   <span><a href='https://github.com/nchivar/LAID' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Nicholas Chivaran, Jianbing Ni
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.05162">LAID: Lightweight AI-Generated Image Detection in Spatial and Spectral Domains</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The recent proliferation of photorealistic AI-generated images (AIGI) has raised urgent concerns about their potential misuse, particularly on social media platforms. Current state-of-the-art AIGI detection methods typically rely on large, deep neural architectures, creating significant computational barriers to real-time, large-scale deployment on platforms like social media. To challenge this reliance on computationally intensive models, we introduce LAID, the first framework -- to our knowledge -- that benchmarks and evaluates the detection performance and efficiency of off-the-shelf lightweight neural networks. In this framework, we comprehensively train and evaluate selected models on a representative subset of the GenImage dataset across spatial, spectral, and fusion image domains. Our results demonstrate that lightweight models can achieve competitive accuracy, even under adversarial conditions, while incurring substantially lower memory and computation costs compared to current state-of-the-art methods. This study offers valuable insight into the trade-off between efficiency and performance in AIGI detection and lays a foundation for the development of practical, scalable, and trustworthy detection systems. The source code of LAID can be found at: https://github.com/nchivar/LAID.
<div id='section'>Paperid: <span id='pid'>11, <a href='https://arxiv.org/pdf/2507.02398.pdf' target='_blank'>https://arxiv.org/pdf/2507.02398.pdf</a></span>   <span><a href='https://github.com/rama0126/PwTF-DVD' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Taehoon Kim, Jongwook Choi, Yonghyun Jeong, Haeun Noh, Jaejun Yoo, Seungryul Baek, Jongwon Choi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.02398">Beyond Spatial Frequency: Pixel-wise Temporal Frequency-based Deepfake Video Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce a deepfake video detection approach that exploits pixel-wise temporal inconsistencies, which traditional spatial frequency-based detectors often overlook. Traditional detectors represent temporal information merely by stacking spatial frequency spectra across frames, resulting in the failure to detect temporal artifacts in the pixel plane. Our approach performs a 1D Fourier transform on the time axis for each pixel, extracting features highly sensitive to temporal inconsistencies, especially in areas prone to unnatural movements. To precisely locate regions containing the temporal artifacts, we introduce an attention proposal module trained in an end-to-end manner. Additionally, our joint transformer module effectively integrates pixel-wise temporal frequency features with spatio-temporal context features, expanding the range of detectable forgery artifacts. Our framework represents a significant advancement in deepfake video detection, providing robust performance across diverse and challenging detection scenarios.
<div id='section'>Paperid: <span id='pid'>12, <a href='https://arxiv.org/pdf/2506.10975.pdf' target='_blank'>https://arxiv.org/pdf/2506.10975.pdf</a></span>   <span><a href='https://chen-wl20.github.io/GenWorld' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Weiliang Chen, Wenzhao Zheng, Yu Zheng, Lei Chen, Jie Zhou, Jiwen Lu, Yueqi Duan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.10975">GenWorld: Towards Detecting AI-generated Real-world Simulation Videos</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The flourishing of video generation technologies has endangered the credibility of real-world information and intensified the demand for AI-generated video detectors. Despite some progress, the lack of high-quality real-world datasets hinders the development of trustworthy detectors. In this paper, we propose GenWorld, a large-scale, high-quality, and real-world simulation dataset for AI-generated video detection. GenWorld features the following characteristics: (1) Real-world Simulation: GenWorld focuses on videos that replicate real-world scenarios, which have a significant impact due to their realism and potential influence; (2) High Quality: GenWorld employs multiple state-of-the-art video generation models to provide realistic and high-quality forged videos; (3) Cross-prompt Diversity: GenWorld includes videos generated from diverse generators and various prompt modalities (e.g., text, image, video), offering the potential to learn more generalizable forensic features. We analyze existing methods and find they fail to detect high-quality videos generated by world models (i.e., Cosmos), revealing potential drawbacks of ignoring real-world clues. To address this, we propose a simple yet effective model, SpannDetector, to leverage multi-view consistency as a strong criterion for real-world AI-generated video detection. Experiments show that our method achieves superior results, highlighting a promising direction for explainable AI-generated video detection based on physical plausibility. We believe that GenWorld will advance the field of AI-generated video detection. Project Page: https://chen-wl20.github.io/GenWorld
<div id='section'>Paperid: <span id='pid'>13, <a href='https://arxiv.org/pdf/2506.03007.pdf' target='_blank'>https://arxiv.org/pdf/2506.03007.pdf</a></span>   <span><a href='https://github.com/IntMeGroup/DFBench' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiarui Wang, Huiyu Duan, Juntong Wang, Ziheng Jia, Woo Yi Yang, Xiaorong Zhu, Yu Zhao, Jiaying Qian, Yuke Xing, Guangtao Zhai, Xiongkuo Min
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.03007">DFBench: Benchmarking Deepfake Image Detection Capability of Large Multimodal Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the rapid advancement of generative models, the realism of AI-generated images has significantly improved, posing critical challenges for verifying digital content authenticity. Current deepfake detection methods often depend on datasets with limited generation models and content diversity that fail to keep pace with the evolving complexity and increasing realism of the AI-generated content. Large multimodal models (LMMs), widely adopted in various vision tasks, have demonstrated strong zero-shot capabilities, yet their potential in deepfake detection remains largely unexplored. To bridge this gap, we present \textbf{DFBench}, a large-scale DeepFake Benchmark featuring (i) broad diversity, including 540,000 images across real, AI-edited, and AI-generated content, (ii) latest model, the fake images are generated by 12 state-of-the-art generation models, and (iii) bidirectional benchmarking and evaluating for both the detection accuracy of deepfake detectors and the evasion capability of generative models. Based on DFBench, we propose \textbf{MoA-DF}, Mixture of Agents for DeepFake detection, leveraging a combined probability strategy from multiple LMMs. MoA-DF achieves state-of-the-art performance, further proving the effectiveness of leveraging LMMs for deepfake detection. Database and codes are publicly available at https://github.com/IntMeGroup/DFBench.
<div id='section'>Paperid: <span id='pid'>14, <a href='https://arxiv.org/pdf/2505.15217.pdf' target='_blank'>https://arxiv.org/pdf/2505.15217.pdf</a></span>   <span><a href='https://github.com/Ant0ny44/InfoFD' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Haotian Qin, Dongliang Chang, Yueying Gao, Bingyao Yu, Lei Chen, Zhanyu Ma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.15217">Multimodal Conditional Information Bottleneck for Generalizable AI-Generated Image Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Although existing CLIP-based methods for detecting AI-generated images have achieved promising results, they are still limited by severe feature redundancy, which hinders their generalization ability. To address this issue, incorporating an information bottleneck network into the task presents a straightforward solution. However, relying solely on image-corresponding prompts results in suboptimal performance due to the inherent diversity of prompts. In this paper, we propose a multimodal conditional bottleneck network to reduce feature redundancy while enhancing the discriminative power of features extracted by CLIP, thereby improving the model's generalization ability. We begin with a semantic analysis experiment, where we observe that arbitrary text features exhibit lower cosine similarity with real image features than with fake image features in the CLIP feature space, a phenomenon we refer to as "bias". Therefore, we introduce InfoFD, a text-guided AI-generated image detection framework. InfoFD consists of two key components: the Text-Guided Conditional Information Bottleneck (TGCIB) and Dynamic Text Orthogonalization (DTO). TGCIB improves the generalizability of learned representations by conditioning on both text and class modalities. DTO dynamically updates weighted text features, preserving semantic information while leveraging the global "bias". Our model achieves exceptional generalization performance on the GenImage dataset and latest generative models. Our code is available at https://github.com/Ant0ny44/InfoFD.
<div id='section'>Paperid: <span id='pid'>15, <a href='https://arxiv.org/pdf/2505.11003.pdf' target='_blank'>https://arxiv.org/pdf/2505.11003.pdf</a></span>   <span><a href='https://github.com/scu-zjz/ForensicHub' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Bo Du, Xuekang Zhu, Xiaochen Ma, Chenfan Qu, Kaiwen Feng, Zhe Yang, Chi-Man Pun, Jian Liu, Jizhe Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.11003">ForensicHub: A Unified Benchmark & Codebase for All-Domain Fake Image Detection and Localization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The field of Fake Image Detection and Localization (FIDL) is highly fragmented, encompassing four domains: deepfake detection (Deepfake), image manipulation detection and localization (IMDL), artificial intelligence-generated image detection (AIGC), and document image manipulation localization (Doc). Although individual benchmarks exist in some domains, a unified benchmark for all domains in FIDL remains blank. The absence of a unified benchmark results in significant domain silos, where each domain independently constructs its datasets, models, and evaluation protocols without interoperability, preventing cross-domain comparisons and hindering the development of the entire FIDL field. To close the domain silo barrier, we propose ForensicHub, the first unified benchmark & codebase for all-domain fake image detection and localization. Considering drastic variations on dataset, model, and evaluation configurations across all domains, as well as the scarcity of open-sourced baseline models and the lack of individual benchmarks in some domains, ForensicHub: i) proposes a modular and configuration-driven architecture that decomposes forensic pipelines into interchangeable components across datasets, transforms, models, and evaluators, allowing flexible composition across all domains; ii) fully implements 10 baseline models, 6 backbones, 2 new benchmarks for AIGC and Doc, and integrates 2 existing benchmarks of DeepfakeBench and IMDLBenCo through an adapter-based design; iii) conducts indepth analysis based on the ForensicHub, offering 8 key actionable insights into FIDL model architecture, dataset characteristics, and evaluation standards. ForensicHub represents a significant leap forward in breaking the domain silos in the FIDL field and inspiring future breakthroughs.
<div id='section'>Paperid: <span id='pid'>16, <a href='https://arxiv.org/pdf/2504.20865.pdf' target='_blank'>https://arxiv.org/pdf/2504.20865.pdf</a></span>   <span><a href='https://github.com/MI-BioLab/AI-GenBench' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Lorenzo Pellegrini, Davide Cozzolino, Serafino Pandolfini, Davide Maltoni, Matteo Ferrara, Luisa Verdoliva, Marco Prati, Marco Ramilli
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.20865">AI-GenBench: A New Ongoing Benchmark for AI-Generated Image Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The rapid advancement of generative AI has revolutionized image creation, enabling high-quality synthesis from text prompts while raising critical challenges for media authenticity. We present Ai-GenBench, a novel benchmark designed to address the urgent need for robust detection of AI-generated images in real-world scenarios. Unlike existing solutions that evaluate models on static datasets, Ai-GenBench introduces a temporal evaluation framework where detection methods are incrementally trained on synthetic images, historically ordered by their generative models, to test their ability to generalize to new generative models, such as the transition from GANs to diffusion models. Our benchmark focuses on high-quality, diverse visual content and overcomes key limitations of current approaches, including arbitrary dataset splits, unfair comparisons, and excessive computational demands. Ai-GenBench provides a comprehensive dataset, a standardized evaluation protocol, and accessible tools for both researchers and non-experts (e.g., journalists, fact-checkers), ensuring reproducibility while maintaining practical training requirements. By establishing clear evaluation rules and controlled augmentation strategies, Ai-GenBench enables meaningful comparison of detection methods and scalable solutions. Code and data are publicly available to ensure reproducibility and to support the development of robust forensic detectors to keep pace with the rise of new synthetic generators.
<div id='section'>Paperid: <span id='pid'>17, <a href='https://arxiv.org/pdf/2504.14245.pdf' target='_blank'>https://arxiv.org/pdf/2504.14245.pdf</a></span>   <span><a href='https://github.com/Gennadiyev/mllm-defake' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yikun Ji, Yan Hong, Jiahui Zhan, Haoxing Chen, jun lan, Huijia Zhu, Weiqiang Wang, Liqing Zhang, Jianfu Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.14245">Towards Explainable Fake Image Detection with Multi-Modal Large Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Progress in image generation raises significant public security concerns. We argue that fake image detection should not operate as a "black box". Instead, an ideal approach must ensure both strong generalization and transparency. Recent progress in Multi-modal Large Language Models (MLLMs) offers new opportunities for reasoning-based AI-generated image detection. In this work, we evaluate the capabilities of MLLMs in comparison to traditional detection methods and human evaluators, highlighting their strengths and limitations. Furthermore, we design six distinct prompts and propose a framework that integrates these prompts to develop a more robust, explainable, and reasoning-driven detection system. The code is available at https://github.com/Gennadiyev/mllm-defake.
<div id='section'>Paperid: <span id='pid'>18, <a href='https://arxiv.org/pdf/2504.11922.pdf' target='_blank'>https://arxiv.org/pdf/2504.11922.pdf</a></span>   <span><a href='https://github.com/clpbc/BR-Gen' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Lvpan Cai, Haowei Wang, Jiayi Ji, YanShu ZhouMen, Yiwei Ma, Xiaoshuai Sun, Liujuan Cao, Rongrong Ji
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.11922">Zooming In on Fakes: A Novel Dataset for Localized AI-Generated Image Detection with Forgery Amplification Approach</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The rise of AI-generated image editing tools has made localized forgeries increasingly realistic, posing challenges for visual content integrity. Although recent efforts have explored localized AIGC detection, existing datasets predominantly focus on object-level forgeries while overlooking broader scene edits in regions such as sky or ground. To address these limitations, we introduce \textbf{BR-Gen}, a large-scale dataset of 150,000 locally forged images with diverse scene-aware annotations, which are based on semantic calibration to ensure high-quality samples. BR-Gen is constructed through a fully automated Perception-Creation-Evaluation pipeline to ensure semantic coherence and visual realism. In addition, we further propose \textbf{NFA-ViT}, a Noise-guided Forgery Amplification Vision Transformer that enhances the detection of localized forgeries by amplifying forgery-related features across the entire image. NFA-ViT mines heterogeneous regions in images, \emph{i.e.}, potential edited areas, by noise fingerprints. Subsequently, attention mechanism is introduced to compel the interaction between normal and abnormal features, thereby propagating the generalization traces throughout the entire image, allowing subtle forgeries to influence a broader context and improving overall detection robustness. Extensive experiments demonstrate that BR-Gen constructs entirely new scenarios that are not covered by existing methods. Take a step further, NFA-ViT outperforms existing methods on BR-Gen and generalizes well across current benchmarks. All data and codes are available at https://github.com/clpbc/BR-Gen.
<div id='section'>Paperid: <span id='pid'>19, <a href='https://arxiv.org/pdf/2502.08821.pdf' target='_blank'>https://arxiv.org/pdf/2502.08821.pdf</a></span>   <span><a href='https://github.com/Noodulz/dejAIvu' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jocelyn Dzuong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.08821">DejAIvu: Identifying and Explaining AI Art on the Web in Real-Time with Saliency Maps</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The recent surge in advanced generative models, such as diffusion models and generative adversarial networks (GANs), has led to an alarming rise in AI-generated images across various domains on the web. While such technologies offer benefits such as democratizing artistic creation, they also pose challenges in misinformation, digital forgery, and authenticity verification. Additionally, the uncredited use of AI-generated images in media and marketing has sparked significant backlash from online communities. In response to this, we introduce DejAIvu, a Chrome Web extension that combines real-time AI-generated image detection with saliency-based explainability while users browse the web. Using an ONNX-optimized deep learning model, DejAIvu automatically analyzes images on websites such as Google Images, identifies AI-generated content using model inference, and overlays a saliency heatmap to highlight AI-related artifacts. Our approach integrates efficient in-browser inference, gradient-based saliency analysis, and a seamless user experience, ensuring that AI detection is both transparent and interpretable. We also evaluate DejAIvu across multiple pretrained architectures and benchmark datasets, demonstrating high accuracy and low latency, making it a practical and deployable tool for enhancing AI image accountability. The code for this system can be found at https://github.com/Noodulz/dejAIvu.
<div id='section'>Paperid: <span id='pid'>20, <a href='https://arxiv.org/pdf/2501.08763.pdf' target='_blank'>https://arxiv.org/pdf/2501.08763.pdf</a></span>   <span><a href='https://github.com/teheperinko541/Few-Shot-AIGI-Detector' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Shiyu Wu, Jing Liu, Jing Li, Yequan Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.08763">Few-Shot Learner Generalizes Across AI-Generated Image Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Current fake image detectors trained on large synthetic image datasets perform satisfactorily on limited studied generative models. However, these detectors suffer a notable performance decline over unseen models. Besides, collecting adequate training data from online generative models is often expensive or infeasible. To overcome these issues, we propose Few-Shot Detector (FSD), a novel AI-generated image detector which learns a specialized metric space for effectively distinguishing unseen fake images using very few samples. Experiments show that FSD achieves state-of-the-art performance by $+11.6\%$ average accuracy on the GenImage dataset with only $10$ additional samples. More importantly, our method is better capable of capturing the intra-category commonality in unseen images without further training. Our code is available at https://github.com/teheperinko541/Few-Shot-AIGI-Detector.
<div id='section'>Paperid: <span id='pid'>21, <a href='https://arxiv.org/pdf/2501.01184.pdf' target='_blank'>https://arxiv.org/pdf/2501.01184.pdf</a></span>   <span><a href='https://datdaigia.github.io/FakeSTormer/' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/10Ring/FakeSTormer' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Dat Nguyen, Marcella Astrid, Anis Kacem, Enjie Ghorbel, Djamila Aouada
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.01184">Vulnerability-Aware Spatio-Temporal Learning for Generalizable Deepfake Video Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Detecting deepfake videos is highly challenging given the complexity of characterizing spatio-temporal artifacts. Most existing methods rely on binary classifiers trained using real and fake image sequences, therefore hindering their generalization capabilities to unseen generation methods. Moreover, with the constant progress in generative Artificial Intelligence (AI), deepfake artifacts are becoming imperceptible at both the spatial and the temporal levels, making them extremely difficult to capture. To address these issues, we propose a fine-grained deepfake video detection approach called FakeSTormer that enforces the modeling of subtle spatio-temporal inconsistencies while avoiding overfitting. Specifically, we introduce a multi-task learning framework that incorporates two auxiliary branches for explicitly attending artifact-prone spatial and temporal regions. Additionally, we propose a video-level data synthesis strategy that generates pseudo-fake videos with subtle spatio-temporal artifacts, providing high-quality samples and hand-free annotations for our additional branches. Extensive experiments on several challenging benchmarks demonstrate the superiority of our approach compared to recent state-of-the-art methods. The code is available at https://github.com/10Ring/FakeSTormer.
<div id='section'>Paperid: <span id='pid'>22, <a href='https://arxiv.org/pdf/2412.17671.pdf' target='_blank'>https://arxiv.org/pdf/2412.17671.pdf</a></span>   <span><a href='https://grip-unina.github.io/B-Free/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Fabrizio Guillaro, Giada Zingarini, Ben Usman, Avneesh Sud, Davide Cozzolino, Luisa Verdoliva
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.17671">A Bias-Free Training Paradigm for More General AI-generated Image Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Successful forensic detectors can produce excellent results in supervised learning benchmarks but struggle to transfer to real-world applications. We believe this limitation is largely due to inadequate training data quality. While most research focuses on developing new algorithms, less attention is given to training data selection, despite evidence that performance can be strongly impacted by spurious correlations such as content, format, or resolution. A well-designed forensic detector should detect generator specific artifacts rather than reflect data biases. To this end, we propose B-Free, a bias-free training paradigm, where fake images are generated from real ones using the conditioning procedure of stable diffusion models. This ensures semantic alignment between real and fake images, allowing any differences to stem solely from the subtle artifacts introduced by AI generation. Through content-based augmentation, we show significant improvements in both generalization and robustness over state-of-the-art detectors and more calibrated results across 27 different generative models, including recent releases, like FLUX and Stable Diffusion 3.5. Our findings emphasize the importance of a careful dataset design, highlighting the need for further research on this topic. Code and data are publicly available at https://grip-unina.github.io/B-Free/.
<div id='section'>Paperid: <span id='pid'>23, <a href='https://arxiv.org/pdf/2412.05897.pdf' target='_blank'>https://arxiv.org/pdf/2412.05897.pdf</a></span>   <span><a href='https://github.com/tmlr-group/WePe' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jun Nie, Yonggang Zhang, Tongliang Liu, Yiu-ming Cheung, Bo Han, Xinmei Tian
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.05897">Epistemic Uncertainty for Generated Image Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce a novel framework for AI-generated image detection through epistemic uncertainty, aiming to address critical security concerns in the era of generative models. Our key insight stems from the observation that distributional discrepancies between training and testing data manifest distinctively in the epistemic uncertainty space of machine learning models. In this context, the distribution shift between natural and generated images leads to elevated epistemic uncertainty in models trained on natural images when evaluating generated ones. Hence, we exploit this phenomenon by using epistemic uncertainty as a proxy for detecting generated images. This converts the challenge of generated image detection into the problem of uncertainty estimation, underscoring the generalization performance of the model used for uncertainty estimation. Fortunately, advanced large-scale vision models pre-trained on extensive natural images have shown excellent generalization performance for various scenarios. Thus, we utilize these pre-trained models to estimate the epistemic uncertainty of images and flag those with high uncertainty as generated. Extensive experiments demonstrate the efficacy of our method. Code is available at https://github.com/tmlr-group/WePe.
<div id='section'>Paperid: <span id='pid'>24, <a href='https://arxiv.org/pdf/2411.19417.pdf' target='_blank'>https://arxiv.org/pdf/2411.19417.pdf</a></span>   <span><a href='https://mever-team.github.io/spai' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Dimitrios Karageorgiou, Symeon Papadopoulos, Ioannis Kompatsiaris, Efstratios Gavves
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.19417">Any-Resolution AI-Generated Image Detection by Spectral Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent works have established that AI models introduce spectral artifacts into generated images and propose approaches for learning to capture them using labeled data. However, the significant differences in such artifacts among different generative models hinder these approaches from generalizing to generators not seen during training. In this work, we build upon the key idea that the spectral distribution of real images constitutes both an invariant and highly discriminative pattern for AI-generated image detection. To model this under a self-supervised setup, we employ masked spectral learning using the pretext task of frequency reconstruction. Since generated images constitute out-of-distribution samples for this model, we propose spectral reconstruction similarity to capture this divergence. Moreover, we introduce spectral context attention, which enables our approach to efficiently capture subtle spectral inconsistencies in images of any resolution. Our spectral AI-generated image detection approach (SPAI) achieves a 5.5% absolute improvement in AUC over the previous state-of-the-art across 13 recent generative approaches, while exhibiting robustness against common online perturbations. Code is available on https://mever-team.github.io/spai.
<div id='section'>Paperid: <span id='pid'>25, <a href='https://arxiv.org/pdf/2411.15633.pdf' target='_blank'>https://arxiv.org/pdf/2411.15633.pdf</a></span>   <span><a href='https://github.com/YZY-stack/Effort-AIGI-Detection' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/YZY-stack/Effort-AIGI-Detection' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhiyuan Yan, Jiangming Wang, Peng Jin, Ke-Yue Zhang, Chengchun Liu, Shen Chen, Taiping Yao, Shouhong Ding, Baoyuan Wu, Li Yuan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.15633">Orthogonal Subspace Decomposition for Generalizable AI-Generated Image Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>AI-generated images (AIGIs), such as natural or face images, have become increasingly important yet challenging. In this paper, we start from a new perspective to excavate the reason behind the failure generalization in AIGI detection, named the \textit{asymmetry phenomenon}, where a naively trained detector tends to favor overfitting to the limited and monotonous fake patterns, causing the feature space to become highly constrained and low-ranked, which is proved seriously limiting the expressivity and generalization. One potential remedy is incorporating the pre-trained knowledge within the vision foundation models (higher-ranked) to expand the feature space, alleviating the model's overfitting to fake. To this end, we employ Singular Value Decomposition (SVD) to decompose the original feature space into \textit{two orthogonal subspaces}. By freezing the principal components and adapting only the remained components, we preserve the pre-trained knowledge while learning fake patterns. Compared to existing full-parameters and LoRA-based tuning methods, we explicitly ensure orthogonality, enabling the higher rank of the whole feature space, effectively minimizing overfitting and enhancing generalization. We finally identify a crucial insight: our method implicitly learns \textit{a vital prior that fakes are actually derived from the real}, indicating a hierarchical relationship rather than independence. Modeling this prior, we believe, is essential for achieving superior generalization. Our codes are publicly available at \href{https://github.com/YZY-stack/Effort-AIGI-Detection}{GitHub}.
<div id='section'>Paperid: <span id='pid'>26, <a href='https://arxiv.org/pdf/2411.13553.pdf' target='_blank'>https://arxiv.org/pdf/2411.13553.pdf</a></span>   <span><a href='https://github.com/moyangkuo/ImageDetectBench.git' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Moyang Guo, Yuepeng Hu, Zhengyuan Jiang, Zeyu Li, Amir Sadovnik, Arka Daw, Neil Gong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.13553">AI-generated Image Detection: Passive or Watermark?</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>While text-to-image models offer numerous benefits, they also pose significant societal risks. Detecting AI-generated images is crucial for mitigating these risks. Detection methods can be broadly categorized into passive and watermark-based approaches: passive detectors rely on artifacts present in AI-generated images, whereas watermark-based detectors proactively embed watermarks into such images. A key question is which type of detector performs better in terms of effectiveness, robustness, and efficiency. However, the current literature lacks a comprehensive understanding of this issue. In this work, we aim to bridge that gap by developing ImageDetectBench, the first comprehensive benchmark to compare the effectiveness, robustness, and efficiency of passive and watermark-based detectors. Our benchmark includes four datasets, each containing a mix of AI-generated and non-AI-generated images. We evaluate five passive detectors and four watermark-based detectors against eight types of common perturbations and three types of adversarial perturbations. Our benchmark results reveal several interesting findings. For instance, watermark-based detectors consistently outperform passive detectors, both in the presence and absence of perturbations. Based on these insights, we provide recommendations for detecting AI-generated images, e.g., when both types of detectors are applicable, watermark-based detectors should be the preferred choice. Our code and data are publicly available at https://github.com/moyangkuo/ImageDetectBench.git.
<div id='section'>Paperid: <span id='pid'>27, <a href='https://arxiv.org/pdf/2406.19941.pdf' target='_blank'>https://arxiv.org/pdf/2406.19941.pdf</a></span>   <span><a href='https://github.com/ming053l/GRACE' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Chih-Chung Hsu, Shao-Ning Chen, Mei-Hsuan Wu, Yi-Fang Wang, Chia-Ming Lee, Yi-Shiuan Chou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.19941">GRACE: Graph-Regularized Attentive Convolutional Entanglement with Laplacian Smoothing for Robust DeepFake Video Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As DeepFake video manipulation techniques escalate, posing profound threats, the urgent need to develop efficient detection strategies is underscored. However, one particular issue lies with facial images being mis-detected, often originating from degraded videos or adversarial attacks, leading to unexpected temporal artifacts that can undermine the efficacy of DeepFake video detection techniques. This paper introduces a novel method for robust DeepFake video detection, harnessing the power of the proposed Graph-Regularized Attentive Convolutional Entanglement (GRACE) based on the graph convolutional network with graph Laplacian to address the aforementioned challenges. First, conventional Convolution Neural Networks are deployed to perform spatiotemporal features for the entire video. Then, the spatial and temporal features are mutually entangled by constructing a graph with sparse constraint, enforcing essential features of valid face images in the noisy face sequences remaining, thus augmenting stability and performance for DeepFake video detection. Furthermore, the Graph Laplacian prior is proposed in the graph convolutional network to remove the noise pattern in the feature space to further improve the performance. Comprehensive experiments are conducted to illustrate that our proposed method delivers state-of-the-art performance in DeepFake video detection under noisy face sequences. The source code is available at https://github.com/ming053l/GRACE.
<div id='section'>Paperid: <span id='pid'>28, <a href='https://arxiv.org/pdf/2406.19435.pdf' target='_blank'>https://arxiv.org/pdf/2406.19435.pdf</a></span>   <span><a href='https://shilinyan99.github.io/AIDE' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/shilinyan99/AIDE' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Shilin Yan, Ouxiang Li, Jiayin Cai, Yanbin Hao, Xiaolong Jiang, Yao Hu, Weidi Xie
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.19435">A Sanity Check for AI-generated Image Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the rapid development of generative models, discerning AI-generated content has evoked increasing attention from both industry and academia. In this paper, we conduct a sanity check on "whether the task of AI-generated image detection has been solved". To start with, we present Chameleon dataset, consisting AIgenerated images that are genuinely challenging for human perception. To quantify the generalization of existing methods, we evaluate 9 off-the-shelf AI-generated image detectors on Chameleon dataset. Upon analysis, almost all models classify AI-generated images as real ones. Later, we propose AIDE (AI-generated Image DEtector with Hybrid Features), which leverages multiple experts to simultaneously extract visual artifacts and noise patterns. Specifically, to capture the high-level semantics, we utilize CLIP to compute the visual embedding. This effectively enables the model to discern AI-generated images based on semantics or contextual information; Secondly, we select the highest frequency patches and the lowest frequency patches in the image, and compute the low-level patchwise features, aiming to detect AI-generated images by low-level artifacts, for example, noise pattern, anti-aliasing, etc. While evaluating on existing benchmarks, for example, AIGCDetectBenchmark and GenImage, AIDE achieves +3.5% and +4.6% improvements to state-of-the-art methods, and on our proposed challenging Chameleon benchmarks, it also achieves the promising results, despite this problem for detecting AI-generated images is far from being solved.
<div id='section'>Paperid: <span id='pid'>29, <a href='https://arxiv.org/pdf/2405.19707.pdf' target='_blank'>https://arxiv.org/pdf/2405.19707.pdf</a></span>   <span><a href='https://github.com/chenhaoxing/DeMamba' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Haoxing Chen, Yan Hong, Zizheng Huang, Zhuoer Xu, Zhangxuan Gu, Yaohui Li, Jun Lan, Huijia Zhu, Jianfu Zhang, Weiqiang Wang, Huaxiong Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.19707">DeMamba: AI-Generated Video Detection on Million-Scale GenVideo Benchmark</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recently, video generation techniques have advanced rapidly. Given the popularity of video content on social media platforms, these models intensify concerns about the spread of fake information. Therefore, there is a growing demand for detectors capable of distinguishing between fake AI-generated videos and mitigating the potential harm caused by fake information. However, the lack of large-scale datasets from the most advanced video generators poses a barrier to the development of such detectors. To address this gap, we introduce the first AI-generated video detection dataset, GenVideo. It features the following characteristics: (1) a large volume of videos, including over one million AI-generated and real videos collected; (2) a rich diversity of generated content and methodologies, covering a broad spectrum of video categories and generation techniques. We conducted extensive studies of the dataset and proposed two evaluation methods tailored for real-world-like scenarios to assess the detectors' performance: the cross-generator video classification task assesses the generalizability of trained detectors on generators; the degraded video classification task evaluates the robustness of detectors to handle videos that have degraded in quality during dissemination. Moreover, we introduced a plug-and-play module, named Detail Mamba (DeMamba), designed to enhance the detectors by identifying AI-generated videos through the analysis of inconsistencies in temporal and spatial dimensions. Our extensive experiments demonstrate DeMamba's superior generalizability and robustness on GenVideo compared to existing detectors. We believe that the GenVideo dataset and the DeMamba module will significantly advance the field of AI-generated video detection. Our code and dataset will be aviliable at \url{https://github.com/chenhaoxing/DeMamba}.
<div id='section'>Paperid: <span id='pid'>30, <a href='https://arxiv.org/pdf/2404.04883.pdf' target='_blank'>https://arxiv.org/pdf/2404.04883.pdf</a></span>   <span><a href='https://github.com/zhliuworks/CLIPMoLE' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zihan Liu, Hanyi Wang, Yaoyu Kang, Shilin Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.04883">Mixture of Low-rank Experts for Transferable AI-Generated Image Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generative models have shown a giant leap in synthesizing photo-realistic images with minimal expertise, sparking concerns about the authenticity of online information. This study aims to develop a universal AI-generated image detector capable of identifying images from diverse sources. Existing methods struggle to generalize across unseen generative models when provided with limited sample sources. Inspired by the zero-shot transferability of pre-trained vision-language models, we seek to harness the nontrivial visual-world knowledge and descriptive proficiency of CLIP-ViT to generalize over unknown domains. This paper presents a novel parameter-efficient fine-tuning approach, mixture of low-rank experts, to fully exploit CLIP-ViT's potential while preserving knowledge and expanding capacity for transferable detection. We adapt only the MLP layers of deeper ViT blocks via an integration of shared and separate LoRAs within an MoE-based structure. Extensive experiments on public benchmarks show that our method achieves superiority over state-of-the-art approaches in cross-generator generalization and robustness to perturbations. Remarkably, our best-performing ViT-L/14 variant requires training only 0.08% of its parameters to surpass the leading baseline by +3.64% mAP and +12.72% avg.Acc across unseen diffusion and autoregressive models. This even outperforms the baseline with just 0.28% of the training data. Our code and pre-trained models will be available at https://github.com/zhliuworks/CLIPMoLE.
<div id='section'>Paperid: <span id='pid'>31, <a href='https://arxiv.org/pdf/2403.16638.pdf' target='_blank'>https://arxiv.org/pdf/2403.16638.pdf</a></span>   <span><a href='https://github.com/multimediaFor/AIGVDet' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jianfa Bai, Man Lin, Gang Cao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.16638">AI-Generated Video Detection via Spatio-Temporal Anomaly Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The advancement of generation models has led to the emergence of highly realistic artificial intelligence (AI)-generated videos. Malicious users can easily create non-existent videos to spread false information. This letter proposes an effective AI-generated video detection (AIGVDet) scheme by capturing the forensic traces with a two-branch spatio-temporal convolutional neural network (CNN). Specifically, two ResNet sub-detectors are learned separately for identifying the anomalies in spatical and optical flow domains, respectively. Results of such sub-detectors are fused to further enhance the discrimination ability. A large-scale generated video dataset (GVD) is constructed as a benchmark for model training and evaluation. Extensive experimental results verify the high generalization and robustness of our AIGVDet scheme. Code and dataset will be available at https://github.com/multimediaFor/AIGVDet.
<div id='section'>Paperid: <span id='pid'>32, <a href='https://arxiv.org/pdf/2403.10261.pdf' target='_blank'>https://arxiv.org/pdf/2403.10261.pdf</a></span>   <span><a href='https://github.com/rainy-xu/TALL4Deepfake' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuting Xu, Jian Liang, Lijun Sheng, Xiao-Yu Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.10261">Learning Spatiotemporal Inconsistency via Thumbnail Layout for Face Deepfake Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The deepfake threats to society and cybersecurity have provoked significant public apprehension, driving intensified efforts within the realm of deepfake video detection. Current video-level methods are mostly based on {3D CNNs} resulting in high computational demands, although have achieved good performance. This paper introduces an elegantly simple yet effective strategy named Thumbnail Layout (TALL), which transforms a video clip into a pre-defined layout to realize the preservation of spatial and temporal dependencies. This transformation process involves sequentially masking frames at the same positions within each frame. These frames are then resized into sub-frames and reorganized into the predetermined layout, forming thumbnails. TALL is model-agnostic and has remarkable simplicity, necessitating only minimal code modifications. Furthermore, we introduce a graph reasoning block (GRB) and semantic consistency (SC) loss to strengthen TALL, culminating in TALL++. GRB enhances interactions between different semantic regions to capture semantic-level inconsistency clues. The semantic consistency loss imposes consistency constraints on semantic features to improve model generalization ability. Extensive experiments on intra-dataset, cross-dataset, diffusion-generated image detection, and deepfake generation method recognition show that TALL++ achieves results surpassing or comparable to the state-of-the-art methods, demonstrating the effectiveness of our approaches for various deepfake detection problems. The code is available at https://github.com/rainy-xu/TALL4Deepfake.
<div id='section'>Paperid: <span id='pid'>33, <a href='https://arxiv.org/pdf/2312.00195.pdf' target='_blank'>https://arxiv.org/pdf/2312.00195.pdf</a></span>   <span><a href='https://grip-unina.github.io/ClipBased-SyntheticImageDetection/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Davide Cozzolino, Giovanni Poggi, Riccardo Corvi, Matthias NieÃner, Luisa Verdoliva
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.00195">Raising the Bar of AI-generated Image Detection with CLIP</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The aim of this work is to explore the potential of pre-trained vision-language models (VLMs) for universal detection of AI-generated images. We develop a lightweight detection strategy based on CLIP features and study its performance in a wide variety of challenging scenarios. We find that, contrary to previous beliefs, it is neither necessary nor convenient to use a large domain-specific dataset for training. On the contrary, by using only a handful of example images from a single generative model, a CLIP-based detector exhibits surprising generalization ability and high robustness across different architectures, including recent commercial tools such as Dalle-3, Midjourney v5, and Firefly. We match the state-of-the-art (SoTA) on in-distribution data and significantly improve upon it in terms of generalization to out-of-distribution data (+6% AUC) and robustness to impaired/laundered data (+13%). Our project is available at https://grip-unina.github.io/ClipBased-SyntheticImageDetection/
<div id='section'>Paperid: <span id='pid'>34, <a href='https://arxiv.org/pdf/2311.12397.pdf' target='_blank'>https://arxiv.org/pdf/2311.12397.pdf</a></span>   <span><a href='https://fdmas.github.io/AIGCDetect/' target='_blank'>  GitHub</a></span> <span><a href='https://fdmas.github.io/AIGCDetect' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Nan Zhong, Yiran Xu, Sheng Li, Zhenxing Qian, Xinpeng Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.12397">PatchCraft: Exploring Texture Patch for Efficient AI-generated Image Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent generative models show impressive performance in generating photographic images. Humans can hardly distinguish such incredibly realistic-looking AI-generated images from real ones. AI-generated images may lead to ubiquitous disinformation dissemination. Therefore, it is of utmost urgency to develop a detector to identify AI generated images. Most existing detectors suffer from sharp performance drops over unseen generative models. In this paper, we propose a novel AI-generated image detector capable of identifying fake images created by a wide range of generative models. We observe that the texture patches of images tend to reveal more traces left by generative models compared to the global semantic information of the images. A novel Smash&Reconstruction preprocessing is proposed to erase the global semantic information and enhance texture patches. Furthermore, pixels in rich texture regions exhibit more significant fluctuations than those in poor texture regions. Synthesizing realistic rich texture regions proves to be more challenging for existing generative models. Based on this principle, we leverage the inter-pixel correlation contrast between rich and poor texture regions within an image to further boost the detection performance.
  In addition, we build a comprehensive AI-generated image detection benchmark, which includes 17 kinds of prevalent generative models, to evaluate the effectiveness of existing baselines and our approach. Our benchmark provides a leaderboard for follow-up studies. Extensive experimental results show that our approach outperforms state-of-the-art baselines by a significant margin. Our project: https://fdmas.github.io/AIGCDetect
<div id='section'>Paperid: <span id='pid'>35, <a href='https://arxiv.org/pdf/2307.07494.pdf' target='_blank'>https://arxiv.org/pdf/2307.07494.pdf</a></span>   <span><a href='https://github.com/rainy-xu/TALL4Deepfake' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuting Xu, Jian Liang, Gengyun Jia, Ziming Yang, Yanhao Zhang, Ran He
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.07494">TALL: Thumbnail Layout for Deepfake Video Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The growing threats of deepfakes to society and cybersecurity have raised enormous public concerns, and increasing efforts have been devoted to this critical topic of deepfake video detection. Existing video methods achieve good performance but are computationally intensive. This paper introduces a simple yet effective strategy named Thumbnail Layout (TALL), which transforms a video clip into a pre-defined layout to realize the preservation of spatial and temporal dependencies. Specifically, consecutive frames are masked in a fixed position in each frame to improve generalization, then resized to sub-images and rearranged into a pre-defined layout as the thumbnail. TALL is model-agnostic and extremely simple by only modifying a few lines of code. Inspired by the success of vision transformers, we incorporate TALL into Swin Transformer, forming an efficient and effective method TALL-Swin. Extensive experiments on intra-dataset and cross-dataset validate the validity and superiority of TALL and SOTA TALL-Swin. TALL-Swin achieves 90.79$\%$ AUC on the challenging cross-dataset task, FaceForensics++ $\to$ Celeb-DF. The code is available at https://github.com/rainy-xu/TALL4Deepfake.
<div id='section'>Paperid: <span id='pid'>36, <a href='https://arxiv.org/pdf/2307.07036.pdf' target='_blank'>https://arxiv.org/pdf/2307.07036.pdf</a></span>   <span><a href='https://github.com/erprogs/GenConViT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Deressa Wodajo Deressa, Hannes Mareen, Peter Lambert, Solomon Atnafu, Zahid Akhtar, Glenn Van Wallendael
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.07036">GenConViT: Deepfake Video Detection Using Generative Convolutional Vision Transformer</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deepfakes have raised significant concerns due to their potential to spread false information and compromise digital media integrity. Current deepfake detection models often struggle to generalize across a diverse range of deepfake generation techniques and video content. In this work, we propose a Generative Convolutional Vision Transformer (GenConViT) for deepfake video detection. Our model combines ConvNeXt and Swin Transformer models for feature extraction, and it utilizes Autoencoder and Variational Autoencoder to learn from the latent data distribution. By learning from the visual artifacts and latent data distribution, GenConViT achieves improved performance in detecting a wide range of deepfake videos. The model is trained and evaluated on DFDC, FF++, TM, DeepfakeTIMIT, and Celeb-DF (v$2$) datasets. The proposed GenConViT model demonstrates strong performance in deepfake video detection, achieving high accuracy across the tested datasets. While our model shows promising results in deepfake video detection by leveraging visual and latent features, we demonstrate that further work is needed to improve its generalizability, i.e., when encountering out-of-distribution data. Our model provides an effective solution for identifying a wide range of fake videos while preserving media integrity. The open-source code for GenConViT is available at https://github.com/erprogs/GenConViT.
<div id='section'>Paperid: <span id='pid'>37, <a href='https://arxiv.org/pdf/2411.16754.pdf' target='_blank'>https://arxiv.org/pdf/2411.16754.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Nasrin Imanpour, Shashwat Bajpai, Subhankar Ghosh, Sainath Reddy Sankepally, Abhilekh Borah, Hasnat Md Abdullah, Nishoak Kosaraju, Shreyas Dixit, Ashhar Aziz, Shwetangshu Biswas, Vinija Jain, Aman Chadha, Amit Sheth, Amitava Das
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.16754">Visual Counter Turing Test (VCT^2): Discovering the Challenges for AI-Generated Image Detection and Introducing Visual AI Index (V_AI)</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The proliferation of AI techniques for image generation, coupled with their increasing accessibility, has raised significant concerns about the potential misuse of these images to spread misinformation. Recent AI-generated image detection (AGID) methods include CNNDetection, NPR, DM Image Detection, Fake Image Detection, DIRE, LASTED, GAN Image Detection, AIDE, SSP, DRCT, RINE, OCC-CLIP, De-Fake, and Deep Fake Detection. However, we argue that the current state-of-the-art AGID techniques are inadequate for effectively detecting contemporary AI-generated images and advocate for a comprehensive reevaluation of these methods. We introduce the Visual Counter Turing Test (VCT^2), a benchmark comprising ~130K images generated by contemporary text-to-image models (Stable Diffusion 2.1, Stable Diffusion XL, Stable Diffusion 3, DALL-E 3, and Midjourney 6). VCT^2 includes two sets of prompts sourced from tweets by the New York Times Twitter account and captions from the MS COCO dataset. We also evaluate the performance of the aforementioned AGID techniques on the VCT$^2$ benchmark, highlighting their ineffectiveness in detecting AI-generated images. As image-generative AI models continue to evolve, the need for a quantifiable framework to evaluate these models becomes increasingly critical. To meet this need, we propose the Visual AI Index (V_AI), which assesses generated images from various visual perspectives, including texture complexity and object coherence, setting a new standard for evaluating image-generative AI models. To foster research in this domain, we make our https://huggingface.co/datasets/anonymous1233/COCO_AI and https://huggingface.co/datasets/anonymous1233/twitter_AI datasets publicly available.
<div id='section'>Paperid: <span id='pid'>38, <a href='https://arxiv.org/pdf/2511.00181.pdf' target='_blank'>https://arxiv.org/pdf/2511.00181.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mengfei Liang, Yiting Qu, Yukun Jiang, Michael Backes, Yang Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.00181">From Evidence to Verdict: An Agent-Based Forensic Framework for AI-Generated Image Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The rapid evolution of AI-generated images poses unprecedented challenges to information integrity and media authenticity. Existing detection approaches suffer from fundamental limitations: traditional classifiers lack interpretability and fail to generalize across evolving generative models, while vision-language models (VLMs), despite their promise, remain constrained to single-shot analysis and pixel-level reasoning. To address these challenges, we introduce AIFo (Agent-based Image Forensics), a novel training-free framework that emulates human forensic investigation through multi-agent collaboration. Unlike conventional methods, our framework employs a set of forensic tools, including reverse image search, metadata extraction, pre-trained classifiers, and VLM analysis, coordinated by specialized LLM-based agents that collect, synthesize, and reason over cross-source evidence. When evidence is conflicting or insufficient, a structured multi-agent debate mechanism allows agents to exchange arguments and reach a reliable conclusion. Furthermore, we enhance the framework with a memory-augmented reasoning module that learns from historical cases to improve future detection accuracy. Our comprehensive evaluation spans 6,000 images across both controlled laboratory settings and challenging real-world scenarios, including images from modern generative platforms and diverse online sources. AIFo achieves 97.05% accuracy, substantially outperforming traditional classifiers and state-of-the-art VLMs. These results demonstrate that agent-based procedural reasoning offers a new paradigm for more robust, interpretable, and adaptable AI-generated image detection.
<div id='section'>Paperid: <span id='pid'>39, <a href='https://arxiv.org/pdf/2508.01402.pdf' target='_blank'>https://arxiv.org/pdf/2508.01402.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chuangchuang Tan, Jinglu Wang, Xiang Ming, Renshuai Tao, Yunchao Wei, Yao Zhao, Yan Lu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.01402">ForenX: Towards Explainable AI-Generated Image Detection with Multimodal Large Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Advances in generative models have led to AI-generated images visually indistinguishable from authentic ones. Despite numerous studies on detecting AI-generated images with classifiers, a gap persists between such methods and human cognitive forensic analysis. We present ForenX, a novel method that not only identifies the authenticity of images but also provides explanations that resonate with human thoughts. ForenX employs the powerful multimodal large language models (MLLMs) to analyze and interpret forensic cues. Furthermore, we overcome the limitations of standard MLLMs in detecting forgeries by incorporating a specialized forensic prompt that directs the MLLMs attention to forgery-indicative attributes. This approach not only enhance the generalization of forgery detection but also empowers the MLLMs to provide explanations that are accurate, relevant, and comprehensive. Additionally, we introduce ForgReason, a dataset dedicated to descriptions of forgery evidences in AI-generated images. Curated through collaboration between an LLM-based agent and a team of human annotators, this process provides refined data that further enhances our model's performance. We demonstrate that even limited manual annotations significantly improve explanation quality. We evaluate the effectiveness of ForenX on two major benchmarks. The model's explainability is verified by comprehensive subjective evaluations.
<div id='section'>Paperid: <span id='pid'>40, <a href='https://arxiv.org/pdf/2506.14827.pdf' target='_blank'>https://arxiv.org/pdf/2506.14827.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yifeng Gao, Yifan Ding, Hongyu Su, Juncheng Li, Yunhan Zhao, Lin Luo, Zixing Chen, Li Wang, Xin Wang, Yixu Wang, Xingjun Ma, Yu-Gang Jiang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.14827">DAVID-XR1: Detecting AI-Generated Videos with Explainable Reasoning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As AI-generated video becomes increasingly pervasive across media platforms, the ability to reliably distinguish synthetic content from authentic footage has become both urgent and essential. Existing approaches have primarily treated this challenge as a binary classification task, offering limited insight into where or why a model identifies a video as AI-generated. However, the core challenge extends beyond simply detecting subtle artifacts; it requires providing fine-grained, persuasive evidence that can convince auditors and end-users alike. To address this critical gap, we introduce DAVID-X, the first dataset to pair AI-generated videos with detailed defect-level, temporal-spatial annotations and written rationales. Leveraging these rich annotations, we present DAVID-XR1, a video-language model designed to deliver an interpretable chain of visual reasoning-including defect categorization, temporal-spatial localization, and natural language explanations. This approach fundamentally transforms AI-generated video detection from an opaque black-box decision into a transparent and verifiable diagnostic process. We demonstrate that a general-purpose backbone, fine-tuned on our compact dataset and enhanced with chain-of-thought distillation, achieves strong generalization across a variety of generators and generation modes. Our results highlight the promise of explainable detection methods for trustworthy identification of AI-generated video content.
<div id='section'>Paperid: <span id='pid'>41, <a href='https://arxiv.org/pdf/2507.02664.pdf' target='_blank'>https://arxiv.org/pdf/2507.02664.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ziyin Zhou, Yunpeng Luo, Yuanchen Wu, Ke Sun, Jiayi Ji, Ke Yan, Shouhong Ding, Xiaoshuai Sun, Yunsheng Wu, Rongrong Ji
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.02664">AIGI-Holmes: Towards Explainable and Generalizable AI-Generated Image Detection via Multimodal Large Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The rapid development of AI-generated content (AIGC) technology has led to the misuse of highly realistic AI-generated images (AIGI) in spreading misinformation, posing a threat to public information security. Although existing AIGI detection techniques are generally effective, they face two issues: 1) a lack of human-verifiable explanations, and 2) a lack of generalization in the latest generation technology. To address these issues, we introduce a large-scale and comprehensive dataset, Holmes-Set, which includes the Holmes-SFTSet, an instruction-tuning dataset with explanations on whether images are AI-generated, and the Holmes-DPOSet, a human-aligned preference dataset. Our work introduces an efficient data annotation method called the Multi-Expert Jury, enhancing data generation through structured MLLM explanations and quality control via cross-model evaluation, expert defect filtering, and human preference modification. In addition, we propose Holmes Pipeline, a meticulously designed three-stage training framework comprising visual expert pre-training, supervised fine-tuning, and direct preference optimization. Holmes Pipeline adapts multimodal large language models (MLLMs) for AIGI detection while generating human-verifiable and human-aligned explanations, ultimately yielding our model AIGI-Holmes. During the inference stage, we introduce a collaborative decoding strategy that integrates the model perception of the visual expert with the semantic reasoning of MLLMs, further enhancing the generalization capabilities. Extensive experiments on three benchmarks validate the effectiveness of our AIGI-Holmes.
<div id='section'>Paperid: <span id='pid'>42, <a href='https://arxiv.org/pdf/2511.08423.pdf' target='_blank'>https://arxiv.org/pdf/2511.08423.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuncheng Guo, Junyan Ye, Chenjue Zhang, Hengrui Kang, Haohuan Fu, Conghui He, Weijia Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.08423">OmniAID: Decoupling Semantic and Artifacts for Universal AI-Generated Image Detection in the Wild</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>A truly universal AI-Generated Image (AIGI) detector must simultaneously generalize across diverse generative models and varied semantic content. Current state-of-the-art methods learn a single, entangled forgery representation--conflating content-dependent flaws with content-agnostic artifacts--and are further constrained by outdated benchmarks. To overcome these limitations, we propose OmniAID, a novel framework centered on a decoupled Mixture-of-Experts (MoE) architecture. The core of our method is a hybrid expert system engineered to decouple: (1) semantic flaws across distinct content domains, and (2) these content-dependent flaws from content-agnostic universal artifacts. This system employs a set of Routable Specialized Semantic Experts, each for a distinct domain (e.g., human, animal), complemented by a Fixed Universal Artifact Expert. This architecture is trained using a bespoke two-stage strategy: we first train the experts independently with domain-specific hard-sampling to ensure specialization, and subsequently train a lightweight gating network for effective input routing. By explicitly decoupling "what is generated" (content-specific flaws) from "how it is generated" (universal artifacts), OmniAID achieves robust generalization. To address outdated benchmarks and validate real-world applicability, we introduce Mirage, a new large-scale, contemporary dataset. Extensive experiments, using both traditional benchmarks and our Mirage dataset, demonstrate our model surpasses existing monolithic detectors, establishing a new, robust standard for AIGI authentication against modern, in-the-wild threats.
<div id='section'>Paperid: <span id='pid'>43, <a href='https://arxiv.org/pdf/2505.12620.pdf' target='_blank'>https://arxiv.org/pdf/2505.12620.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haiquan Wen, Yiwei He, Zhenglin Huang, Tianxiao Li, Zihan Yu, Xingru Huang, Lu Qi, Baoyuan Wu, Xiangtai Li, Guangliang Cheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.12620">BusterX: MLLM-Powered AI-Generated Video Forgery Detection and Explanation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Advances in AI generative models facilitate super-realistic video synthesis, amplifying misinformation risks via social media and eroding trust in digital content. Several research works have explored new deepfake detection methods on AI-generated images to alleviate these risks. However, with the fast development of video generation models, such as Sora and WanX, there is currently a lack of large-scale, high-quality AI-generated video datasets for forgery detection. In addition, existing detection approaches predominantly treat the task as binary classification, lacking explainability in model decision-making and failing to provide actionable insights or guidance for the public. To address these challenges, we propose \textbf{GenBuster-200K}, a large-scale AI-generated video dataset featuring 200K high-resolution video clips, diverse latest generative techniques, and real-world scenes. We further introduce \textbf{BusterX}, a novel AI-generated video detection and explanation framework leveraging multimodal large language model (MLLM) and reinforcement learning for authenticity determination and explainable rationale. To our knowledge, GenBuster-200K is the {\it \textbf{first}} large-scale, high-quality AI-generated video dataset that incorporates the latest generative techniques for real-world scenarios. BusterX is the {\it \textbf{first}} framework to integrate MLLM with reinforcement learning for explainable AI-generated video detection. Extensive comparisons with state-of-the-art methods and ablation studies validate the effectiveness and generalizability of BusterX. The code, models, and datasets will be released.
<div id='section'>Paperid: <span id='pid'>44, <a href='https://arxiv.org/pdf/2512.05651.pdf' target='_blank'>https://arxiv.org/pdf/2512.05651.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Nan Zhong, Mian Zou, Yiran Xu, Zhenxing Qian, Xinpeng Zhang, Baoyuan Wu, Kede Ma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.05651">Self-Supervised AI-Generated Image Detection: A Camera Metadata Perspective</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The proliferation of AI-generated imagery poses escalating challenges for multimedia forensics, yet many existing detectors depend on assumptions about the internals of specific generative models, limiting their cross-model applicability. We introduce a self-supervised approach for detecting AI-generated images that leverages camera metadata -- specifically exchangeable image file format (EXIF) tags -- to learn features intrinsic to digital photography. Our pretext task trains a feature extractor solely on camera-captured photographs by classifying categorical EXIF tags (\eg, camera model and scene type) and pairwise-ranking ordinal and continuous EXIF tags (\eg, focal length and aperture value). Using these EXIF-induced features, we first perform one-class detection by modeling the distribution of photographic images with a Gaussian mixture model and flagging low-likelihood samples as AI-generated. We then extend to binary detection that treats the learned extractor as a strong regularizer for a classifier of the same architecture, operating on high-frequency residuals from spatially scrambled patches. Extensive experiments across various generative models demonstrate that our EXIF-induced detectors substantially advance the state of the art, delivering strong generalization to in-the-wild samples and robustness to common benign image perturbations.
<div id='section'>Paperid: <span id='pid'>45, <a href='https://arxiv.org/pdf/2510.16442.pdf' target='_blank'>https://arxiv.org/pdf/2510.16442.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haoran Sun, Chen Cai, Huiping Zhuang, Kong Aik Lee, Lap-Pui Chau, Yi Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.16442">EDVD-LLaMA: Explainable Deepfake Video Detection via Multimodal Large Language Model Reasoning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The rapid development of deepfake video technology has not only facilitated artistic creation but also made it easier to spread misinformation. Traditional deepfake video detection (DVD) methods face issues such as a lack of transparency in their principles and insufficient generalization capabilities to cope with evolving forgery techniques. This highlights an urgent need for detectors that can identify forged content and provide verifiable reasoning explanations. This paper proposes the explainable deepfake video detection (EDVD) task and designs the EDVD-LLaMA multimodal, a large language model (MLLM) reasoning framework, which provides traceable reasoning processes alongside accurate detection results and trustworthy explanations. Our approach first incorporates a Spatio-Temporal Subtle Information Tokenization (ST-SIT) to extract and fuse global and local cross-frame deepfake features, providing rich spatio-temporal semantic information input for MLLM reasoning. Second, we construct a Fine-grained Multimodal Chain-of-Thought (Fg-MCoT) mechanism, which introduces facial feature data as hard constraints during the reasoning process to achieve pixel-level spatio-temporal video localization, suppress hallucinated outputs, and enhance the reliability of the chain of thought. In addition, we build an Explainable Reasoning FF++ benchmark dataset (ER-FF++set), leveraging structured data to annotate videos and ensure quality control, thereby supporting dual supervision for reasoning and detection. Extensive experiments demonstrate that EDVD-LLaMA achieves outstanding performance and robustness in terms of detection accuracy, explainability, and its ability to handle cross-forgery methods and cross-dataset scenarios. Compared to previous DVD methods, it provides a more explainable and superior solution. The source code and dataset will be publicly available.
<div id='section'>Paperid: <span id='pid'>46, <a href='https://arxiv.org/pdf/2309.14751.pdf' target='_blank'>https://arxiv.org/pdf/2309.14751.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yunzhuo Chen, Nur Al Hasan Haldar, Naveed Akhtar, Ajmal Mian
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.14751">Text-image guided Diffusion Model for generating Deepfake celebrity interactions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deepfake images are fast becoming a serious concern due to their realism. Diffusion models have recently demonstrated highly realistic visual content generation, which makes them an excellent potential tool for Deepfake generation. To curb their exploitation for Deepfakes, it is imperative to first explore the extent to which diffusion models can be used to generate realistic content that is controllable with convenient prompts. This paper devises and explores a novel method in that regard. Our technique alters the popular stable diffusion model to generate a controllable high-quality Deepfake image with text and image prompts. In addition, the original stable model lacks severely in generating quality images that contain multiple persons. The modified diffusion model is able to address this problem, it add input anchor image's latent at the beginning of inferencing rather than Gaussian random latent as input. Hence, we focus on generating forged content for celebrity interactions, which may be used to spread rumors. We also apply Dreambooth to enhance the realism of our fake images. Dreambooth trains the pairing of center words and specific features to produce more refined and personalized output images. Our results show that with the devised scheme, it is possible to create fake visual content with alarming realism, such that the content can serve as believable evidence of meetings between powerful political figures.
<div id='section'>Paperid: <span id='pid'>47, <a href='https://arxiv.org/pdf/2503.09314.pdf' target='_blank'>https://arxiv.org/pdf/2503.09314.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinghan Li, Jingjing Chen, Yue Yu, Xue Song, Haijun Shan, Yu-Gang Jiang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.09314">Revealing the Implicit Noise-based Imprint of Generative Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the rapid advancement of vision generation models, the potential security risks stemming from synthetic visual content have garnered increasing attention, posing significant challenges for AI-generated image detection. Existing methods suffer from inadequate generalization capabilities, resulting in unsatisfactory performance on emerging generative models. To address this issue, this paper presents a novel framework that leverages noise-based model-specific imprint for the detection task. Specifically, we propose a novel noise-based imprint simulator to capture intrinsic patterns imprinted in images generated by different models. By aggregating imprints from various generative models, imprints of future models can be extrapolated to expand training data, thereby enhancing generalization and robustness. Furthermore, we design a new pipeline that pioneers the use of noise patterns, derived from a noise-based imprint extractor, alongside other visual features for AI-generated image detection, resulting in a significant improvement in performance. Our approach achieves state-of-the-art performance across three public benchmarks including GenImage, Synthbuster and Chameleon.
<div id='section'>Paperid: <span id='pid'>48, <a href='https://arxiv.org/pdf/2501.11340.pdf' target='_blank'>https://arxiv.org/pdf/2501.11340.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhenliang Ni, Qiangyu Yan, Mouxiao Huang, Tianning Yuan, Yehui Tang, Hailin Hu, Xinghao Chen, Yunhe Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.11340">GenVidBench: A Challenging Benchmark for Detecting AI-Generated Video</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The rapid advancement of video generation models has made it increasingly challenging to distinguish AI-generated videos from real ones. This issue underscores the urgent need for effective AI-generated video detectors to prevent the dissemination of false information through such videos. However, the development of high-performance generative video detectors is currently impeded by the lack of large-scale, high-quality datasets specifically designed for generative video detection. To this end, we introduce GenVidBench, a challenging AI-generated video detection dataset with several key advantages: 1) Cross Source and Cross Generator: The cross-generation source mitigates the interference of video content on the detection. The cross-generator ensures diversity in video attributes between the training and test sets, preventing them from being overly similar. 2) State-of-the-Art Video Generators: The dataset includes videos from 8 state-of-the-art AI video generators, ensuring that it covers the latest advancements in the field of video generation. 3) Rich Semantics: The videos in GenVidBench are analyzed from multiple dimensions and classified into various semantic categories based on their content. This classification ensures that the dataset is not only large but also diverse, aiding in the development of more generalized and effective detection models. We conduct a comprehensive evaluation of different advanced video generators and present a challenging setting. Additionally, we present rich experimental results including advanced video classification models as baselines. With the GenVidBench, researchers can efficiently develop and evaluate AI-generated video detection models. Datasets and code are available at https://genvidbench.github.io.
<div id='section'>Paperid: <span id='pid'>49, <a href='https://arxiv.org/pdf/2508.13223.pdf' target='_blank'>https://arxiv.org/pdf/2508.13223.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Cheng Xia, Manxi Lin, Jiexiang Tan, Xiaoxiong Du, Yang Qiu, Junjun Zheng, Xiangheng Kong, Yuning Jiang, Bo Zheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.13223">MIRAGE: Towards AI-Generated Image Detection in the Wild</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The spreading of AI-generated images (AIGI), driven by advances in generative AI, poses a significant threat to information security and public trust. Existing AIGI detectors, while effective against images in clean laboratory settings, fail to generalize to in-the-wild scenarios. These real-world images are noisy, varying from ``obviously fake" images to realistic ones derived from multiple generative models and further edited for quality control. We address in-the-wild AIGI detection in this paper. We introduce Mirage, a challenging benchmark designed to emulate the complexity of in-the-wild AIGI. Mirage is constructed from two sources: (1) a large corpus of Internet-sourced AIGI verified by human experts, and (2) a synthesized dataset created through the collaboration between multiple expert generators, closely simulating the realistic AIGI in the wild. Building on this benchmark, we propose Mirage-R1, a vision-language model with heuristic-to-analytic reasoning, a reflective reasoning mechanism for AIGI detection. Mirage-R1 is trained in two stages: a supervised-fine-tuning cold start, followed by a reinforcement learning stage. By further adopting an inference-time adaptive thinking strategy, Mirage-R1 is able to provide either a quick judgment or a more robust and accurate conclusion, effectively balancing inference speed and performance. Extensive experiments show that our model leads state-of-the-art detectors by 5% and 10% on Mirage and the public benchmark, respectively. The benchmark and code will be made publicly available.
<div id='section'>Paperid: <span id='pid'>50, <a href='https://arxiv.org/pdf/2509.13107.pdf' target='_blank'>https://arxiv.org/pdf/2509.13107.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kohou Wang, Huan Hu, Xiang Liu, Zezhou Chen, Ping Chen, Zhaoxiang Liu, Shiguo Lian
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.13107">Hierarchical Deep Fusion Framework for Multi-dimensional Facial Forgery Detection -- The 2024 Global Deepfake Image Detection Challenge</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The proliferation of sophisticated deepfake technology poses significant challenges to digital security and authenticity. Detecting these forgeries, especially across a wide spectrum of manipulation techniques, requires robust and generalized models. This paper introduces the Hierarchical Deep Fusion Framework (HDFF), an ensemble-based deep learning architecture designed for high-performance facial forgery detection. Our framework integrates four diverse pre-trained sub-models, Swin-MLP, CoAtNet, EfficientNetV2, and DaViT, which are meticulously fine-tuned through a multi-stage process on the MultiFFDI dataset. By concatenating the feature representations from these specialized models and training a final classifier layer, HDFF effectively leverages their collective strengths. This approach achieved a final score of 0.96852 on the competition's private leaderboard, securing the 20th position out of 184 teams, demonstrating the efficacy of hierarchical fusion for complex image classification tasks.
<div id='section'>Paperid: <span id='pid'>51, <a href='https://arxiv.org/pdf/2312.08880.pdf' target='_blank'>https://arxiv.org/pdf/2312.08880.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mingjian Zhu, Hanting Chen, Mouxiao Huang, Wei Li, Hailin Hu, Jie Hu, Yunhe Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.08880">GenDet: Towards Good Generalizations for AI-Generated Image Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The misuse of AI imagery can have harmful societal effects, prompting the creation of detectors to combat issues like the spread of fake news. Existing methods can effectively detect images generated by seen generators, but it is challenging to detect those generated by unseen generators. They do not concentrate on amplifying the output discrepancy when detectors process real versus fake images. This results in a close output distribution of real and fake samples, increasing classification difficulty in detecting unseen generators. This paper addresses the unseen-generator detection problem by considering this task from the perspective of anomaly detection and proposes an adversarial teacher-student discrepancy-aware framework. Our method encourages smaller output discrepancies between the student and the teacher models for real images while aiming for larger discrepancies for fake images. We employ adversarial learning to train a feature augmenter, which promotes smaller discrepancies between teacher and student networks when the inputs are fake images. Our method has achieved state-of-the-art on public benchmarks, and the visualization results show that a large output discrepancy is maintained when faced with various types of generators.
<div id='section'>Paperid: <span id='pid'>52, <a href='https://arxiv.org/pdf/2509.10250.pdf' target='_blank'>https://arxiv.org/pdf/2509.10250.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haozhen Yan, Yan Hong, Suning Lang, Jiahui Zhan, Yikun Ji, Yujie Gao, Jun Lan, Huijia Zhu, Weiqiang Wang, Jianfu Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.10250">GAMMA: Generalizable Alignment via Multi-task and Manipulation-Augmented Training for AI-Generated Image Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With generative models becoming increasingly sophisticated and diverse, detecting AI-generated images has become increasingly challenging. While existing AI-genereted Image detectors achieve promising performance on in-distribution generated images, their generalization to unseen generative models remains limited. This limitation is largely attributed to their reliance on generation-specific artifacts, such as stylistic priors and compression patterns. To address these limitations, we propose GAMMA, a novel training framework designed to reduce domain bias and enhance semantic alignment. GAMMA introduces diverse manipulation strategies, such as inpainting-based manipulation and semantics-preserving perturbations, to ensure consistency between manipulated and authentic content. We employ multi-task supervision with dual segmentation heads and a classification head, enabling pixel-level source attribution across diverse generative domains. In addition, a reverse cross-attention mechanism is introduced to allow the segmentation heads to guide and correct biased representations in the classification branch. Our method achieves state-of-the-art generalization performance on the GenImage benchmark, imporving accuracy by 5.8%, but also maintains strong robustness on newly released generative model such as GPT-4o.
<div id='section'>Paperid: <span id='pid'>53, <a href='https://arxiv.org/pdf/2512.06746.pdf' target='_blank'>https://arxiv.org/pdf/2512.06746.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ruoxin Chen, Jiahui Gao, Kaiqing Lin, Keyue Zhang, Yandan Zhao, Isabel Guan, Taiping Yao, Shouhong Ding
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.06746">Task-Model Alignment: A Simple Path to Generalizable AI-Generated Image Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision Language Models (VLMs) are increasingly adopted for AI-generated images (AIGI) detection, yet converting VLMs into detectors requires substantial resource, while the resulting models still exhibit severe hallucinations. To probe the core issue, we conduct an empirical analysis and observe two characteristic behaviors: (i) fine-tuning VLMs on high-level semantic supervision strengthens semantic discrimination and well generalize to unseen data; (ii) fine-tuning VLMs on low-level pixel-artifact supervision yields poor transfer. We attribute VLMs' underperformance to task-model misalignment: semantics-oriented VLMs inherently lack sensitivity to fine-grained pixel artifacts, and semantically non-discriminative pixel artifacts thus exceeds their inductive biases. In contrast, we observe that conventional pixel-artifact detectors capture low-level pixel artifacts yet exhibit limited semantic awareness relative to VLMs, highlighting that distinct models are better matched to distinct tasks. In this paper, we formalize AIGI detection as two complementary tasks--semantic consistency checking and pixel-artifact detection--and show that neglecting either induces systematic blind spots. Guided by this view, we introduce the Task-Model Alignment principle and instantiate it as a two-branch detector, AlignGemini, comprising a VLM fine-tuned exclusively with pure semantic supervision and a pixel-artifact expert trained exclusively with pure pixel-artifact supervision. By enforcing orthogonal supervision on two simplified datasets, each branch trains to its strengths, producing complementary discrimination over semantic and pixel cues. On five in-the-wild benchmarks, AlignGemini delivers a +9.5 gain in average accuracy, supporting task-model alignment as an effective path to generalizable AIGI detection.
<div id='section'>Paperid: <span id='pid'>54, <a href='https://arxiv.org/pdf/2505.22604.pdf' target='_blank'>https://arxiv.org/pdf/2505.22604.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ruixuan Zhang, He Wang, Zhengyu Zhao, Zhiqing Guo, Xun Yang, Yunfeng Diao, Meng Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.22604">Adversarially Robust AI-Generated Image Detection for Free: An Information Theoretic Perspective</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Rapid advances in Artificial Intelligence Generated Images (AIGI) have facilitated malicious use, such as forgery and misinformation. Therefore, numerous methods have been proposed to detect fake images. Although such detectors have been proven to be universally vulnerable to adversarial attacks, defenses in this field are scarce. In this paper, we first identify that adversarial training (AT), widely regarded as the most effective defense, suffers from performance collapse in AIGI detection. Through an information-theoretic lens, we further attribute the cause of collapse to feature entanglement, which disrupts the preservation of feature-label mutual information. Instead, standard detectors show clear feature separation. Motivated by this difference, we propose Training-free Robust Detection via Information-theoretic Measures (TRIM), the first training-free adversarial defense for AIGI detection. TRIM builds on standard detectors and quantifies feature shifts using prediction entropy and KL divergence. Extensive experiments across multiple datasets and attacks validate the superiority of our TRIM, e.g., outperforming the state-of-the-art defense by 33.88% (28.91%) on ProGAN (GenImage), while well maintaining original accuracy.
<div id='section'>Paperid: <span id='pid'>55, <a href='https://arxiv.org/pdf/2504.01396.pdf' target='_blank'>https://arxiv.org/pdf/2504.01396.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zheng Yang, Ruoxin Chen, Zhiyuan Yan, Ke-Yue Zhang, Xinghe Fu, Shuang Wu, Xiujun Shu, Taiping Yao, Shouhong Ding, Xi Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.01396">All Patches Matter, More Patches Better: Enhance AI-Generated Image Detection via Panoptic Patch Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The exponential growth of AI-generated images (AIGIs) underscores the urgent need for robust and generalizable detection methods. In this paper, we establish two key principles for AIGI detection through systematic analysis: (1) All Patches Matter: Unlike conventional image classification where discriminative features concentrate on object-centric regions, each patch in AIGIs inherently contains synthetic artifacts due to the uniform generation process, suggesting that every patch serves as an important artifact source for detection. (2) More Patches Better: Leveraging distributed artifacts across more patches improves detection robustness by capturing complementary forensic evidence and reducing over-reliance on specific patches, thereby enhancing robustness and generalization. However, our counterfactual analysis reveals an undesirable phenomenon: naively trained detectors often exhibit a Few-Patch Bias, discriminating between real and synthetic images based on minority patches. We identify Lazy Learner as the root cause: detectors preferentially learn conspicuous artifacts in limited patches while neglecting broader artifact distributions. To address this bias, we propose the Panoptic Patch Learning (PPL) framework, involving: (1) Random Patch Replacement that randomly substitutes synthetic patches with real counterparts to compel models to identify artifacts in underutilized regions, encouraging the broader use of more patches; (2) Patch-wise Contrastive Learning that enforces consistent discriminative capability across all patches, ensuring uniform utilization of all patches. Extensive experiments across two different settings on several benchmarks verify the effectiveness of our approach.
<div id='section'>Paperid: <span id='pid'>56, <a href='https://arxiv.org/pdf/2408.17065.pdf' target='_blank'>https://arxiv.org/pdf/2408.17065.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhiyuan Yan, Yandan Zhao, Shen Chen, Mingyi Guo, Xinghe Fu, Taiping Yao, Shouhong Ding, Li Yuan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.17065">Generalizing Deepfake Video Detection with Plug-and-Play: Video-Level Blending and Spatiotemporal Adapter Tuning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Three key challenges hinder the development of current deepfake video detection: (1) Temporal features can be complex and diverse: how can we identify general temporal artifacts to enhance model generalization? (2) Spatiotemporal models often lean heavily on one type of artifact and ignore the other: how can we ensure balanced learning from both? (3) Videos are naturally resource-intensive: how can we tackle efficiency without compromising accuracy? This paper attempts to tackle the three challenges jointly. First, inspired by the notable generality of using image-level blending data for image forgery detection, we investigate whether and how video-level blending can be effective in video. We then perform a thorough analysis and identify a previously underexplored temporal forgery artifact: Facial Feature Drift (FFD), which commonly exists across different forgeries. To reproduce FFD, we then propose a novel Video-level Blending data (VB), where VB is implemented by blending the original image and its warped version frame-by-frame, serving as a hard negative sample to mine more general artifacts. Second, we carefully design a lightweight Spatiotemporal Adapter (StA) to equip a pretrained image model (both ViTs and CNNs) with the ability to capture both spatial and temporal features jointly and efficiently. StA is designed with two-stream 3D-Conv with varying kernel sizes, allowing it to process spatial and temporal features separately. Extensive experiments validate the effectiveness of the proposed methods; and show our approach can generalize well to previously unseen forgery videos, even the latest generation methods.
<div id='section'>Paperid: <span id='pid'>57, <a href='https://arxiv.org/pdf/2407.20836.pdf' target='_blank'>https://arxiv.org/pdf/2407.20836.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yunfeng Diao, Naixin Zhai, Changtao Miao, Zitong Yu, Xingxing Wei, Xun Yang, Meng Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.20836">Vulnerabilities in AI-generated Image Detection: The Challenge of Adversarial Attacks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advancements in image synthesis, particularly with the advent of GAN and Diffusion models, have amplified public concerns regarding the dissemination of disinformation. To address such concerns, numerous AI-generated Image (AIGI) Detectors have been proposed and achieved promising performance in identifying fake images. However, there still lacks a systematic understanding of the adversarial robustness of AIGI detectors. In this paper, we examine the vulnerability of state-of-the-art AIGI detectors against adversarial attack under white-box and black-box settings, which has been rarely investigated so far. To this end, we propose a new method to attack AIGI detectors. First, inspired by the obvious difference between real images and fake images in the frequency domain, we add perturbations under the frequency domain to push the image away from its original frequency distribution. Second, we explore the full posterior distribution of the surrogate model to further narrow this gap between heterogeneous AIGI detectors, e.g. transferring adversarial examples across CNNs and ViTs. This is achieved by introducing a novel post-train Bayesian strategy that turns a single surrogate into a Bayesian one, capable of simulating diverse victim models using one pre-trained surrogate, without the need for re-training. We name our method as Frequency-based Post-train Bayesian Attack, or FPBA. Through FPBA, we show that adversarial attack is truly a real threat to AIGI detectors, because FPBA can deliver successful black-box attacks across models, generators, defense methods, and even evade cross-generator detection, which is a crucial real-world detection scenario. The code will be shared upon acceptance.
<div id='section'>Paperid: <span id='pid'>58, <a href='https://arxiv.org/pdf/2412.03261.pdf' target='_blank'>https://arxiv.org/pdf/2412.03261.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Edoardo Daniele Cannas, Sara Mandelli, NataÅ¡a PopoviÄ, Ayman Alkhateeb, Alessandro Gnutti, Paolo Bestagini, Stefano Tubaro
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.03261">Is JPEG AI going to change image forensics?</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we investigate the counter-forensic effects of the new JPEG AI standard based on neural image compression, focusing on two critical areas: deepfake image detection and image splicing localization. Neural image compression leverages advanced neural network algorithms to achieve higher compression rates while maintaining image quality. However, it introduces artifacts that closely resemble those generated by image synthesis techniques and image splicing pipelines, complicating the work of researchers when discriminating pristine from manipulated content. We comprehensively analyze JPEG AI's counter-forensic effects through extensive experiments on several state-of-the-art detectors and datasets. Our results demonstrate a reduction in the performance of leading forensic detectors when analyzing content processed through JPEG AI. By exposing the vulnerabilities of the available forensic tools, we aim to raise the urgent need for multimedia forensics researchers to include JPEG AI images in their experimental setups and develop robust forensic techniques to distinguish between neural compression artifacts and actual manipulations.
<div id='section'>Paperid: <span id='pid'>59, <a href='https://arxiv.org/pdf/2510.10053.pdf' target='_blank'>https://arxiv.org/pdf/2510.10053.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Bo Peng, Zichuan Wang, Sheng Yu, Xiaochuan Jin, Wei Wang, Jing Dong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.10053">DREAM: A Benchmark Study for Deepfake REalism AssessMent</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep learning based face-swap videos, widely known as deepfakes, have drawn wide attention due to their threat to information credibility. Recent works mainly focus on the problem of deepfake detection that aims to reliably tell deepfakes apart from real ones, in an objective way. On the other hand, the subjective perception of deepfakes, especially its computational modeling and imitation, is also a significant problem but lacks adequate study. In this paper, we focus on the visual realism assessment of deepfakes, which is defined as the automatic assessment of deepfake visual realism that approximates human perception of deepfakes. It is important for evaluating the quality and deceptiveness of deepfakes which can be used for predicting the influence of deepfakes on Internet, and it also has potentials in improving the deepfake generation process by serving as a critic. This paper prompts this new direction by presenting a comprehensive benchmark called DREAM, which stands for Deepfake REalism AssessMent. It is comprised of a deepfake video dataset of diverse quality, a large scale annotation that includes 140,000 realism scores and textual descriptions obtained from 3,500 human annotators, and a comprehensive evaluation and analysis of 16 representative realism assessment methods, including recent large vision language model based methods and a newly proposed description-aligned CLIP method. The benchmark and insights included in this study can lay the foundation for future research in this direction and other related areas.
<div id='section'>Paperid: <span id='pid'>60, <a href='https://arxiv.org/pdf/2510.02282.pdf' target='_blank'>https://arxiv.org/pdf/2510.02282.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kyoungjun Park, Yifan Yang, Juheon Yi, Shicheng Zheng, Yifei Shen, Dongqi Han, Caihua Shan, Muhammad Muaz, Lili Qiu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.02282">VidGuard-R1: AI-Generated Video Detection and Explanation via Reasoning MLLMs and RL</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the rapid advancement of AI-generated videos, there is an urgent need for effective detection tools to mitigate societal risks such as misinformation and reputational harm. In addition to accurate classification, it is essential that detection models provide interpretable explanations to ensure transparency for regulators and end users. To address these challenges, we introduce VidGuard-R1, the first video authenticity detector that fine-tunes a multi-modal large language model (MLLM) using group relative policy optimization (GRPO). Our model delivers both highly accurate judgments and insightful reasoning. We curate a challenging dataset of 140k real and AI-generated videos produced by state-of-the-art generation models, carefully designing the generation process to maximize discrimination difficulty. We then fine-tune Qwen-VL using GRPO with two specialized reward models that target temporal artifacts and generation complexity. Extensive experiments demonstrate that VidGuard-R1 achieves state-of-the-art zero-shot performance on existing benchmarks, with additional training pushing accuracy above 95%. Case studies further show that VidGuard-R1 produces precise and interpretable rationales behind its predictions. The code is publicly available at https://VidGuard-R1.github.io.
<div id='section'>Paperid: <span id='pid'>61, <a href='https://arxiv.org/pdf/2405.15343.pdf' target='_blank'>https://arxiv.org/pdf/2405.15343.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lichuan Ji, Yingqi Lin, Zhenhua Huang, Yan Han, Xiaogang Xu, Jiafei Wu, Chong Wang, Zhe Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.15343">Distinguish Any Fake Videos: Unleashing the Power of Large-scale Data and Motion Features</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The development of AI-Generated Content (AIGC) has empowered the creation of remarkably realistic AI-generated videos, such as those involving Sora. However, the widespread adoption of these models raises concerns regarding potential misuse, including face video scams and copyright disputes. Addressing these concerns requires the development of robust tools capable of accurately determining video authenticity. The main challenges lie in the dataset and neural classifier for training. Current datasets lack a varied and comprehensive repository of real and generated content for effective discrimination. In this paper, we first introduce an extensive video dataset designed specifically for AI-Generated Video Detection (GenVidDet). It includes over 2.66 M instances of both real and generated videos, varying in categories, frames per second, resolutions, and lengths. The comprehensiveness of GenVidDet enables the training of a generalizable video detector. We also present the Dual-Branch 3D Transformer (DuB3D), an innovative and effective method for distinguishing between real and generated videos, enhanced by incorporating motion information alongside visual appearance. DuB3D utilizes a dual-branch architecture that adaptively leverages and fuses raw spatio-temporal data and optical flow. We systematically explore the critical factors affecting detection performance, achieving the optimal configuration for DuB3D. Trained on GenVidDet, DuB3D can distinguish between real and generated video content with 96.77% accuracy, and strong generalization capability even for unseen types.
<div id='section'>Paperid: <span id='pid'>62, <a href='https://arxiv.org/pdf/2405.20112.pdf' target='_blank'>https://arxiv.org/pdf/2405.20112.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhiyuan He, Pin-Yu Chen, Tsung-Yi Ho
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.20112">RIGID: A Training-free and Model-Agnostic Framework for Robust AI-Generated Image Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The rapid advances in generative AI models have empowered the creation of highly realistic images with arbitrary content, raising concerns about potential misuse and harm, such as Deepfakes. Current research focuses on training detectors using large datasets of generated images. However, these training-based solutions are often computationally expensive and show limited generalization to unseen generated images. In this paper, we propose a training-free method to distinguish between real and AI-generated images. We first observe that real images are more robust to tiny noise perturbations than AI-generated images in the representation space of vision foundation models. Based on this observation, we propose RIGID, a training-free and model-agnostic method for robust AI-generated image detection. RIGID is a simple yet effective approach that identifies whether an image is AI-generated by comparing the representation similarity between the original and the noise-perturbed counterpart. Our evaluation on a diverse set of AI-generated images and benchmarks shows that RIGID significantly outperforms existing trainingbased and training-free detectors. In particular, the average performance of RIGID exceeds the current best training-free method by more than 25%. Importantly, RIGID exhibits strong generalization across different image generation methods and robustness to image corruptions.
<div id='section'>Paperid: <span id='pid'>63, <a href='https://arxiv.org/pdf/2507.10236.pdf' target='_blank'>https://arxiv.org/pdf/2507.10236.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Despina Konstantinidou, Dimitrios Karageorgiou, Christos Koutlis, Olga Papadopoulou, Emmanouil Schinas, Symeon Papadopoulos
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.10236">Navigating the Challenges of AI-Generated Image Detection in the Wild: What Truly Matters?</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The rapid advancement of generative technologies presents both unprecedented creative opportunities and significant challenges, particularly in maintaining social trust and ensuring the integrity of digital information. Following these concerns, the challenge of AI-Generated Image Detection (AID) becomes increasingly critical. As these technologies become more sophisticated, the quality of AI-generated images has reached a level that can easily deceive even the most discerning observers. Our systematic evaluation highlights a critical weakness in current AI-Generated Image Detection models: while they perform exceptionally well on controlled benchmark datasets, they struggle significantly with real-world variations. To assess this, we introduce ITW-SM, a new dataset of real and AI-generated images collected from major social media platforms. In this paper, we identify four key factors that influence AID performance in real-world scenarios: backbone architecture, training data composition, pre-processing strategies and data augmentation combinations. By systematically analyzing these components, we shed light on their impact on detection efficacy. Our modifications result in an average AUC improvement of 26.87% across various AID models under real-world conditions.
<div id='section'>Paperid: <span id='pid'>64, <a href='https://arxiv.org/pdf/2402.11843.pdf' target='_blank'>https://arxiv.org/pdf/2402.11843.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yan Hong, Jianfu Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.11843">WildFake: A Large-scale Challenging Dataset for AI-Generated Images Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The extraordinary ability of generative models enabled the generation of images with such high quality that human beings cannot distinguish Artificial Intelligence (AI) generated images from real-life photographs. The development of generation techniques opened up new opportunities but concurrently introduced potential risks to privacy, authenticity, and security. Therefore, the task of detecting AI-generated imagery is of paramount importance to prevent illegal activities. To assess the generalizability and robustness of AI-generated image detection, we present a large-scale dataset, referred to as WildFake, comprising state-of-the-art generators, diverse object categories, and real-world applications. WildFake dataset has the following advantages: 1) Rich Content with Wild collection: WildFake collects fake images from the open-source community, enriching its diversity with a broad range of image classes and image styles. 2) Hierarchical structure: WildFake contains fake images synthesized by different types of generators from GANs, diffusion models, to other generative models. These key strengths enhance the generalization and robustness of detectors trained on WildFake, thereby demonstrating WildFake's considerable relevance and effectiveness for AI-generated detectors in real-world scenarios. Moreover, our extensive evaluation experiments are tailored to yield profound insights into the capabilities of different levels of generative models, a distinctive advantage afforded by WildFake's unique hierarchical structure.
<div id='section'>Paperid: <span id='pid'>65, <a href='https://arxiv.org/pdf/2509.12995.pdf' target='_blank'>https://arxiv.org/pdf/2509.12995.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yue Zhou, Xinan He, Kaiqing Lin, Bing Fan, Feng Ding, Jinhua Zeng, Bin Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.12995">Brought a Gun to a Knife Fight: Modern VFM Baselines Outgun Specialized Detectors on In-the-Wild AI Image Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>While specialized detectors for AI-generated images excel on curated benchmarks, they fail catastrophically in real-world scenarios, as evidenced by their critically high false-negative rates on `in-the-wild' benchmarks. Instead of crafting another specialized `knife' for this problem, we bring a `gun' to the fight: a simple linear classifier on a modern Vision Foundation Model (VFM). Trained on identical data, this baseline decisively `outguns' bespoke detectors, boosting in-the-wild accuracy by a striking margin of over 20\%. Our analysis pinpoints the source of the VFM's `firepower': First, by probing text-image similarities, we find that recent VLMs (e.g., Perception Encoder, Meta CLIP2) have learned to align synthetic images with forgery-related concepts (e.g., `AI-generated'), unlike previous versions. Second, we speculate that this is due to data exposure, as both this alignment and overall accuracy plummet on a novel dataset scraped after the VFM's pre-training cut-off date, ensuring it was unseen during pre-training. Our findings yield two critical conclusions: 1) For the real-world `gunfight' of AI-generated image detection, the raw `firepower' of an updated VFM is far more effective than the `craftsmanship' of a static detector. 2) True generalization evaluation requires test data to be independent of the model's entire training history, including pre-training.
<div id='section'>Paperid: <span id='pid'>66, <a href='https://arxiv.org/pdf/2308.09921.pdf' target='_blank'>https://arxiv.org/pdf/2308.09921.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Juan Hu, Xin Liao, Difei Gao, Satoshi Tsutsui, Qian Wang, Zheng Qin, Mike Zheng Shou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.09921">Recap: Detecting Deepfake Video with Unpredictable Tampered Traces via Recovering Faces and Mapping Recovered Faces</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The exploitation of Deepfake techniques for malicious intentions has driven significant research interest in Deepfake detection. Deepfake manipulations frequently introduce random tampered traces, leading to unpredictable outcomes in different facial regions. However, existing detection methods heavily rely on specific forgery indicators, and as the forgery mode improves, these traces become increasingly randomized, resulting in a decline in the detection performance of methods reliant on specific forgery traces. To address the limitation, we propose Recap, a novel Deepfake detection model that exposes unspecific facial part inconsistencies by recovering faces and enlarges the differences between real and fake by mapping recovered faces. In the recovering stage, the model focuses on randomly masking regions of interest (ROIs) and reconstructing real faces without unpredictable tampered traces, resulting in a relatively good recovery effect for real faces while a poor recovery effect for fake faces. In the mapping stage, the output of the recovery phase serves as supervision to guide the facial mapping process. This mapping process strategically emphasizes the mapping of fake faces with poor recovery, leading to a further deterioration in their representation, while enhancing and refining the mapping of real faces with good representation. As a result, this approach significantly amplifies the discrepancies between real and fake videos. Our extensive experiments on standard benchmarks demonstrate that Recap is effective in multiple scenarios.
<div id='section'>Paperid: <span id='pid'>67, <a href='https://arxiv.org/pdf/2305.05943.pdf' target='_blank'>https://arxiv.org/pdf/2305.05943.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Juan Hu, Xin Liao, Difei Gao, Satoshi Tsutsui, Qian Wang, Zheng Qin, Mike Zheng Shou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.05943">Mover: Mask and Recovery based Facial Part Consistency Aware Method for Deepfake Video Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deepfake techniques have been widely used for malicious purposes, prompting extensive research interest in developing Deepfake detection methods. Deepfake manipulations typically involve tampering with facial parts, which can result in inconsistencies across different parts of the face. For instance, Deepfake techniques may change smiling lips to an upset lip, while the eyes remain smiling. Existing detection methods depend on specific indicators of forgery, which tend to disappear as the forgery patterns are improved. To address the limitation, we propose Mover, a new Deepfake detection model that exploits unspecific facial part inconsistencies, which are inevitable weaknesses of Deepfake videos. Mover randomly masks regions of interest (ROIs) and recovers faces to learn unspecific features, which makes it difficult for fake faces to be recovered, while real faces can be easily recovered. Specifically, given a real face image, we first pretrain a masked autoencoder to learn facial part consistency by dividing faces into three parts and randomly masking ROIs, which are then recovered based on the unmasked facial parts. Furthermore, to maximize the discrepancy between real and fake videos, we propose a novel model with dual networks that utilize the pretrained encoder and masked autoencoder, respectively. 1) The pretrained encoder is finetuned for capturing the encoding of inconsistent information in the given video. 2) The pretrained masked autoencoder is utilized for mapping faces and distinguishing real and fake videos. Our extensive experiments on standard benchmarks demonstrate that Mover is highly effective.
<div id='section'>Paperid: <span id='pid'>68, <a href='https://arxiv.org/pdf/2303.01740.pdf' target='_blank'>https://arxiv.org/pdf/2303.01740.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Juan Hu, Xin Liao, Difei Gao, Satoshi Tsutsui, Qian Wang, Zheng Qin, Mike Zheng Shou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.01740">Mover: Mask and Recovery based Facial Part Consistency Aware Method for Deepfake Video Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deepfake techniques have been widely used for malicious purposes, prompting extensive research interest in developing Deepfake detection methods. Deepfake manipulations typically involve tampering with facial parts, which can result in inconsistencies across different parts of the face. For instance, Deepfake techniques may change smiling lips to an upset lip, while the eyes remain smiling. Existing detection methods depend on specific indicators of forgery, which tend to disappear as the forgery patterns are improved. To address the limitation, we propose Mover, a new Deepfake detection model that exploits unspecific facial part inconsistencies, which are inevitable weaknesses of Deepfake videos. Mover randomly masks regions of interest (ROIs) and recovers faces to learn unspecific features, which makes it difficult for fake faces to be recovered, while real faces can be easily recovered. Specifically, given a real face image, we first pretrain a masked autoencoder to learn facial part consistency by dividing faces into three parts and randomly masking ROIs, which are then recovered based on the unmasked facial parts. Furthermore, to maximize the discrepancy between real and fake videos, we propose a novel model with dual networks that utilize the pretrained encoder and masked autoencoder, respectively. 1) The pretrained encoder is finetuned for capturing the encoding of inconsistent information in the given video. 2) The pretrained masked autoencoder is utilized for mapping faces and distinguishing real and fake videos. Our extensive experiments on standard benchmarks demonstrate that Mover is highly effective.
<div id='section'>Paperid: <span id='pid'>69, <a href='https://arxiv.org/pdf/2508.01525.pdf' target='_blank'>https://arxiv.org/pdf/2508.01525.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kuo Shi, Jie Lu, Shanshan Ye, Guangquan Zhang, Zhen Fang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.01525">MiraGe: Multimodal Discriminative Representation Learning for Generalizable AI-Generated Image Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in generative models have highlighted the need for robust detectors capable of distinguishing real images from AI-generated images. While existing methods perform well on known generators, their performance often declines when tested with newly emerging or unseen generative models due to overlapping feature embeddings that hinder accurate cross-generator classification. In this paper, we propose Multimodal Discriminative Representation Learning for Generalizable AI-generated Image Detection (MiraGe), a method designed to learn generator-invariant features. Motivated by theoretical insights on intra-class variation minimization and inter-class separation, MiraGe tightly aligns features within the same class while maximizing separation between classes, enhancing feature discriminability. Moreover, we apply multimodal prompt learning to further refine these principles into CLIP, leveraging text embeddings as semantic anchors for effective discriminative representation learning, thereby improving generalizability. Comprehensive experiments across multiple benchmarks show that MiraGe achieves state-of-the-art performance, maintaining robustness even against unseen generators like Sora.
<div id='section'>Paperid: <span id='pid'>70, <a href='https://arxiv.org/pdf/2506.10474.pdf' target='_blank'>https://arxiv.org/pdf/2506.10474.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shahroz Tariq, David Nguyen, M. A. P. Chamikara, Tingmin Wu, Alsharif Abuadbba, Kristen Moore
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.10474">LLMs Are Not Yet Ready for Deepfake Image Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The growing sophistication of deepfakes presents substantial challenges to the integrity of media and the preservation of public trust. Concurrently, vision-language models (VLMs), large language models enhanced with visual reasoning capabilities, have emerged as promising tools across various domains, sparking interest in their applicability to deepfake detection. This study conducts a structured zero-shot evaluation of four prominent VLMs: ChatGPT, Claude, Gemini, and Grok, focusing on three primary deepfake types: faceswap, reenactment, and synthetic generation. Leveraging a meticulously assembled benchmark comprising authentic and manipulated images from diverse sources, we evaluate each model's classification accuracy and reasoning depth. Our analysis indicates that while VLMs can produce coherent explanations and detect surface-level anomalies, they are not yet dependable as standalone detection systems. We highlight critical failure modes, such as an overemphasis on stylistic elements and vulnerability to misleading visual patterns like vintage aesthetics. Nevertheless, VLMs exhibit strengths in interpretability and contextual analysis, suggesting their potential to augment human expertise in forensic workflows. These insights imply that although general-purpose models currently lack the reliability needed for autonomous deepfake detection, they hold promise as integral components in hybrid or human-in-the-loop detection frameworks.
<div id='section'>Paperid: <span id='pid'>71, <a href='https://arxiv.org/pdf/2410.13156.pdf' target='_blank'>https://arxiv.org/pdf/2410.13156.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Juncong Xu, Yang Yang, Han Fang, Honggu Liu, Weiming Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.13156">FAMSeC: A Few-shot-sample-based General AI-generated Image Detection Method</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The explosive growth of generative AI has saturated the internet with AI-generated images, raising security concerns and increasing the need for reliable detection methods. The primary requirement for such detection is generalizability, typically achieved by training on numerous fake images from various models. However, practical limitations, such as closed-source models and restricted access, often result in limited training samples. Therefore, training a general detector with few-shot samples is essential for modern detection mechanisms. To address this challenge, we propose FAMSeC, a general AI-generated image detection method based on LoRA-based Forgery Awareness Module and Semantic feature-guided Contrastive learning strategy. To effectively learn from limited samples and prevent overfitting, we developed a Forgery Awareness Module (FAM) based on LoRA, maintaining the generalization of pre-trained features. Additionally, to cooperate with FAM, we designed a Semantic feature-guided Contrastive learning strategy (SeC), making the FAM focus more on the differences between real/fake image than on the features of the samples themselves. Experiments show that FAMSeC outperforms state-of-the-art method, enhancing classification accuracy by 14.55% with just 0.56% of the training samples.
<div id='section'>Paperid: <span id='pid'>72, <a href='https://arxiv.org/pdf/2504.00463.pdf' target='_blank'>https://arxiv.org/pdf/2504.00463.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ziyin Zhou, Ke Sun, Zhongxi Chen, Xianming Lin, Yunpeng Luo, Ke Yan, Shouhong Ding, Xiaoshuai Sun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.00463">Exploring the Collaborative Advantage of Low-level Information on Generalizable AI-Generated Image Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Existing state-of-the-art AI-Generated image detection methods mostly consider extracting low-level information from RGB images to help improve the generalization of AI-Generated image detection, such as noise patterns. However, these methods often consider only a single type of low-level information, which may lead to suboptimal generalization. Through empirical analysis, we have discovered a key insight: different low-level information often exhibits generalization capabilities for different types of forgeries. Furthermore, we found that simple fusion strategies are insufficient to leverage the detection advantages of each low-level and high-level information for various forgery types. Therefore, we propose the Adaptive Low-level Experts Injection (ALEI) framework. Our approach introduces Lora Experts, enabling the backbone network, which is trained with high-level semantic RGB images, to accept and learn knowledge from different low-level information. We utilize a cross-attention method to adaptively fuse these features at intermediate layers. To prevent the backbone network from losing the modeling capabilities of different low-level features during the later stages of modeling, we developed a Low-level Information Adapter that interacts with the features extracted by the backbone network. Finally, we propose Dynamic Feature Selection, which dynamically selects the most suitable features for detecting the current image to maximize generalization detection capability. Extensive experiments demonstrate that our method, finetuned on only four categories of mainstream ProGAN data, performs excellently and achieves state-of-the-art results on multiple datasets containing unseen GAN and Diffusion methods.
<div id='section'>Paperid: <span id='pid'>73, <a href='https://arxiv.org/pdf/2503.08484.pdf' target='_blank'>https://arxiv.org/pdf/2503.08484.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shengpeng Xiao, Yuanfang Guo, Heqi Peng, Zeming Liu, Liang Yang, Yunhong Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.08484">Generalizable AI-Generated Image Detection Based on Fractal Self-Similarity in the Spectrum</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The generalization performance of AI-generated image detection remains a critical challenge. Although most existing methods perform well in detecting images from generative models included in the training set, their accuracy drops significantly when faced with images from unseen generators. To address this limitation, we propose a novel detection method based on the fractal self-similarity of the spectrum, a common feature among images generated by different models. Specifically, we demonstrate that AI-generated images exhibit fractal-like spectral growth through periodic extension and low-pass filtering. This observation motivates us to exploit the similarity among different fractal branches of the spectrum. Instead of directly analyzing the spectrum, our method mitigates the impact of varying spectral characteristics across different generators, improving detection performance for images from unseen models. Experiments on a public benchmark demonstrated the generalized detection performance across both GANs and diffusion models.
<div id='section'>Paperid: <span id='pid'>74, <a href='https://arxiv.org/pdf/2512.20937.pdf' target='_blank'>https://arxiv.org/pdf/2512.20937.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ruiqi Liu, Yi Han, Zhengbo Zhang, Liwei Yao, Zhiyuan Yan, Jialiang Shen, ZhiJin Chen, Boyi Sun, Lubin Weng, Jing Dong, Yan Wang, Shu Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.20937">Beyond Artifacts: Real-Centric Envelope Modeling for Reliable AI-Generated Image Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The rapid progress of generative models has intensified the need for reliable and robust detection under real-world conditions. However, existing detectors often overfit to generator-specific artifacts and remain highly sensitive to real-world degradations. As generative architectures evolve and images undergo multi-round cross-platform sharing and post-processing (chain degradations), these artifact cues become obsolete and harder to detect. To address this, we propose Real-centric Envelope Modeling (REM), a new paradigm that shifts detection from learning generator artifacts to modeling the robust distribution of real images. REM introduces feature-level perturbations in self-reconstruction to generate near-real samples, and employs an envelope estimator with cross-domain consistency to learn a boundary enclosing the real image manifold. We further build RealChain, a comprehensive benchmark covering both open-source and commercial generators with simulated real-world degradation. Across eight benchmark evaluations, REM achieves an average improvement of 7.5% over state-of-the-art methods, and notably maintains exceptional generalization on the severely degraded RealChain benchmark, establishing a solid foundation for synthetic image detection under real-world conditions. The code and the RealChain benchmark will be made publicly available upon acceptance of the paper.
<div id='section'>Paperid: <span id='pid'>75, <a href='https://arxiv.org/pdf/2509.09172.pdf' target='_blank'>https://arxiv.org/pdf/2509.09172.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chunxiao Li, Xiaoxiao Wang, Meiling Li, Boming Miao, Peng Sun, Yunjian Zhang, Xiangyang Ji, Yao Zhu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.09172">Bridging the Gap Between Ideal and Real-world Evaluation: Benchmarking AI-Generated Image Detection in Challenging Scenarios</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the rapid advancement of generative models, highly realistic image synthesis has posed new challenges to digital security and media credibility. Although AI-generated image detection methods have partially addressed these concerns, a substantial research gap remains in evaluating their performance under complex real-world conditions. This paper introduces the Real-World Robustness Dataset (RRDataset) for comprehensive evaluation of detection models across three dimensions: 1) Scenario Generalization: RRDataset encompasses high-quality images from seven major scenarios (War and Conflict, Disasters and Accidents, Political and Social Events, Medical and Public Health, Culture and Religion, Labor and Production, and everyday life), addressing existing dataset gaps from a content perspective. 2) Internet Transmission Robustness: examining detector performance on images that have undergone multiple rounds of sharing across various social media platforms. 3) Re-digitization Robustness: assessing model effectiveness on images altered through four distinct re-digitization methods. We benchmarked 17 detectors and 10 vision-language models (VLMs) on RRDataset and conducted a large-scale human study involving 192 participants to investigate human few-shot learning capabilities in detecting AI-generated images. The benchmarking results reveal the limitations of current AI detection methods under real-world conditions and underscore the importance of drawing on human adaptability to develop more robust detection algorithms.
<div id='section'>Paperid: <span id='pid'>76, <a href='https://arxiv.org/pdf/2502.14994.pdf' target='_blank'>https://arxiv.org/pdf/2502.14994.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qingyuan Liu, Yun-Yun Tsai, Ruijian Zha, Victoria Li, Pengyuan Shi, Chengzhi Mao, Junfeng Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.14994">LAVID: An Agentic LVLM Framework for Diffusion-Generated Video Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The impressive achievements of generative models in creating high-quality videos have raised concerns about digital integrity and privacy vulnerabilities. Recent works of AI-generated content detection have been widely studied in the image field (e.g., deepfake), yet the video field has been unexplored. Large Vision Language Model (LVLM) has become an emerging tool for AI-generated content detection for its strong reasoning and multimodal capabilities. It breaks the limitations of traditional deep learning based methods faced with like lack of transparency and inability to recognize new artifacts. Motivated by this, we propose LAVID, a novel LVLMs-based ai-generated video detection with explicit knowledge enhancement. Our insight list as follows: (1) The leading LVLMs can call external tools to extract useful information to facilitate its own video detection task; (2) Structuring the prompt can affect LVLM's reasoning ability to interpret information in video content. Our proposed pipeline automatically selects a set of explicit knowledge tools for detection, and then adaptively adjusts the structure prompt by self-rewriting. Different from prior SOTA that trains additional detectors, our method is fully training-free and only requires inference of the LVLM for detection. To facilitate our research, we also create a new benchmark \vidfor with high-quality videos generated from multiple sources of video generation tools. Evaluation results show that LAVID improves F1 scores by 6.2 to 30.2% over the top baselines on our datasets across four SOTA LVLMs.
<div id='section'>Paperid: <span id='pid'>77, <a href='https://arxiv.org/pdf/2506.16802.pdf' target='_blank'>https://arxiv.org/pdf/2506.16802.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Riccardo Corvi, Davide Cozzolino, Ekta Prashnani, Shalini De Mello, Koki Nagano, Luisa Verdoliva
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.16802">Seeing What Matters: Generalizable AI-generated Video Detection with Forensic-Oriented Augmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Synthetic video generation is progressing very rapidly. The latest models can produce very realistic high-resolution videos that are virtually indistinguishable from real ones. Although several video forensic detectors have been recently proposed, they often exhibit poor generalization, which limits their applicability in a real-world scenario. Our key insight to overcome this issue is to guide the detector towards seeing what really matters. In fact, a well-designed forensic classifier should focus on identifying intrinsic low-level artifacts introduced by a generative architecture rather than relying on high-level semantic flaws that characterize a specific model. In this work, first, we study different generative architectures, searching and identifying discriminative features that are unbiased, robust to impairments, and shared across models. Then, we introduce a novel forensic-oriented data augmentation strategy based on the wavelet decomposition and replace specific frequency-related bands to drive the model to exploit more relevant forensic cues. Our novel training paradigm improves the generalizability of AI-generated video detectors, without the need for complex algorithms and large datasets that include multiple synthetic generators. To evaluate our approach, we train the detector using data from a single generative model and test it against videos produced by a wide range of other models. Despite its simplicity, our method achieves a significant accuracy improvement over state-of-the-art detectors and obtains excellent results even on very recent generative models, such as NOVA and FLUX. Code and data will be made publicly available.
<div id='section'>Paperid: <span id='pid'>78, <a href='https://arxiv.org/pdf/2407.19553.pdf' target='_blank'>https://arxiv.org/pdf/2407.19553.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Vincenzo De Rosa, Fabrizio Guillaro, Giovanni Poggi, Davide Cozzolino, Luisa Verdoliva
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.19553">Exploring the Adversarial Robustness of CLIP for AI-generated Image Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In recent years, many forensic detectors have been proposed to detect AI-generated images and prevent their use for malicious purposes. Convolutional neural networks (CNNs) have long been the dominant architecture in this field and have been the subject of intense study. However, recently proposed Transformer-based detectors have been shown to match or even outperform CNN-based detectors, especially in terms of generalization. In this paper, we study the adversarial robustness of AI-generated image detectors, focusing on Contrastive Language-Image Pretraining (CLIP)-based methods that rely on Visual Transformer (ViT) backbones and comparing their performance with CNN-based methods. We study the robustness to different adversarial attacks under a variety of conditions and analyze both numerical results and frequency-domain patterns. CLIP-based detectors are found to be vulnerable to white-box attacks just like CNN-based detectors. However, attacks do not easily transfer between CNN-based and CLIP-based methods. This is also confirmed by the different distribution of the adversarial noise patterns in the frequency domain. Overall, this analysis provides new insights into the properties of forensic detectors that can help to develop more effective strategies.
<div id='section'>Paperid: <span id='pid'>79, <a href='https://arxiv.org/pdf/2511.16136.pdf' target='_blank'>https://arxiv.org/pdf/2511.16136.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiazhen Yan, Ziqiang Li, Fan Wang, Kai Zeng, Zhangjie Fu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.16136">How Noise Benefits AI-generated Image Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The rapid advancement of generative models has made real and synthetic images increasingly indistinguishable. Although extensive efforts have been devoted to detecting AI-generated images, out-of-distribution generalization remains a persistent challenge. We trace this weakness to spurious shortcuts exploited during training and we also observe that small feature-space perturbations can mitigate shortcut dominance. To address this problem in a more controllable manner, we propose the Positive-Incentive Noise for CLIP (PiN-CLIP), which jointly trains a noise generator and a detection network under a variational positive-incentive principle. Specifically, we construct positive-incentive noise in the feature space via cross-attention fusion of visual and categorical semantic features. During optimization, the noise is injected into the feature space to fine-tune the visual encoder, suppressing shortcut-sensitive directions while amplifying stable forensic cues, thereby enabling the extraction of more robust and generalized artifact representations. Comparative experiments are conducted on an open-world dataset comprising synthetic images generated by 42 distinct generative models. Our method achieves new state-of-the-art performance, with notable improvements of 5.4 in average accuracy over existing approaches.
<div id='section'>Paperid: <span id='pid'>80, <a href='https://arxiv.org/pdf/2511.13108.pdf' target='_blank'>https://arxiv.org/pdf/2511.13108.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiazhen Yan, Ziqiang Li, Fan Wang, Boyu Wang, Zhangjie Fu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.13108">DGS-Net: Distillation-Guided Gradient Surgery for CLIP Fine-Tuning in AI-Generated Image Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The rapid progress of generative models such as GANs and diffusion models has led to the widespread proliferation of AI-generated images, raising concerns about misinformation, privacy violations, and trust erosion in digital media. Although large-scale multimodal models like CLIP offer strong transferable representations for detecting synthetic content, fine-tuning them often induces catastrophic forgetting, which degrades pre-trained priors and limits cross-domain generalization. To address this issue, we propose the Distillation-guided Gradient Surgery Network (DGS-Net), a novel framework that preserves transferable pre-trained priors while suppressing task-irrelevant components. Specifically, we introduce a gradient-space decomposition that separates harmful and beneficial descent directions during optimization. By projecting task gradients onto the orthogonal complement of harmful directions and aligning with beneficial ones distilled from a frozen CLIP encoder, DGS-Net achieves unified optimization of prior preservation and irrelevant suppression. Extensive experiments on 50 generative models demonstrate that our method outperforms state-of-the-art approaches by an average margin of 6.6, achieving superior detection performance and generalization across diverse generation techniques.
<div id='section'>Paperid: <span id='pid'>81, <a href='https://arxiv.org/pdf/2508.01603.pdf' target='_blank'>https://arxiv.org/pdf/2508.01603.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yiheng Li, Zichang Tan, Zhen Lei, Xu Zhou, Yang Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.01603">Towards Generalizable AI-Generated Image Detection via Image-Adaptive Prompt Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>A major struggle for AI-generated image detection is identifying fake images from unseen generators. Existing cutting-edge methods typically customize pre-trained foundation models to this task via partial-parameter fine-tuning. However, these parameters trained on a narrow range of generators may fail to generalize to unknown sources. In light of this, we propose a novel framework named Image-Adaptive Prompt Learning (IAPL), which enhances flexibility in processing diverse testing images. It consists of two adaptive modules, i.e., the Conditional Information Learner and the Confidence-Driven Adaptive Prediction. The former employs CNN-based feature extractors to learn forgery-specific and image-specific conditions, which are then propagated to learnable tokens via a gated mechanism. The latter optimizes the shallowest learnable tokens based on a single test sample and selects the cropped view with the highest prediction confidence for final detection. These two modules enable the prompts fed into the foundation model to be automatically adjusted based on the input image, rather than being fixed after training, thereby enhancing the model's adaptability to various forged images. Extensive experiments show that IAPL achieves state-of-the-art performance, with 95.61% and 96.7% mean accuracy on two widely used UniversalFakeDetect and GenImage datasets, respectively.
<div id='section'>Paperid: <span id='pid'>82, <a href='https://arxiv.org/pdf/2508.01248.pdf' target='_blank'>https://arxiv.org/pdf/2508.01248.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiazhen Yan, Fan Wang, Weiwei Jiang, Ziqiang Li, Zhangjie Fu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.01248">NS-Net: Decoupling CLIP Semantic Information through NULL-Space for Generalizable AI-Generated Image Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The rapid progress of generative models, such as GANs and diffusion models, has facilitated the creation of highly realistic images, raising growing concerns over their misuse in security-sensitive domains. While existing detectors perform well under known generative settings, they often fail to generalize to unknown generative models, especially when semantic content between real and fake images is closely aligned. In this paper, we revisit the use of CLIP features for AI-generated image detection and uncover a critical limitation: the high-level semantic information embedded in CLIP's visual features hinders effective discrimination. To address this, we propose NS-Net, a novel detection framework that leverages NULL-Space projection to decouple semantic information from CLIP's visual features, followed by contrastive learning to capture intrinsic distributional differences between real and generated images. Furthermore, we design a Patch Selection strategy to preserve fine-grained artifacts by mitigating semantic bias caused by global image structures. Extensive experiments on an open-world benchmark comprising images generated by 40 diverse generative models show that NS-Net outperforms existing state-of-the-art methods, achieving a 7.4\% improvement in detection accuracy, thereby demonstrating strong generalization across both GAN- and diffusion-based image generation techniques.
<div id='section'>Paperid: <span id='pid'>83, <a href='https://arxiv.org/pdf/2501.15253.pdf' target='_blank'>https://arxiv.org/pdf/2501.15253.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiazhen Yan, Ziqiang Li, Fan Wang, Ziwen He, Zhangjie Fu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.15253">Dual Frequency Branch Framework with Reconstructed Sliding Windows Attention for AI-Generated Image Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The rapid advancement of Generative Adversarial Networks (GANs) and diffusion models has enabled the creation of highly realistic synthetic images, presenting significant societal risks, such as misinformation and deception. As a result, detecting AI-generated images has emerged as a critical challenge. Existing researches emphasize extracting fine-grained features to enhance detector generalization, yet they often lack consideration for the importance and interdependencies of internal elements within local regions and are limited to a single frequency domain, hindering the capture of general forgery traces. To overcome the aforementioned limitations, we first utilize a sliding window to restrict the attention mechanism to a local window, and reconstruct the features within the window to model the relationships between neighboring internal elements within the local region. Then, we design a dual frequency domain branch framework consisting of four frequency domain subbands of DWT and the phase part of FFT to enrich the extraction of local forgery features from different perspectives. Through feature enrichment of dual frequency domain branches and fine-grained feature extraction of reconstruction sliding window attention, our method achieves superior generalization detection capabilities on both GAN and diffusion model-based generative images. Evaluated on diverse datasets comprising images from 65 distinct generative models, our approach achieves a 2.13\% improvement in detection accuracy over state-of-the-art methods.
<div id='section'>Paperid: <span id='pid'>84, <a href='https://arxiv.org/pdf/2510.25141.pdf' target='_blank'>https://arxiv.org/pdf/2510.25141.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wan Jiang, Jing Yan, Ruixuan Zhang, Xiaojing Chen, Changtao Miao, Zhe Li, Chenhao Lin, Yunfeng Diao, Richang Hong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.25141">Revisiting Reconstruction-based AI-generated Image Detection: A Geometric Perspective</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The rise of generative Artificial Intelligence (AI) has made detecting AI-generated images a critical challenge for ensuring authenticity. Existing reconstruction-based methods lack theoretical foundations and on empirical heuristics, limiting interpretability and reliability. In this paper, we introduce the Jacobian-Spectral Lower Bound for reconstruction error from a geometric perspective, showing that real images off the reconstruction manifold exhibit a non-trivial error lower bound, while generated images on the manifold have near-zero error. Furthermore, we reveal the limitations of existing methods that rely on static reconstruction error from a single pass. These methods often fail when some real images exhibit lower error than generated ones. This counterintuitive behavior reduces detection accuracy and requires data-specific threshold tuning, limiting their applicability in real-world scenarios. To address these challenges, we propose ReGap, a training-free method that computes dynamic reconstruction error by leveraging structured editing operations to introduce controlled perturbations. This enables measuring error changes before and after editing, improving detection accuracy by enhancing error separation. Experimental results show that our method outperforms existing baselines, exhibits robustness to common post-processing operations and generalizes effectively across diverse conditions.
<div id='section'>Paperid: <span id='pid'>85, <a href='https://arxiv.org/pdf/2512.14601.pdf' target='_blank'>https://arxiv.org/pdf/2512.14601.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhaolun Li, Jichang Li, Yinqi Cai, Junye Chen, Xiaonan Luo, Guanbin Li, Rushi Lan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.14601">FakeRadar: Probing Forgery Outliers to Detect Unknown Deepfake Videos</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we propose FakeRadar, a novel deepfake video detection framework designed to address the challenges of cross-domain generalization in real-world scenarios. Existing detection methods typically rely on manipulation-specific cues, performing well on known forgery types but exhibiting severe limitations against emerging manipulation techniques. This poor generalization stems from their inability to adapt effectively to unseen forgery patterns. To overcome this, we leverage large-scale pretrained models (e.g. CLIP) to proactively probe the feature space, explicitly highlighting distributional gaps between real videos, known forgeries, and unseen manipulations. Specifically, FakeRadar introduces Forgery Outlier Probing, which employs dynamic subcluster modeling and cluster-conditional outlier generation to synthesize outlier samples near boundaries of estimated subclusters, simulating novel forgery artifacts beyond known manipulation types. Additionally, we design Outlier-Guided Tri-Training, which optimizes the detector to distinguish real, fake, and outlier samples using proposed outlier-driven contrastive learning and outlier-conditioned cross-entropy losses. Experiments show that FakeRadar outperforms existing methods across various benchmark datasets for deepfake video detection, particularly in cross-domain evaluations, by handling the variety of emerging manipulation techniques.
<div id='section'>Paperid: <span id='pid'>86, <a href='https://arxiv.org/pdf/2510.25237.pdf' target='_blank'>https://arxiv.org/pdf/2510.25237.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yinqi Cai, Jichang Li, Zhaolun Li, Weikai Chen, Rushi Lan, Xi Xie, Xiaonan Luo, Guanbin Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.25237">DeepShield: Fortifying Deepfake Video Detection with Local and Global Forgery Analysis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in deep generative models have made it easier to manipulate face videos, raising significant concerns about their potential misuse for fraud and misinformation. Existing detectors often perform well in in-domain scenarios but fail to generalize across diverse manipulation techniques due to their reliance on forgery-specific artifacts. In this work, we introduce DeepShield, a novel deepfake detection framework that balances local sensitivity and global generalization to improve robustness across unseen forgeries. DeepShield enhances the CLIP-ViT encoder through two key components: Local Patch Guidance (LPG) and Global Forgery Diversification (GFD). LPG applies spatiotemporal artifact modeling and patch-wise supervision to capture fine-grained inconsistencies often overlooked by global models. GFD introduces domain feature augmentation, leveraging domain-bridging and boundary-expanding feature generation to synthesize diverse forgeries, mitigating overfitting and enhancing cross-domain adaptability. Through the integration of novel local and global analysis for deepfake detection, DeepShield outperforms state-of-the-art methods in cross-dataset and cross-manipulation evaluations, achieving superior robustness against unseen deepfake attacks.
<div id='section'>Paperid: <span id='pid'>87, <a href='https://arxiv.org/pdf/2507.18031.pdf' target='_blank'>https://arxiv.org/pdf/2507.18031.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ahmad ALBarqawi, Mahmoud Nazzal, Issa Khalil, Abdallah Khreishah, NhatHai Phan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.18031">ViGText: Deepfake Image Detection with Vision-Language Model Explanations and Graph Neural Networks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The rapid rise of deepfake technology, which produces realistic but fraudulent digital content, threatens the authenticity of media. Traditional deepfake detection approaches often struggle with sophisticated, customized deepfakes, especially in terms of generalization and robustness against malicious attacks. This paper introduces ViGText, a novel approach that integrates images with Vision Large Language Model (VLLM) Text explanations within a Graph-based framework to improve deepfake detection. The novelty of ViGText lies in its integration of detailed explanations with visual data, as it provides a more context-aware analysis than captions, which often lack specificity and fail to reveal subtle inconsistencies. ViGText systematically divides images into patches, constructs image and text graphs, and integrates them for analysis using Graph Neural Networks (GNNs) to identify deepfakes. Through the use of multi-level feature extraction across spatial and frequency domains, ViGText captures details that enhance its robustness and accuracy to detect sophisticated deepfakes. Extensive experiments demonstrate that ViGText significantly enhances generalization and achieves a notable performance boost when it detects user-customized deepfakes. Specifically, average F1 scores rise from 72.45% to 98.32% under generalization evaluation, and reflects the model's superior ability to generalize to unseen, fine-tuned variations of stable diffusion models. As for robustness, ViGText achieves an increase of 11.1% in recall compared to other deepfake detection approaches. When facing targeted attacks that exploit its graph-based architecture, ViGText limits classification performance degradation to less than 4%. ViGText uses detailed visual and textual analysis to set a new standard for detecting deepfakes, helping ensure media authenticity and information integrity.
<div id='section'>Paperid: <span id='pid'>88, <a href='https://arxiv.org/pdf/2511.23158.pdf' target='_blank'>https://arxiv.org/pdf/2511.23158.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Huangsen Cao, Qin Mei, Zhiheng Li, Yuxi Li, Ying Zhang, Chen Li, Zhimeng Zhang, Xin Ding, Yongwei Wang, Jing Lyu, Fei Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.23158">REVEAL: Reasoning-enhanced Forensic Evidence Analysis for Explainable AI-generated Image Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the rapid advancement of generative models, visually realistic AI-generated images have become increasingly difficult to distinguish from authentic ones, posing severe threats to social trust and information integrity. Consequently, there is an urgent need for efficient and truly explainable image forensic methods. Recent detection paradigms have shifted towards explainable forensics. However, state-of-the-art approaches primarily rely on post-hoc rationalizations or visual discrimination, lacking a verifiable chain of evidence. This reliance on surface-level pattern matching limits the generation of causally grounded explanations and often results in poor generalization. To bridge this critical gap, we introduce \textbf{REVEAL-Bench}, the first reasoning-enhanced multimodal benchmark for AI-generated image detection that is explicitly structured around a chain-of-evidence derived from multiple lightweight expert models, then records step-by-step reasoning traces and evidential justifications. Building upon this dataset, we propose \textbf{REVEAL} (\underline{R}easoning-\underline{e}nhanced Forensic E\underline{v}id\underline{e}nce \underline{A}na\underline{l}ysis), an effective and explainable forensic framework that integrates detection with a novel expert-grounded reinforcement learning. Our reward mechanism is specially tailored to jointly optimize detection accuracy, explanation fidelity, and logical coherence grounded in explicit forensic evidence, enabling REVEAL to produce fine-grained, interpretable, and verifiable reasoning chains alongside its detection outcomes. Extensive experimental results demonstrate that REVEAL significantly enhances detection accuracy, explanation fidelity, and robust cross-model generalization, benchmarking a new state of the art for explainable image forensics.
<div id='section'>Paperid: <span id='pid'>89, <a href='https://arxiv.org/pdf/2511.19126.pdf' target='_blank'>https://arxiv.org/pdf/2511.19126.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Beilin Chu, Weike You, Mengtao Li, Tingting Zheng, Kehan Zhao, Xuan Xu, Zhigao Lu, Jia Song, Moxuan Xu, Linna Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.19126">When Semantics Regulate: Rethinking Patch Shuffle and Internal Bias for Generated Image Detection with CLIP</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The rapid progress of GANs and Diffusion Models poses new challenges for detecting AI-generated images. Although CLIP-based detectors exhibit promising generalization, they often rely on semantic cues rather than generator artifacts, leading to brittle performance under distribution shifts. In this work, we revisit the nature of semantic bias and uncover that Patch Shuffle provides an unusually strong benefit for CLIP, that disrupts global semantic continuity while preserving local artifact cues, which reduces semantic entropy and homogenizes feature distributions between natural and synthetic images. Through a detailed layer-wise analysis, we further show that CLIP's deep semantic structure functions as a regulator that stabilizes cross-domain representations once semantic bias is suppressed. Guided by these findings, we propose SemAnti, a semantic-antagonistic fine-tuning paradigm that freezes the semantic subspace and adapts only artifact-sensitive layers under shuffled semantics. Despite its simplicity, SemAnti achieves state-of-the-art cross-domain generalization on AIGCDetectBenchmark and GenImage, demonstrating that regulating semantics is key to unlocking CLIP's full potential for robust AI-generated image detection.
<div id='section'>Paperid: <span id='pid'>90, <a href='https://arxiv.org/pdf/2308.04177.pdf' target='_blank'>https://arxiv.org/pdf/2308.04177.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Boquan Li, Jun Sun, Christopher M. Poskitt, Xingmei Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.04177">How Generalizable are Deepfake Image Detectors? An Empirical Study</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deepfakes are becoming increasingly credible, posing a significant threat given their potential to facilitate fraud or bypass access control systems. This has motivated the development of deepfake detection methods, in which deep learning models are trained to distinguish between real and synthesized footage. Unfortunately, existing detectors struggle to generalize to deepfakes from datasets they were not trained on, but little work has been done to examine why or how this limitation can be addressed. Especially, those single-modality deepfake images reveal little available forgery evidence, posing greater challenges than detecting deepfake videos. In this work, we present the first empirical study on the generalizability of deepfake detectors, an essential goal for detectors to stay one step ahead of attackers. Our study utilizes six deepfake datasets, five deepfake image detection methods, and two model augmentation approaches, confirming that detectors do not generalize in zero-shot settings. Additionally, we find that detectors are learning unwanted properties specific to synthesis methods and struggling to extract discriminative features, limiting their ability to generalize. Finally, we find that there are neurons universally contributing to detection across seen and unseen datasets, suggesting a possible path towards zero-shot generalizability.
<div id='section'>Paperid: <span id='pid'>91, <a href='https://arxiv.org/pdf/2512.00539.pdf' target='_blank'>https://arxiv.org/pdf/2512.00539.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yongkang Hu, Yu Cheng, Yushuo Zhang, Yuan Xie, Zhaoxia Yin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.00539">SAIDO: Generalizable Detection of AI-Generated Images via Scene-Aware and Importance-Guided Dynamic Optimization in Continual Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The widespread misuse of image generation technologies has raised security concerns, driving the development of AI-generated image detection methods. However, generalization has become a key challenge and open problem: existing approaches struggle to adapt to emerging generative methods and content types in real-world scenarios. To address this issue, we propose a Scene-Aware and Importance-Guided Dynamic Optimization detection framework with continual learning (SAIDO). Specifically, we design Scene-Awareness-Based Expert Module (SAEM) that dynamically identifies and incorporates new scenes using VLLMs. For each scene, independent expert modules are dynamically allocated, enabling the framework to capture scene-specific forgery features better and enhance cross-scene generalization. To mitigate catastrophic forgetting when learning from multiple image generative methods, we introduce Importance-Guided Dynamic Optimization Mechanism (IDOM), which optimizes each neuron through an importance-guided gradient projection strategy, thereby achieving an effective balance between model plasticity and stability. Extensive experiments on continual learning tasks demonstrate that our method outperforms the current SOTA method in both stability and plasticity, achieving 44.22\% and 40.57\% relative reductions in average detection error rate and forgetting rate, respectively. On open-world datasets, it improves the average detection accuracy by 9.47\% compared to the current SOTA method.
<div id='section'>Paperid: <span id='pid'>92, <a href='https://arxiv.org/pdf/2508.17346.pdf' target='_blank'>https://arxiv.org/pdf/2508.17346.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lianrui Mu, Zou Xingze, Jianhong Bai, Jiaqi Hu, Wenjie Zheng, Jiangnan Ye, Jiedong Zhuang, Mudassar Ali, Jing Wang, Haoji Hu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.17346">No Pixel Left Behind: A Detail-Preserving Architecture for Robust High-Resolution AI-Generated Image Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The rapid growth of high-resolution, meticulously crafted AI-generated images poses a significant challenge to existing detection methods, which are often trained and evaluated on low-resolution, automatically generated datasets that do not align with the complexities of high-resolution scenarios. A common practice is to resize or center-crop high-resolution images to fit standard network inputs. However, without full coverage of all pixels, such strategies risk either obscuring subtle, high-frequency artifacts or discarding information from uncovered regions, leading to input information loss. In this paper, we introduce the High-Resolution Detail-Aggregation Network (HiDA-Net), a novel framework that ensures no pixel is left behind. We use the Feature Aggregation Module (FAM), which fuses features from multiple full-resolution local tiles with a down-sampled global view of the image. These local features are aggregated and fused with global representations for final prediction, ensuring that native-resolution details are preserved and utilized for detection. To enhance robustness against challenges such as localized AI manipulations and compression, we introduce Token-wise Forgery Localization (TFL) module for fine-grained spatial sensitivity and JPEG Quality Factor Estimation (QFE) module to disentangle generative artifacts from compression noise explicitly. Furthermore, to facilitate future research, we introduce HiRes-50K, a new challenging benchmark consisting of 50,568 images with up to 64 megapixels. Extensive experiments show that HiDA-Net achieves state-of-the-art, increasing accuracy by over 13% on the challenging Chameleon dataset and 10% on our HiRes-50K.
<div id='section'>Paperid: <span id='pid'>93, <a href='https://arxiv.org/pdf/2508.09487.pdf' target='_blank'>https://arxiv.org/pdf/2508.09487.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ju Yeon Kang, Jaehong Park, Semin Kim, Ji Won Yoon, Nam Soo Kim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.09487">Semantic-Aware Reconstruction Error for Detecting AI-Generated Images</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recently, AI-generated image detection has gained increasing attention, as the rapid advancement of image generation technologies has raised serious concerns about their potential misuse. While existing detection methods have achieved promising results, their performance often degrades significantly when facing fake images from unseen, out-of-distribution (OOD) generative models, since they primarily rely on model-specific artifacts and thus overfit to the models used for training. To address this limitation, we propose a novel representation, namely Semantic-Aware Reconstruction Error (SARE), that measures the semantic difference between an image and its caption-guided reconstruction. The key hypothesis behind SARE is that real images, whose captions often fail to fully capture their complex visual content, may undergo noticeable semantic shifts during the caption-guided reconstruction process. In contrast, fake images, which closely align with their captions, show minimal semantic changes. By quantifying these semantic shifts, SARE provides a robust and discriminative feature for detecting fake images across diverse generative models. Additionally, we introduce a fusion module that integrates SARE into the backbone detector via a cross-attention mechanism. Image features attend to semantic representations extracted from SARE, enabling the model to adaptively leverage semantic information. Experimental results demonstrate that the proposed method achieves strong generalization, outperforming existing baselines on benchmarks including GenImage and ForenSynths. We further validate the effectiveness of caption guidance through a detailed analysis of semantic shifts, confirming its ability to enhance detection robustness.
<div id='section'>Paperid: <span id='pid'>94, <a href='https://arxiv.org/pdf/2411.11016.pdf' target='_blank'>https://arxiv.org/pdf/2411.11016.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ziyue Zeng, Haoyuan Liu, Dingjie Peng, Luoxu Jing, Hiroshi Watanabe
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.11016">Time Step Generating: A Universal Synthesized Deepfake Image Detector</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Currently, high-fidelity text-to-image models are developed in an accelerating pace. Among them, Diffusion Models have led to a remarkable improvement in the quality of image generation, making it vary challenging to distinguish between real and synthesized images. It simultaneously raises serious concerns regarding privacy and security. Some methods are proposed to distinguish the diffusion model generated images through reconstructing. However, the inversion and denoising processes are time-consuming and heavily reliant on the pre-trained generative model. Consequently, if the pre-trained generative model meet the problem of out-of-domain, the detection performance declines. To address this issue, we propose a universal synthetic image detector Time Step Generating (TSG), which does not rely on pre-trained models' reconstructing ability, specific datasets, or sampling algorithms. Our method utilizes a pre-trained diffusion model's network as a feature extractor to capture fine-grained details, focusing on the subtle differences between real and synthetic images. By controlling the time step t of the network input, we can effectively extract these distinguishing detail features. Then, those features can be passed through a classifier (i.e. Resnet), which efficiently detects whether an image is synthetic or real. We test the proposed TSG on the large-scale GenImage benchmark and it achieves significant improvements in both accuracy and generalizability.
<div id='section'>Paperid: <span id='pid'>95, <a href='https://arxiv.org/pdf/2511.19499.pdf' target='_blank'>https://arxiv.org/pdf/2511.19499.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hong-Hanh Nguyen-Le, Van-Tuan Tran, Dinh-Thuc Nguyen, Nhien-An Le-Khac
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.19499">Beyond Binary Classification: A Semi-supervised Approach to Generalized AI-generated Image Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The rapid advancement of generators (e.g., StyleGAN, Midjourney, DALL-E) has produced highly realistic synthetic images, posing significant challenges to digital media authenticity. These generators are typically based on a few core architectural families, primarily Generative Adversarial Networks (GANs) and Diffusion Models (DMs). A critical vulnerability in current forensics is the failure of detectors to achieve cross-generator generalization, especially when crossing architectural boundaries (e.g., from GANs to DMs). We hypothesize that this gap stems from fundamental differences in the artifacts produced by these \textbf{distinct architectures}. In this work, we provide a theoretical analysis explaining how the distinct optimization objectives of the GAN and DM architectures lead to different manifold coverage behaviors. We demonstrate that GANs permit partial coverage, often leading to boundary artifacts, while DMs enforce complete coverage, resulting in over-smoothing patterns. Motivated by this analysis, we propose the \textbf{Tri}archy \textbf{Detect}or (TriDetect), a semi-supervised approach that enhances binary classification by discovering latent architectural patterns within the "fake" class. TriDetect employs balanced cluster assignment via the Sinkhorn-Knopp algorithm and a cross-view consistency mechanism, encouraging the model to learn fundamental architectural distincts. We evaluate our approach on two standard benchmarks and three in-the-wild datasets against 13 baselines to demonstrate its generalization capability to unseen generators.
<div id='section'>Paperid: <span id='pid'>96, <a href='https://arxiv.org/pdf/2509.19841.pdf' target='_blank'>https://arxiv.org/pdf/2509.19841.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tai-Ming Huang, Wei-Tung Lin, Kai-Lung Hua, Wen-Huang Cheng, Junichi Yamagishi, Jun-Cheng Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.19841">ThinkFake: Reasoning in Multimodal Large Language Models for AI-Generated Image Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The increasing realism of AI-generated images has raised serious concerns about misinformation and privacy violations, highlighting the urgent need for accurate and interpretable detection methods. While existing approaches have made progress, most rely on binary classification without explanations or depend heavily on supervised fine-tuning, resulting in limited generalization. In this paper, we propose ThinkFake, a novel reasoning-based and generalizable framework for AI-generated image detection. Our method leverages a Multimodal Large Language Model (MLLM) equipped with a forgery reasoning prompt and is trained using Group Relative Policy Optimization (GRPO) reinforcement learning with carefully designed reward functions. This design enables the model to perform step-by-step reasoning and produce interpretable, structured outputs. We further introduce a structured detection pipeline to enhance reasoning quality and adaptability. Extensive experiments show that ThinkFake outperforms state-of-the-art methods on the GenImage benchmark and demonstrates strong zero-shot generalization on the challenging LOKI benchmark. These results validate our framework's effectiveness and robustness. Code will be released upon acceptance.
<div id='section'>Paperid: <span id='pid'>97, <a href='https://arxiv.org/pdf/2507.13224.pdf' target='_blank'>https://arxiv.org/pdf/2507.13224.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Keerthi Veeramachaneni, Praveen Tirupattur, Amrit Singh Bedi, Mubarak Shah
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.13224">Leveraging Pre-Trained Visual Models for AI-Generated Video Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in Generative AI (GenAI) have led to significant improvements in the quality of generated visual content. As AI-generated visual content becomes increasingly indistinguishable from real content, the challenge of detecting the generated content becomes critical in combating misinformation, ensuring privacy, and preventing security threats. Although there has been substantial progress in detecting AI-generated images, current methods for video detection are largely focused on deepfakes, which primarily involve human faces. However, the field of video generation has advanced beyond DeepFakes, creating an urgent need for methods capable of detecting AI-generated videos with generic content. To address this gap, we propose a novel approach that leverages pre-trained visual models to distinguish between real and generated videos. The features extracted from these pre-trained models, which have been trained on extensive real visual content, contain inherent signals that can help distinguish real from generated videos. Using these extracted features, we achieve high detection performance without requiring additional model training, and we further improve performance by training a simple linear classification layer on top of the extracted features. We validated our method on a dataset we compiled (VID-AID), which includes around 10,000 AI-generated videos produced by 9 different text-to-video models, along with 4,000 real videos, totaling over 7 hours of video content. Our evaluation shows that our approach achieves high detection accuracy, above 90% on average, underscoring its effectiveness. Upon acceptance, we plan to publicly release the code, the pre-trained models, and our dataset to support ongoing research in this critical area.
<div id='section'>Paperid: <span id='pid'>98, <a href='https://arxiv.org/pdf/2505.18035.pdf' target='_blank'>https://arxiv.org/pdf/2505.18035.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Naseem Khan, Tuan Nguyen, Amine Bermak, Issa Khalil
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.18035">CAMME: Adaptive Deepfake Image Detection with Multi-Modal Cross-Attention</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The proliferation of sophisticated AI-generated deepfakes poses critical challenges for digital media authentication and societal security. While existing detection methods perform well within specific generative domains, they exhibit significant performance degradation when applied to manipulations produced by unseen architectures--a fundamental limitation as generative technologies rapidly evolve. We propose CAMME (Cross-Attention Multi-Modal Embeddings), a framework that dynamically integrates visual, textual, and frequency-domain features through a multi-head cross-attention mechanism to establish robust cross-domain generalization. Extensive experiments demonstrate CAMME's superiority over state-of-the-art methods, yielding improvements of 12.56% on natural scenes and 13.25% on facial deepfakes. The framework demonstrates exceptional resilience, maintaining (over 91%) accuracy under natural image perturbations and achieving 89.01% and 96.14% accuracy against PGD and FGSM adversarial attacks, respectively. Our findings validate that integrating complementary modalities through cross-attention enables more effective decision boundary realignment for reliable deepfake detection across heterogeneous generative architectures.
<div id='section'>Paperid: <span id='pid'>99, <a href='https://arxiv.org/pdf/2504.19212.pdf' target='_blank'>https://arxiv.org/pdf/2504.19212.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tuan Nguyen, Naseem Khan, Issa Khalil
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.19212">CapsFake: A Multimodal Capsule Network for Detecting Instruction-Guided Deepfakes</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The rapid evolution of deepfake technology, particularly in instruction-guided image editing, threatens the integrity of digital images by enabling subtle, context-aware manipulations. Generated conditionally from real images and textual prompts, these edits are often imperceptible to both humans and existing detection systems, revealing significant limitations in current defenses. We propose a novel multimodal capsule network, CapsFake, designed to detect such deepfake image edits by integrating low-level capsules from visual, textual, and frequency-domain modalities. High-level capsules, predicted through a competitive routing mechanism, dynamically aggregate local features to identify manipulated regions with precision. Evaluated on diverse datasets, including MagicBrush, Unsplash Edits, Open Images Edits, and Multi-turn Edits, CapsFake outperforms state-of-the-art methods by up to 20% in detection accuracy. Ablation studies validate its robustness, achieving detection rates above 94% under natural perturbations and 96% against adversarial attacks, with excellent generalization to unseen editing scenarios. This approach establishes a powerful framework for countering sophisticated image manipulations.
<div id='section'>Paperid: <span id='pid'>100, <a href='https://arxiv.org/pdf/2502.17105.pdf' target='_blank'>https://arxiv.org/pdf/2502.17105.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Seoyeon Gye, Junwon Ko, Hyounguk Shon, Minchan Kwon, Junmo Kim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.17105">SFLD: Reducing the content bias for AI-generated Image Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Identifying AI-generated content is critical for the safe and ethical use of generative AI. Recent research has focused on developing detectors that generalize to unknown generators, with popular methods relying either on high-level features or low-level fingerprints. However, these methods have clear limitations: biased towards unseen content, or vulnerable to common image degradations, such as JPEG compression. To address these issues, we propose a novel approach, SFLD, which incorporates PatchShuffle to integrate high-level semantic and low-level textural information. SFLD applies PatchShuffle at multiple levels, improving robustness and generalization across various generative models. Additionally, current benchmarks face challenges such as low image quality, insufficient content preservation, and limited class diversity. In response, we introduce TwinSynths, a new benchmark generation methodology that constructs visually near-identical pairs of real and synthetic images to ensure high quality and content preservation. Our extensive experiments and analysis show that SFLD outperforms existing methods on detecting a wide variety of fake images sourced from GANs, diffusion models, and TwinSynths, demonstrating the state-of-the-art performance and generalization capabilities to novel generative models.
<div id='section'>Paperid: <span id='pid'>101, <a href='https://arxiv.org/pdf/2512.21695.pdf' target='_blank'>https://arxiv.org/pdf/2512.21695.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Md. Zahid Hossain, Most. Sharmin Sultana Samu, Md. Kamrozzaman Bhuiyan, Farhad Uz Zaman, Md. Rakibul Islam
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.21695">FUSE: Unifying Spectral and Semantic Cues for Robust AI-Generated Image Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The fast evolution of generative models has heightened the demand for reliable detection of AI-generated images. To tackle this challenge, we introduce FUSE, a hybrid system that combines spectral features extracted through Fast Fourier Transform with semantic features obtained from the CLIP's Vision encoder. The features are fused into a joint representation and trained progressively in two stages. Evaluations on GenImage, WildFake, DiTFake, GPT-ImgEval and Chameleon datasets demonstrate strong generalization across multiple generators. Our FUSE (Stage 1) model demonstrates state-of-the-art results on the Chameleon benchmark. It also attains 91.36% mean accuracy on the GenImage dataset, 88.71% accuracy across all tested generators, and a mean Average Precision of 94.96%. Stage 2 training further improves performance for most generators. Unlike existing methods, which often perform poorly on high-fidelity images in Chameleon, our approach maintains robustness across diverse generators. These findings highlight the benefits of integrating spectral and semantic features for generalized detection of images generated by AI.
<div id='section'>Paperid: <span id='pid'>102, <a href='https://arxiv.org/pdf/2508.03967.pdf' target='_blank'>https://arxiv.org/pdf/2508.03967.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mamadou Keita, Wassim Hamidouche, Hessen Bougueffa Eutamene, Abdelmalik Taleb-Ahmed, Abdenour Hadid
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.03967">RAVID: Retrieval-Augmented Visual Detection: A Knowledge-Driven Approach for AI-Generated Image Identification</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we introduce RAVID, the first framework for AI-generated image detection that leverages visual retrieval-augmented generation (RAG). While RAG methods have shown promise in mitigating factual inaccuracies in foundation models, they have primarily focused on text, leaving visual knowledge underexplored. Meanwhile, existing detection methods, which struggle with generalization and robustness, often rely on low-level artifacts and model-specific features, limiting their adaptability. To address this, RAVID dynamically retrieves relevant images to enhance detection. Our approach utilizes a fine-tuned CLIP image encoder, RAVID CLIP, enhanced with category-related prompts to improve representation learning. We further integrate a vision-language model (VLM) to fuse retrieved images with the query, enriching the input and improving accuracy. Given a query image, RAVID generates an embedding using RAVID CLIP, retrieves the most relevant images from a database, and combines these with the query image to form an enriched input for a VLM (e.g., Qwen-VL or Openflamingo). Experiments on the UniversalFakeDetect benchmark, which covers 19 generative models, show that RAVID achieves state-of-the-art performance with an average accuracy of 93.85%. RAVID also outperforms traditional methods in terms of robustness, maintaining high accuracy even under image degradations such as Gaussian blur and JPEG compression. Specifically, RAVID achieves an average accuracy of 80.27% under degradation conditions, compared to 63.44% for the state-of-the-art model C2P-CLIP, demonstrating consistent improvements in both Gaussian blur and JPEG compression scenarios. The code will be publicly available upon acceptance.
<div id='section'>Paperid: <span id='pid'>103, <a href='https://arxiv.org/pdf/2507.00583.pdf' target='_blank'>https://arxiv.org/pdf/2507.00583.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Christian InternÃ², Robert Geirhos, Markus Olhofer, Sunny Liu, Barbara Hammer, David Klindt
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.00583">AI-Generated Video Detection via Perceptual Straightening</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The rapid advancement of generative AI enables highly realistic synthetic videos, posing significant challenges for content authentication and raising urgent concerns about misuse. Existing detection methods often struggle with generalization and capturing subtle temporal inconsistencies. We propose ReStraV(Representation Straightening Video), a novel approach to distinguish natural from AI-generated videos. Inspired by the "perceptual straightening" hypothesis -- which suggests real-world video trajectories become more straight in neural representation domain -- we analyze deviations from this expected geometric property. Using a pre-trained self-supervised vision transformer (DINOv2), we quantify the temporal curvature and stepwise distance in the model's representation domain. We aggregate statistics of these measures for each video and train a classifier. Our analysis shows that AI-generated videos exhibit significantly different curvature and distance patterns compared to real videos. A lightweight classifier achieves state-of-the-art detection performance (e.g., 97.17% accuracy and 98.63% AUROC on the VidProM benchmark), substantially outperforming existing image- and video-based methods. ReStraV is computationally efficient, it is offering a low-cost and effective detection solution. This work provides new insights into using neural representation geometry for AI-generated video detection.
<div id='section'>Paperid: <span id='pid'>104, <a href='https://arxiv.org/pdf/2504.13726.pdf' target='_blank'>https://arxiv.org/pdf/2504.13726.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lin Yuan, Xiaowan Li, Yan Zhang, Jiawei Zhang, Hongbo Li, Xinbo Gao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.13726">MLEP: Multi-granularity Local Entropy Patterns for Universal AI-generated Image Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Advancements in image generation technologies have raised significant concerns about their potential misuse, such as producing misinformation and deepfakes. Therefore, there is an urgent need for effective methods to detect AI-generated images (AIGI). Despite progress in AIGI detection, achieving reliable performance across diverse generation models and scenes remains challenging due to the lack of source-invariant features and limited generalization capabilities in existing methods. In this work, we explore the potential of using image entropy as a cue for AIGI detection and propose Multi-granularity Local Entropy Patterns (MLEP), a set of entropy feature maps computed across shuffled small patches over multiple image scaled. MLEP comprehensively captures pixel relationships across dimensions and scales while significantly disrupting image semantics, reducing potential content bias. Leveraging MLEP, a robust CNN-based classifier for AIGI detection can be trained. Extensive experiments conducted in an open-world scenario, evaluating images synthesized by 32 distinct generative models, demonstrate significant improvements over state-of-the-art methods in both accuracy and generalization.
<div id='section'>Paperid: <span id='pid'>105, <a href='https://arxiv.org/pdf/2502.10682.pdf' target='_blank'>https://arxiv.org/pdf/2502.10682.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kafi Anan, Anindya Bhattacharjee, Ashir Intesher, Kaidul Islam, Abrar Assaeem Fuad, Utsab Saha, Hafiz Imtiaz
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.10682">CAE-Net: Generalized Deepfake Image Detection using Convolution and Attention Mechanisms with Spatial and Frequency Domain Features</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Effective deepfake detection tools are becoming increasingly essential to the growing usage of deepfakes in unethical practices. There exists a wide range of deepfake generation techniques, which makes it challenging to develop an accurate universal detection mechanism. The 2025 IEEE Signal Processing Cup (\textit{DFWild-Cup} competition) provided a diverse dataset of deepfake images containing significant class imbalance. The images in the dataset are generated from multiple deepfake image generators, for training machine learning model(s) to emphasize the generalization of deepfake detection. To this end, we proposed a disjoint set-based multistage training method to address the class imbalance and devised an ensemble-based architecture \emph{CAE-Net}. Our architecture consists of a convolution- and attention-based ensemble network, and employs three different neural network architectures: EfficientNet, Data-Efficient Image Transformer (DeiT), and ConvNeXt with wavelet transform to capture both local and global features of deepfakes. We visualize the specific regions that these models focus on for classification using Grad-CAM, and empirically demonstrate the effectiveness of these models in grouping real and fake images into cohesive clusters using t-SNE plots. Individually, the EfficientNet B0 architecture has achieved 90.79\% accuracy, whereas the ConvNeXt and the DeiT architecture have achieved 89.49\% and 89.32\% accuracy, respectively. With these networks, our weighted ensemble model achieves an excellent accuracy of 94.63\% on the validation dataset of the SP Cup 2025 competition. The equal error rate of 4.72\% and the Area Under the ROC curve of 97.37\% further confirm the stability of our proposed method. Finally, the robustness of our proposed model against adversarial perturbation attacks is tested as well, showing the inherent defensive properties of the ensemble approach.
<div id='section'>Paperid: <span id='pid'>106, <a href='https://arxiv.org/pdf/2406.02951.pdf' target='_blank'>https://arxiv.org/pdf/2406.02951.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Trevine Oorloff, Surya Koppisetti, NicolÃ² Bonettini, Divyaraj Solanki, Ben Colman, Yaser Yacoob, Ali Shahriyari, Gaurav Bharaj
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.02951">AVFF: Audio-Visual Feature Fusion for Video Deepfake Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the rapid growth in deepfake video content, we require improved and generalizable methods to detect them. Most existing detection methods either use uni-modal cues or rely on supervised training to capture the dissonance between the audio and visual modalities. While the former disregards the audio-visual correspondences entirely, the latter predominantly focuses on discerning audio-visual cues within the training corpus, thereby potentially overlooking correspondences that can help detect unseen deepfakes. We present Audio-Visual Feature Fusion (AVFF), a two-stage cross-modal learning method that explicitly captures the correspondence between the audio and visual modalities for improved deepfake detection. The first stage pursues representation learning via self-supervision on real videos to capture the intrinsic audio-visual correspondences. To extract rich cross-modal representations, we use contrastive learning and autoencoding objectives, and introduce a novel audio-visual complementary masking and feature fusion strategy. The learned representations are tuned in the second stage, where deepfake classification is pursued via supervised learning on both real and fake videos. Extensive experiments and analysis suggest that our novel representation learning paradigm is highly discriminative in nature. We report 98.6% accuracy and 99.1% AUC on the FakeAVCeleb dataset, outperforming the current audio-visual state-of-the-art by 14.9% and 9.9%, respectively.
<div id='section'>Paperid: <span id='pid'>107, <a href='https://arxiv.org/pdf/2512.17350.pdf' target='_blank'>https://arxiv.org/pdf/2512.17350.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chenming Zhou, Jiaan Wang, Yu Li, Lei Li, Juan Cao, Sheng Tang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.17350">Beyond Semantic Features: Pixel-level Mapping for Generalized AI-Generated Image Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The rapid evolution of generative technologies necessitates reliable methods for detecting AI-generated images. A critical limitation of current detectors is their failure to generalize to images from unseen generative models, as they often overfit to source-specific semantic cues rather than learning universal generative artifacts. To overcome this, we introduce a simple yet remarkably effective pixel-level mapping pre-processing step to disrupt the pixel value distribution of images and break the fragile, non-essential semantic patterns that detectors commonly exploit as shortcuts. This forces the detector to focus on more fundamental and generalizable high-frequency traces inherent to the image generation process. Through comprehensive experiments on GAN and diffusion-based generators, we show that our approach significantly boosts the cross-generator performance of state-of-the-art detectors. Extensive analysis further verifies our hypothesis that the disruption of semantic cues is the key to generalization.
<div id='section'>Paperid: <span id='pid'>108, <a href='https://arxiv.org/pdf/2511.02791.pdf' target='_blank'>https://arxiv.org/pdf/2511.02791.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Nusrat Tasnim, Kutub Uddin, Khalid Mahmood Malik
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.02791">AI-Generated Image Detection: An Empirical Study and Future Research Directions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The threats posed by AI-generated media, particularly deepfakes, are now raising significant challenges for multimedia forensics, misinformation detection, and biometric system resulting in erosion of public trust in the legal system, significant increase in frauds, and social engineering attacks. Although several forensic methods have been proposed, they suffer from three critical gaps: (i) use of non-standardized benchmarks with GAN- or diffusion-generated images, (ii) inconsistent training protocols (e.g., scratch, frozen, fine-tuning), and (iii) limited evaluation metrics that fail to capture generalization and explainability. These limitations hinder fair comparison, obscure true robustness, and restrict deployment in security-critical applications. This paper introduces a unified benchmarking framework for systematic evaluation of forensic methods under controlled and reproducible conditions. We benchmark ten SoTA forensic methods (scratch, frozen, and fine-tuned) and seven publicly available datasets (GAN and diffusion) to perform extensive and systematic evaluations. We evaluate performance using multiple metrics, including accuracy, average precision, ROC-AUC, error rate, and class-wise sensitivity. We also further analyze model interpretability using confidence curves and Grad-CAM heatmaps. Our evaluations demonstrate substantial variability in generalization, with certain methods exhibiting strong in-distribution performance but degraded cross-model transferability. This study aims to guide the research community toward a deeper understanding of the strengths and limitations of current forensic approaches, and to inspire the development of more robust, generalizable, and explainable solutions.
<div id='section'>Paperid: <span id='pid'>109, <a href='https://arxiv.org/pdf/2508.11464.pdf' target='_blank'>https://arxiv.org/pdf/2508.11464.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaoya Zhu, Yibing Nan, Shiguo Lian
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.11464">Data-Driven Deepfake Image Detection Method -- The 2024 Global Deepfake Image Detection Challenge</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the rapid development of technology in the field of AI, deepfake technology has emerged as a double-edged sword. It has not only created a large amount of AI-generated content but also posed unprecedented challenges to digital security. The task of the competition is to determine whether a face image is a Deepfake image and output its probability score of being a Deepfake image. In the image track competition, our approach is based on the Swin Transformer V2-B classification network. And online data augmentation and offline sample generation methods are employed to enrich the diversity of training samples and increase the generalization ability of the model. Finally, we got the award of excellence in Deepfake image detection.
<div id='section'>Paperid: <span id='pid'>110, <a href='https://arxiv.org/pdf/2508.08141.pdf' target='_blank'>https://arxiv.org/pdf/2508.08141.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Nicholas Klein, Hemlata Tak, James Fullwood, Krishna Regmi, Leonidas Spinoulas, Ganesh Sivaraman, Tianxiang Chen, Elie Khoury
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.08141">Pindrop it! Audio and Visual Deepfake Countermeasures for Robust Detection and Fine Grained-Localization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The field of visual and audio generation is burgeoning with new state-of-the-art methods. This rapid proliferation of new techniques underscores the need for robust solutions for detecting synthetic content in videos. In particular, when fine-grained alterations via localized manipulations are performed in visual, audio, or both domains, these subtle modifications add challenges to the detection algorithms. This paper presents solutions for the problems of deepfake video classification and localization. The methods were submitted to the ACM 1M Deepfakes Detection Challenge, achieving the best performance in the temporal localization task and a top four ranking in the classification task for the TestA split of the evaluation dataset.
<div id='section'>Paperid: <span id='pid'>111, <a href='https://arxiv.org/pdf/2503.14853.pdf' target='_blank'>https://arxiv.org/pdf/2503.14853.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Peipeng Yu, Jianwei Fei, Hui Gao, Xuan Feng, Zhihua Xia, Chip Hong Chang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.14853">Unlocking the Capabilities of Large Vision-Language Models for Generalizable and Explainable Deepfake Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Current Large Vision-Language Models (LVLMs) have demonstrated remarkable capabilities in understanding multimodal data, but their potential remains underexplored for deepfake detection due to the misalignment of their knowledge and forensics patterns. To this end, we present a novel framework that unlocks LVLMs' potential capabilities for deepfake detection. Our framework includes a Knowledge-guided Forgery Detector (KFD), a Forgery Prompt Learner (FPL), and a Large Language Model (LLM). The KFD is used to calculate correlations between image features and pristine/deepfake image description embeddings, enabling forgery classification and localization. The outputs of the KFD are subsequently processed by the Forgery Prompt Learner to construct fine-grained forgery prompt embeddings. These embeddings, along with visual and question prompt embeddings, are fed into the LLM to generate textual detection responses. Extensive experiments on multiple benchmarks, including FF++, CDF2, DFD, DFDCP, DFDC, and DF40, demonstrate that our scheme surpasses state-of-the-art methods in generalization performance, while also supporting multi-turn dialogue capabilities.
<div id='section'>Paperid: <span id='pid'>112, <a href='https://arxiv.org/pdf/2412.07260.pdf' target='_blank'>https://arxiv.org/pdf/2412.07260.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Peipeng Yu, Hui Gao, Jianwei Fei, Zhitao Huang, Zhihua Xia, Chip-Hong Chang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.07260">DFREC: DeepFake Identity Recovery Based on Identity-aware Masked Autoencoder</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in deepfake forensics have primarily focused on improving the classification accuracy and generalization performance. Despite enormous progress in detection accuracy across a wide variety of forgery algorithms, existing algorithms lack intuitive interpretability and identity traceability to help with forensic investigation. In this paper, we introduce a novel DeepFake Identity Recovery scheme (DFREC) to fill this gap. DFREC aims to recover the pair of source and target faces from a deepfake image to facilitate deepfake identity tracing and reduce the risk of deepfake attack. It comprises three key components: an Identity Segmentation Module (ISM), a Source Identity Reconstruction Module (SIRM), and a Target Identity Reconstruction Module (TIRM). The ISM segments the input face into distinct source and target face information, and the SIRM reconstructs the source face and extracts latent target identity features with the segmented source information. The background context and latent target identity features are synergetically fused by a Masked Autoencoder in the TIRM to reconstruct the target face. We evaluate DFREC on six different high-fidelity face-swapping attacks on FaceForensics++, CelebaMegaFS and FFHQ-E4S datasets, which demonstrate its superior recovery performance over state-of-the-art deepfake recovery algorithms. In addition, DFREC is the only scheme that can recover both pristine source and target faces directly from the forgery image with high fadelity.
<div id='section'>Paperid: <span id='pid'>113, <a href='https://arxiv.org/pdf/2404.18149.pdf' target='_blank'>https://arxiv.org/pdf/2404.18149.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zongmei Chen, Xin Liao, Xiaoshuai Wu, Yanxiang Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.18149">Compressed Deepfake Video Detection Based on 3D Spatiotemporal Trajectories</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The misuse of deepfake technology by malicious actors poses a potential threat to nations, societies, and individuals. However, existing methods for detecting deepfakes primarily focus on uncompressed videos, such as noise characteristics, local textures, or frequency statistics. When applied to compressed videos, these methods experience a decrease in detection performance and are less suitable for real-world scenarios. In this paper, we propose a deepfake video detection method based on 3D spatiotemporal trajectories. Specifically, we utilize a robust 3D model to construct spatiotemporal motion features, integrating feature details from both 2D and 3D frames to mitigate the influence of large head rotation angles or insufficient lighting within frames. Furthermore, we separate facial expressions from head movements and design a sequential analysis method based on phase space motion trajectories to explore the feature differences between genuine and fake faces in deepfake videos. We conduct extensive experiments to validate the performance of our proposed method on several compressed deepfake benchmarks. The robustness of the well-designed features is verified by calculating the consistent distribution of facial landmarks before and after video compression.Our method yields satisfactory results and showcases its potential for practical applications.
<div id='section'>Paperid: <span id='pid'>114, <a href='https://arxiv.org/pdf/2512.18527.pdf' target='_blank'>https://arxiv.org/pdf/2512.18527.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Rahul Yumlembam, Biju Issac, Nauman Aslam, Eaby Kollonoor Babu, Josh Collyer, Fraser Kennedy
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.18527">Detection of AI Generated Images Using Combined Uncertainty Measures and Particle Swarm Optimised Rejection Mechanism</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As AI-generated images become increasingly photorealistic, distinguishing them from natural images poses a growing challenge. This paper presents a robust detection framework that leverages multiple uncertainty measures to decide whether to trust or reject a model's predictions. We focus on three complementary techniques: Fisher Information, which captures the sensitivity of model parameters to input variations; entropy-based uncertainty from Monte Carlo Dropout, which reflects predictive variability; and predictive variance from a Deep Kernel Learning framework using a Gaussian Process classifier. To integrate these diverse uncertainty signals, Particle Swarm Optimisation is used to learn optimal weightings and determine an adaptive rejection threshold. The model is trained on Stable Diffusion-generated images and evaluated on GLIDE, VQDM, Midjourney, BigGAN, and StyleGAN3, each introducing significant distribution shifts. While standard metrics such as prediction probability and Fisher-based measures perform well in distribution, their effectiveness degrades under shift. In contrast, the Combined Uncertainty measure consistently achieves an incorrect rejection rate of approximately 70 percent on unseen generators, successfully filtering most misclassified AI samples. Although the system occasionally rejects correct predictions from newer generators, this conservative behaviour is acceptable, as rejected samples can support retraining. The framework maintains high acceptance of accurate predictions for natural images and in-domain AI data. Under adversarial attacks using FGSM and PGD, the Combined Uncertainty method rejects around 61 percent of successful attacks, while GP-based uncertainty alone achieves up to 80 percent. Overall, the results demonstrate that multi-source uncertainty fusion provides a resilient and adaptive solution for AI-generated image detection.
<div id='section'>Paperid: <span id='pid'>115, <a href='https://arxiv.org/pdf/2511.14030.pdf' target='_blank'>https://arxiv.org/pdf/2511.14030.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sungik Choi, Hankook Lee, Moontae Lee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.14030">Training-free Detection of AI-generated images via Cropping Robustness</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>AI-generated image detection has become crucial with the rapid advancement of vision-generative models. Instead of training detectors tailored to specific datasets, we study a training-free approach leveraging self-supervised models without requiring prior data knowledge. These models, pre-trained with augmentations like RandomResizedCrop, learn to produce consistent representations across varying resolutions. Motivated by this, we propose WaRPAD, a training-free AI-generated image detection algorithm based on self-supervised models. Since neighborhood pixel differences in images are highly sensitive to resizing operations, WaRPAD first defines a base score function that quantifies the sensitivity of image embeddings to perturbations along high-frequency directions extracted via Haar wavelet decomposition. To simulate robustness against cropping augmentation, we rescale each image to a multiple of the models input size, divide it into smaller patches, and compute the base score for each patch. The final detection score is then obtained by averaging the scores across all patches. We validate WaRPAD on real datasets of diverse resolutions and domains, and images generated by 23 different generative models. Our method consistently achieves competitive performance and demonstrates strong robustness to test-time corruptions. Furthermore, as invariance to RandomResizedCrop is a common training scheme across self-supervised models, we show that WaRPAD is applicable across self-supervised models.
<div id='section'>Paperid: <span id='pid'>116, <a href='https://arxiv.org/pdf/2505.03859.pdf' target='_blank'>https://arxiv.org/pdf/2505.03859.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Will Hawkins, Chris Russell, Brent Mittelstadt
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.03859">Deepfakes on Demand: the rise of accessible non-consensual deepfake image generators</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Advances in multimodal machine learning have made text-to-image (T2I) models increasingly accessible and popular. However, T2I models introduce risks such as the generation of non-consensual depictions of identifiable individuals, otherwise known as deepfakes. This paper presents an empirical study exploring the accessibility of deepfake model variants online. Through a metadata analysis of thousands of publicly downloadable model variants on two popular repositories, Hugging Face and Civitai, we demonstrate a huge rise in easily accessible deepfake models. Almost 35,000 examples of publicly downloadable deepfake model variants are identified, primarily hosted on Civitai. These deepfake models have been downloaded almost 15 million times since November 2022, with the models targeting a range of individuals from global celebrities to Instagram users with under 10,000 followers. Both Stable Diffusion and Flux models are used for the creation of deepfake models, with 96% of these targeting women and many signalling intent to generate non-consensual intimate imagery (NCII). Deepfake model variants are often created via the parameter-efficient fine-tuning technique known as low rank adaptation (LoRA), requiring as few as 20 images, 24GB VRAM, and 15 minutes of time, making this process widely accessible via consumer-grade computers. Despite these models violating the Terms of Service of hosting platforms, and regulation seeking to prevent dissemination, these results emphasise the pressing need for greater action to be taken against the creation of deepfakes and NCII.
<div id='section'>Paperid: <span id='pid'>117, <a href='https://arxiv.org/pdf/2412.20704.pdf' target='_blank'>https://arxiv.org/pdf/2412.20704.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sungik Choi, Sungwoo Park, Jaehoon Lee, Seunghyun Kim, Stanley Jungkyu Choi, Moontae Lee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.20704">HFI: A unified framework for training-free detection and implicit watermarking of latent diffusion model generated images</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Dramatic advances in the quality of the latent diffusion models (LDMs) also led to the malicious use of AI-generated images. While current AI-generated image detection methods assume the availability of real/AI-generated images for training, this is practically limited given the vast expressibility of LDMs. This motivates the training-free detection setup where no related data are available in advance. The existing LDM-generated image detection method assumes that images generated by LDM are easier to reconstruct using an autoencoder than real images. However, we observe that this reconstruction distance is overfitted to background information, leading the current method to underperform in detecting images with simple backgrounds. To address this, we propose a novel method called HFI. Specifically, by viewing the autoencoder of LDM as a downsampling-upsampling kernel, HFI measures the extent of aliasing, a distortion of high-frequency information that appears in the reconstructed image. HFI is training-free, efficient, and consistently outperforms other training-free methods in detecting challenging images generated by various generative models. We also show that HFI can successfully detect the images generated from the specified LDM as a means of implicit watermarking. HFI outperforms the best baseline method while achieving magnitudes of
<div id='section'>Paperid: <span id='pid'>118, <a href='https://arxiv.org/pdf/2410.23663.pdf' target='_blank'>https://arxiv.org/pdf/2410.23663.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Fan Nie, Jiangqun Ni, Jian Zhang, Bin Zhang, Weizhe Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.23663">DIP: Diffusion Learning of Inconsistency Pattern for General DeepFake Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the advancement of deepfake generation techniques, the importance of deepfake detection in protecting multimedia content integrity has become increasingly obvious. Recently, temporal inconsistency clues have been explored to improve the generalizability of deepfake video detection. According to our observation, the temporal artifacts of forged videos in terms of motion information usually exhibits quite distinct inconsistency patterns along horizontal and vertical directions, which could be leveraged to improve the generalizability of detectors. In this paper, a transformer-based framework for Diffusion Learning of Inconsistency Pattern (DIP) is proposed, which exploits directional inconsistencies for deepfake video detection. Specifically, DIP begins with a spatiotemporal encoder to represent spatiotemporal information. A directional inconsistency decoder is adopted accordingly, where direction-aware attention and inconsistency diffusion are incorporated to explore potential inconsistency patterns and jointly learn the inherent relationships. In addition, the SpatioTemporal Invariant Loss (STI Loss) is introduced to contrast spatiotemporally augmented sample pairs and prevent the model from overfitting nonessential forgery artifacts. Extensive experiments on several public datasets demonstrate that our method could effectively identify directional forgery clues and achieve state-of-the-art performance.
<div id='section'>Paperid: <span id='pid'>119, <a href='https://arxiv.org/pdf/2402.01123.pdf' target='_blank'>https://arxiv.org/pdf/2402.01123.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiaxuan Chen, Jieteng Yao, Li Niu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.01123">A Single Simple Patch is All You Need for AI-generated Image Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The recent development of generative models unleashes the potential of generating hyper-realistic fake images. To prevent the malicious usage of fake images, AI-generated image detection aims to distinguish fake images from real images. However, existing method suffer from severe performance drop when detecting images generated by unseen generators. We find that generative models tend to focus on generating the patches with rich textures to make the images more realistic while neglecting the hidden noise caused by camera capture present in simple patches. In this paper, we propose to exploit the noise pattern of a single simple patch to identify fake images. Furthermore, due to the performance decline when handling low-quality generated images, we introduce an enhancement module and a perception module to remove the interfering information. Extensive experiments demonstrate that our method can achieve state-of-the-art performance on public benchmarks.
<div id='section'>Paperid: <span id='pid'>120, <a href='https://arxiv.org/pdf/2309.05911.pdf' target='_blank'>https://arxiv.org/pdf/2309.05911.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Binh M. Le, Simon S. Woo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.05911">Quality-Agnostic Deepfake Detection with Intra-model Collaborative Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deepfake has recently raised a plethora of societal concerns over its possible security threats and dissemination of fake information. Much research on deepfake detection has been undertaken. However, detecting low quality as well as simultaneously detecting different qualities of deepfakes still remains a grave challenge. Most SOTA approaches are limited by using a single specific model for detecting certain deepfake video quality type. When constructing multiple models with prior information about video quality, this kind of strategy incurs significant computational cost, as well as model and training data overhead. Further, it cannot be scalable and practical to deploy in real-world settings. In this work, we propose a universal intra-model collaborative learning framework to enable the effective and simultaneous detection of different quality of deepfakes. That is, our approach is the quality-agnostic deepfake detection method, dubbed QAD . In particular, by observing the upper bound of general error expectation, we maximize the dependency between intermediate representations of images from different quality levels via Hilbert-Schmidt Independence Criterion. In addition, an Adversarial Weight Perturbation module is carefully devised to enable the model to be more robust against image corruption while boosting the overall model's performance. Extensive experiments over seven popular deepfake datasets demonstrate the superiority of our QAD model over prior SOTA benchmarks.
<div id='section'>Paperid: <span id='pid'>121, <a href='https://arxiv.org/pdf/2307.03990.pdf' target='_blank'>https://arxiv.org/pdf/2307.03990.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ganglai Wang, Peng Zhang, Junwen Xiong, Feihan Yang, Wei Huang, Yufei Zha
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.03990">FTFDNet: Learning to Detect Talking Face Video Manipulation with Tri-Modality Interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>DeepFake based digital facial forgery is threatening public media security, especially when lip manipulation has been used in talking face generation, and the difficulty of fake video detection is further improved. By only changing lip shape to match the given speech, the facial features of identity are hard to be discriminated in such fake talking face videos. Together with the lack of attention on audio stream as the prior knowledge, the detection failure of fake talking face videos also becomes inevitable. It's found that the optical flow of the fake talking face video is disordered especially in the lip region while the optical flow of the real video changes regularly, which means the motion feature from optical flow is useful to capture manipulation cues. In this study, a fake talking face detection network (FTFDNet) is proposed by incorporating visual, audio and motion features using an efficient cross-modal fusion (CMF) module. Furthermore, a novel audio-visual attention mechanism (AVAM) is proposed to discover more informative features, which can be seamlessly integrated into any audio-visual CNN architecture by modularization. With the additional AVAM, the proposed FTFDNet is able to achieve a better detection performance than other state-of-the-art DeepFake video detection methods not only on the established fake talking face detection dataset (FTFDD) but also on the DeepFake video detection datasets (DFDC and DF-TIMIT).
<div id='section'>Paperid: <span id='pid'>122, <a href='https://arxiv.org/pdf/2404.16212.pdf' target='_blank'>https://arxiv.org/pdf/2404.16212.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sifat Muhammad Abdullah, Aravind Cheruvu, Shravya Kanchi, Taejoong Chung, Peng Gao, Murtuza Jadliwala, Bimal Viswanath
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.16212">An Analysis of Recent Advances in Deepfake Image Detection in an Evolving Threat Landscape</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deepfake or synthetic images produced using deep generative models pose serious risks to online platforms. This has triggered several research efforts to accurately detect deepfake images, achieving excellent performance on publicly available deepfake datasets. In this work, we study 8 state-of-the-art detectors and argue that they are far from being ready for deployment due to two recent developments. First, the emergence of lightweight methods to customize large generative models, can enable an attacker to create many customized generators (to create deepfakes), thereby substantially increasing the threat surface. We show that existing defenses fail to generalize well to such \emph{user-customized generative models} that are publicly available today. We discuss new machine learning approaches based on content-agnostic features, and ensemble modeling to improve generalization performance against user-customized models. Second, the emergence of \textit{vision foundation models} -- machine learning models trained on broad data that can be easily adapted to several downstream tasks -- can be misused by attackers to craft adversarial deepfakes that can evade existing defenses. We propose a simple adversarial attack that leverages existing foundation models to craft adversarial samples \textit{without adding any adversarial noise}, through careful semantic manipulation of the image content. We highlight the vulnerabilities of several defenses against our attack, and explore directions leveraging advanced foundation models and adversarial training to defend against this new threat.
<div id='section'>Paperid: <span id='pid'>123, <a href='https://arxiv.org/pdf/2403.06592.pdf' target='_blank'>https://arxiv.org/pdf/2403.06592.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jongwook Choi, Taehoon Kim, Yonghyun Jeong, Seungryul Baek, Jongwon Choi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.06592">Exploiting Style Latent Flows for Generalizing Deepfake Video Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents a new approach for the detection of fake videos, based on the analysis of style latent vectors and their abnormal behavior in temporal changes in the generated videos. We discovered that the generated facial videos suffer from the temporal distinctiveness in the temporal changes of style latent vectors, which are inevitable during the generation of temporally stable videos with various facial expressions and geometric transformations. Our framework utilizes the StyleGRU module, trained by contrastive learning, to represent the dynamic properties of style latent vectors. Additionally, we introduce a style attention module that integrates StyleGRU-generated features with content-based features, enabling the detection of visual and temporal artifacts. We demonstrate our approach across various benchmark scenarios in deepfake detection, showing its superiority in cross-dataset and cross-manipulation scenarios. Through further analysis, we also validate the importance of using temporal changes of style latent vectors to improve the generality of deepfake video detection.
<div id='section'>Paperid: <span id='pid'>124, <a href='https://arxiv.org/pdf/2512.04175.pdf' target='_blank'>https://arxiv.org/pdf/2512.04175.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Alejandro Cobo, Roberto Valle, José Miguel Buenaposada, Luis Baumela
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.04175">Beyond Flicker: Detecting Kinematic Inconsistencies for Generalizable Deepfake Video Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generalizing deepfake detection to unseen manipulations remains a key challenge. A recent approach to tackle this issue is to train a network with pristine face images that have been manipulated with hand-crafted artifacts to extract more generalizable clues. While effective for static images, extending this to the video domain is an open issue. Existing methods model temporal artifacts as frame-to-frame instabilities, overlooking a key vulnerability: the violation of natural motion dependencies between different facial regions. In this paper, we propose a synthetic video generation method that creates training data with subtle kinematic inconsistencies. We train an autoencoder to decompose facial landmark configurations into motion bases. By manipulating these bases, we selectively break the natural correlations in facial movements and introduce these artifacts into pristine videos via face morphing. A network trained on our data learns to spot these sophisticated biomechanical flaws, achieving state-of-the-art generalization results on several popular benchmarks.
<div id='section'>Paperid: <span id='pid'>125, <a href='https://arxiv.org/pdf/2503.20084.pdf' target='_blank'>https://arxiv.org/pdf/2503.20084.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Simiao Ren, Yao Yao, Kidus Zewde, Zisheng Liang, Tsang, Ng, Ning-Yau Cheng, Xiaoou Zhan, Qinzhe Liu, Yifei Chen, Hengwei Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.20084">Can Multi-modal (reasoning) LLMs work as deepfake detectors?</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deepfake detection remains a critical challenge in the era of advanced generative models, particularly as synthetic media becomes more sophisticated. In this study, we explore the potential of state of the art multi-modal (reasoning) large language models (LLMs) for deepfake image detection such as (OpenAI O1/4o, Gemini thinking Flash 2, Deepseek Janus, Grok 3, llama 3.2, Qwen 2/2.5 VL, Mistral Pixtral, Claude 3.5/3.7 sonnet) . We benchmark 12 latest multi-modal LLMs against traditional deepfake detection methods across multiple datasets, including recently published real-world deepfake imagery. To enhance performance, we employ prompt tuning and conduct an in-depth analysis of the models' reasoning pathways to identify key contributing factors in their decision-making process. Our findings indicate that best multi-modal LLMs achieve competitive performance with promising generalization ability with zero shot, even surpass traditional deepfake detection pipelines in out-of-distribution datasets while the rest of the LLM families performs extremely disappointing with some worse than random guess. Furthermore, we found newer model version and reasoning capabilities does not contribute to performance in such niche tasks of deepfake detection while model size do help in some cases. This study highlights the potential of integrating multi-modal reasoning in future deepfake detection frameworks and provides insights into model interpretability for robustness in real-world scenarios.
<div id='section'>Paperid: <span id='pid'>126, <a href='https://arxiv.org/pdf/2407.06174.pdf' target='_blank'>https://arxiv.org/pdf/2407.06174.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hannah Lee, Changyeon Lee, Kevin Farhat, Lin Qiu, Steve Geluso, Aerin Kim, Oren Etzioni
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.06174">The Tug-of-War Between Deepfake Generation and Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multimodal generative models are rapidly evolving, leading to a surge in the generation of realistic video and audio that offers exciting possibilities but also serious risks. Deepfake videos, which can convincingly impersonate individuals, have particularly garnered attention due to their potential misuse in spreading misinformation and creating fraudulent content. This survey paper examines the dual landscape of deepfake video generation and detection, emphasizing the need for effective countermeasures against potential abuses. We provide a comprehensive overview of current deepfake generation techniques, including face swapping, reenactment, and audio-driven animation, which leverage cutting-edge technologies like GANs and diffusion models to produce highly realistic fake videos. Additionally, we analyze various detection approaches designed to differentiate authentic from altered videos, from detecting visual artifacts to deploying advanced algorithms that pinpoint inconsistencies across video and audio signals.
  The effectiveness of these detection methods heavily relies on the diversity and quality of datasets used for training and evaluation. We discuss the evolution of deepfake datasets, highlighting the importance of robust, diverse, and frequently updated collections to enhance the detection accuracy and generalizability. As deepfakes become increasingly indistinguishable from authentic content, developing advanced detection techniques that can keep pace with generation technologies is crucial. We advocate for a proactive approach in the "tug-of-war" between deepfake creators and detectors, emphasizing the need for continuous research collaboration, standardization of evaluation metrics, and the creation of comprehensive benchmarks.
<div id='section'>Paperid: <span id='pid'>127, <a href='https://arxiv.org/pdf/2511.16541.pdf' target='_blank'>https://arxiv.org/pdf/2511.16541.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jaime Álvarez Urueña, David Camacho, Javier Huertas Tato
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.16541">Supervised Contrastive Learning for Few-Shot AI-Generated Image Detection and Attribution</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The rapid advancement of generative artificial intelligence has enabled the creation of synthetic images that are increasingly indistinguishable from authentic content, posing significant challenges for digital media integrity. This problem is compounded by the accelerated release cycle of novel generative models, which renders traditional detection approaches (reliant on periodic retraining) computationally infeasible and operationally impractical. This work proposes a novel two-stage detection framework designed to address the generalization challenge inherent in synthetic image detection. The first stage employs a vision deep learning model trained via supervised contrastive learning to extract discriminative embeddings from input imagery. Critically, this model was trained on a strategically partitioned subset of available generators, with specific architectures withheld from training to rigorously ablate cross-generator generalization capabilities. The second stage utilizes a k-nearest neighbors (k-NN) classifier operating on the learned embedding space, trained in a few-shot learning paradigm incorporating limited samples from previously unseen test generators. With merely 150 images per class in the few-shot learning regime, which are easily obtainable from current generation models, the proposed framework achieves an average detection accuracy of 91.3%, representing a 5.2 percentage point improvement over existing approaches . For the source attribution task, the proposed approach obtains improvements of of 14.70% and 4.27% in AUC and OSCR respectively on an open set classification context, marking a significant advancement toward robust, scalable forensic attribution systems capable of adapting to the evolving generative AI landscape without requiring exhaustive retraining protocols.
<div id='section'>Paperid: <span id='pid'>128, <a href='https://arxiv.org/pdf/2510.19957.pdf' target='_blank'>https://arxiv.org/pdf/2510.19957.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Amir Hever, Itai Orr
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.19957">A new wave of vehicle insurance fraud fueled by generative AI</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generative AI is supercharging insurance fraud by making it easier to falsify accident evidence at scale and in rapid time. Insurance fraud is a pervasive and costly problem, amounting to tens of billions of dollars in losses each year. In the vehicle insurance sector, fraud schemes have traditionally involved staged accidents, exaggerated damage, or forged documents. The rise of generative AI, including deepfake image and video generation, has introduced new methods for committing fraud at scale. Fraudsters can now fabricate highly realistic crash photos, damage evidence, and even fake identities or documents with minimal effort, exploiting AI tools to bolster false insurance claims. Insurers have begun deploying countermeasures such as AI-based deepfake detection software and enhanced verification processes to detect and mitigate these AI-driven scams. However, current mitigation strategies face significant limitations. Detection tools can suffer from false positives and negatives, and sophisticated fraudsters continuously adapt their tactics to evade automated checks. This cat-and-mouse arms race between generative AI and detection technology, combined with resource and cost barriers for insurers, means that combating AI-enabled insurance fraud remains an ongoing challenge. In this white paper, we present UVeye layered solution for vehicle fraud, representing a major leap forward in the ability to detect, mitigate and deter this new wave of fraud.
<div id='section'>Paperid: <span id='pid'>129, <a href='https://arxiv.org/pdf/2510.17854.pdf' target='_blank'>https://arxiv.org/pdf/2510.17854.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jitendra Sharma, Arthur Carvalho, Suman Bhunia
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.17854">Provenance of AI-Generated Images: A Vector Similarity and Blockchain-based Approach</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Rapid advancement in generative AI and large language models (LLMs) has enabled the generation of highly realistic and contextually relevant digital content. LLMs such as ChatGPT with DALL-E integration and Stable Diffusion techniques can produce images that are often indistinguishable from those created by humans, which poses challenges for digital content authentication. Verifying the integrity and origin of digital data to ensure it remains unaltered and genuine is crucial to maintaining trust and legality in digital media. In this paper, we propose an embedding-based AI image detection framework that utilizes image embeddings and a vector similarity to distinguish AI-generated images from real (human-created) ones. Our methodology is built on the hypothesis that AI-generated images demonstrate closer embedding proximity to other AI-generated content, while human-created images cluster similarly within their domain. To validate this hypothesis, we developed a system that processes a diverse dataset of AI and human-generated images through five benchmark embedding models. Extensive experimentation demonstrates the robustness of our approach, and our results confirm that moderate to high perturbations minimally impact the embedding signatures, with perturbed images maintaining close similarity matches to their original versions. Our solution provides a generalizable framework for AI-generated image detection that balances accuracy with computational efficiency.
<div id='section'>Paperid: <span id='pid'>130, <a href='https://arxiv.org/pdf/2508.17877.pdf' target='_blank'>https://arxiv.org/pdf/2508.17877.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dabbrata Das, Mahshar Yahan, Md Tareq Zaman, Md Rishadul Bayesh
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.17877">Edge-Enhanced Vision Transformer Framework for Accurate AI-Generated Image Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The rapid advancement of generative models has led to a growing prevalence of highly realistic AI-generated images, posing significant challenges for digital forensics and content authentication. Conventional detection methods mainly rely on deep learning models that extract global features, which often overlook subtle structural inconsistencies and demand substantial computational resources. To address these limitations, we propose a hybrid detection framework that combines a fine-tuned Vision Transformer (ViT) with a novel edge-based image processing module. The edge-based module computes variance from edge-difference maps generated before and after smoothing, exploiting the observation that AI-generated images typically exhibit smoother textures, weaker edges, and reduced noise compared to real images. When applied as a post-processing step on ViT predictions, this module enhances sensitivity to fine-grained structural cues while maintaining computational efficiency. Extensive experiments on the CIFAKE, Artistic, and Custom Curated datasets demonstrate that the proposed framework achieves superior detection performance across all benchmarks, attaining 97.75% accuracy and a 97.77% F1-score on CIFAKE, surpassing widely adopted state-of-the-art models. These results establish the proposed method as a lightweight, interpretable, and effective solution for both still images and video frames, making it highly suitable for real-world applications in automated content verification and digital forensics.
<div id='section'>Paperid: <span id='pid'>131, <a href='https://arxiv.org/pdf/2503.18812.pdf' target='_blank'>https://arxiv.org/pdf/2503.18812.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shrikant Malviya, Neelanjan Bhowmik, Stamos Katsigiannis
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.18812">SKDU at De-Factify 4.0: Vision Transformer with Data Augmentation for AI-Generated Image Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The aim of this work is to explore the potential of pre-trained vision-language models, e.g. Vision Transformers (ViT), enhanced with advanced data augmentation strategies for the detection of AI-generated images. Our approach leverages a fine-tuned ViT model trained on the Defactify-4.0 dataset, which includes images generated by state-of-the-art models such as Stable Diffusion 2.1, Stable Diffusion XL, Stable Diffusion 3, DALL-E 3, and MidJourney. We employ perturbation techniques like flipping, rotation, Gaussian noise injection, and JPEG compression during training to improve model robustness and generalisation. The experimental results demonstrate that our ViT-based pipeline achieves state-of-the-art performance, significantly outperforming competing methods on both validation and test datasets.
<div id='section'>Paperid: <span id='pid'>132, <a href='https://arxiv.org/pdf/2306.07005.pdf' target='_blank'>https://arxiv.org/pdf/2306.07005.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ziyi Xi, Wenmin Huang, Kangkang Wei, Weiqi Luo, Peijia Zheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.07005">AI-Generated Image Detection using a Cross-Attention Enhanced Dual-Stream Network</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the rapid evolution of AI Generated Content (AIGC), forged images produced through this technology are inherently more deceptive and require less human intervention compared to traditional Computer-generated Graphics (CG). However, owing to the disparities between CG and AIGC, conventional CG detection methods tend to be inadequate in identifying AIGC-produced images. To address this issue, our research concentrates on the text-to-image generation process in AIGC. Initially, we first assemble two text-to-image databases utilizing two distinct AI systems, DALLE2 and DreamStudio. Aiming to holistically capture the inherent anomalies produced by AIGC, we develope a robust dual-stream network comprised of a residual stream and a content stream. The former employs the Spatial Rich Model (SRM) to meticulously extract various texture information from images, while the latter seeks to capture additional forged traces in low frequency, thereby extracting complementary information that the residual stream may overlook. To enhance the information exchange between these two streams, we incorporate a cross multi-head attention mechanism. Numerous comparative experiments are performed on both databases, and the results show that our detection method consistently outperforms traditional CG detection techniques across a range of image resolutions. Moreover, our method exhibits superior performance through a series of robustness tests and cross-database experiments. When applied to widely recognized traditional CG benchmarks such as SPL2018 and DsTok, our approach significantly exceeds the capabilities of other existing methods in the field of CG detection.
<div id='section'>Paperid: <span id='pid'>133, <a href='https://arxiv.org/pdf/2512.21512.pdf' target='_blank'>https://arxiv.org/pdf/2512.21512.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Md Ashik Khan, Arafat Alam Jion
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.21512">Fixed-Threshold Evaluation of a Hybrid CNN-ViT for AI-Generated Image Detection Across Photos and Art</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>AI image generators create both photorealistic images and stylized art, necessitating robust detectors that maintain performance under common post-processing transformations (JPEG compression, blur, downscaling). Existing methods optimize single metrics without addressing deployment-critical factors such as operating point selection and fixed-threshold robustness. This work addresses misleading robustness estimates by introducing a fixed-threshold evaluation protocol that holds decision thresholds, selected once on clean validation data, fixed across all post-processing transformations. Traditional methods retune thresholds per condition, artificially inflating robustness estimates and masking deployment failures. We report deployment-relevant performance at three operating points (Low-FPR, ROC-optimal, Best-F1) under systematic degradation testing using a lightweight CNN-ViT hybrid with gated fusion and optional frequency enhancement. Our evaluation exposes a statistically validated forensic-semantic spectrum: frequency-aided CNNs excel on pristine photos but collapse under compression (93.33% to 61.49%), whereas ViTs degrade minimally (92.86% to 88.36%) through robust semantic pattern recognition. Multi-seed experiments demonstrate that all architectures achieve 15% higher AUROC on artistic content (0.901-0.907) versus photorealistic images (0.747-0.759), confirming that semantic patterns provide fundamentally more reliable detection cues than forensic artifacts. Our hybrid approach achieves balanced cross-domain performance: 91.4% accuracy on tiny-genimage photos, 89.7% on AiArtData art/graphics, and 98.3% (competitive) on CIFAKE. Fixed-threshold evaluation eliminates retuning inflation, reveals genuine robustness gaps, and yields actionable deployment guidance: prefer CNNs for clean photo verification, ViTs for compressed content, and hybrids for art/graphics screening.
<div id='section'>Paperid: <span id='pid'>134, <a href='https://arxiv.org/pdf/2512.10248.pdf' target='_blank'>https://arxiv.org/pdf/2512.10248.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhuo Wang, Xiliang Liu, Ligang Sun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.10248">RobustSora: De-Watermarked Benchmark for Robust AI-Generated Video Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The proliferation of AI-generated video technologies poses challenges to information integrity. While recent benchmarks advance AIGC video detection, they overlook a critical factor: many state-of-the-art generative models embed digital watermarks in outputs, and detectors may partially rely on these patterns. To evaluate this influence, we present RobustSora, the benchmark designed to assess watermark robustness in AIGC video detection. We systematically construct a dataset of 6,500 videos comprising four types: Authentic-Clean (A-C), Authentic-Spoofed with fake watermarks (A-S), Generated-Watermarked (G-W), and Generated-DeWatermarked (G-DeW). Our benchmark introduces two evaluation tasks: Task-I tests performance on watermark-removed AI videos, while Task-II assesses false alarm rates on authentic videos with fake watermarks. Experiments with ten models spanning specialized AIGC detectors, transformer architectures, and MLLM approaches reveal performance variations of 2-8pp under watermark manipulation. Transformer-based models show consistent moderate dependency (6-8pp), while MLLMs exhibit diverse patterns (2-8pp). These findings indicate partial watermark dependency and highlight the need for watermark-aware training strategies. RobustSora provides essential tools to advance robust AIGC detection research.
<div id='section'>Paperid: <span id='pid'>135, <a href='https://arxiv.org/pdf/2512.09264.pdf' target='_blank'>https://arxiv.org/pdf/2512.09264.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaojing Chen, Dan Li, Lijun Peng, Jun YanŁetter, Zhiqing Guo, Junyang Chen, Xiao Lan, Zhongjie Ba, Yunfeng DiaoŁetter
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.09264">FBA$^2$D: Frequency-based Black-box Attack for AI-generated Image Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The prosperous development of Artificial Intelligence-Generated Content (AIGC) has brought people's anxiety about the spread of false information on social media. Designing detectors for filtering is an effective defense method, but most detectors will be compromised by adversarial samples. Currently, most studies exposing AIGC security issues assume information on model structure and data distribution. In real applications, attackers query and interfere with models that provide services in the form of application programming interfaces (APIs), which constitutes the black-box decision-based attack paradigm. However, to the best of our knowledge, decision-based attacks on AIGC detectors remain unexplored. In this study, we propose \textbf{FBA$^2$D}: a frequency-based black-box attack method for AIGC detection to fill the research gap. Motivated by frequency-domain discrepancies between generated and real images, we develop a decision-based attack that leverages the Discrete Cosine Transform (DCT) for fine-grained spectral partitioning and selects frequency bands as query subspaces, improving both query efficiency and image quality. Moreover, attacks on AIGC detectors should mitigate initialization failures, preserve image quality, and operate under strict query budgets. To address these issues, we adopt an ``adversarial example soup'' method, averaging candidates from successive surrogate iterations and using the result as the initialization to accelerate the query-based attack. The empirical study on the Synthetic LSUN dataset and GenImage dataset demonstrate the effectiveness of our prosed method. This study shows the urgency of addressing practical AIGC security problems.
<div id='section'>Paperid: <span id='pid'>136, <a href='https://arxiv.org/pdf/2507.08917.pdf' target='_blank'>https://arxiv.org/pdf/2507.08917.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Justin D. Norman, Hany Farid
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.08917">Detecting Deepfake Talking Heads from Facial Biometric Anomalies</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The combination of highly realistic voice cloning, along with visually compelling avatar, face-swap, or lip-sync deepfake video generation, makes it relatively easy to create a video of anyone saying anything. Today, such deepfake impersonations are often used to power frauds, scams, and political disinformation. We propose a novel forensic machine learning technique for the detection of deepfake video impersonations that leverages unnatural patterns in facial biometrics. We evaluate this technique across a large dataset of deepfake techniques and impersonations, as well as assess its reliability to video laundering and its generalization to previously unseen video deepfake generators.
<div id='section'>Paperid: <span id='pid'>137, <a href='https://arxiv.org/pdf/2506.21711.pdf' target='_blank'>https://arxiv.org/pdf/2506.21711.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Aryan Thakre, Omkar Nagwekar, Vedang Talekar, Aparna Santra Biswas
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.21711">CAST: Cross-Attentive Spatio-Temporal feature fusion for Deepfake detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deepfakes have emerged as a significant threat to digital media authenticity, increasing the need for advanced detection techniques that can identify subtle and time-dependent manipulations. CNNs are effective at capturing spatial artifacts, and Transformers excel at modeling temporal inconsistencies. However, many existing CNN-Transformer models process spatial and temporal features independently. In particular, attention-based methods often use separate attention mechanisms for spatial and temporal features and combine them using naive approaches like averaging, addition, or concatenation, which limits the depth of spatio-temporal interaction. To address this challenge, we propose a unified CAST model that leverages cross-attention to effectively fuse spatial and temporal features in a more integrated manner. Our approach allows temporal features to dynamically attend to relevant spatial regions, enhancing the model's ability to detect fine-grained, time-evolving artifacts such as flickering eyes or warped lips. This design enables more precise localization and deeper contextual understanding, leading to improved performance across diverse and challenging scenarios. We evaluate the performance of our model using the FaceForensics++, Celeb-DF, and DeepfakeDetection datasets in both intra- and cross-dataset settings to affirm the superiority of our approach. Our model achieves strong performance with an AUC of 99.49 percent and an accuracy of 97.57 percent in intra-dataset evaluations. In cross-dataset testing, it demonstrates impressive generalization by achieving a 93.31 percent AUC on the unseen DeepfakeDetection dataset. These results highlight the effectiveness of cross-attention-based feature fusion in enhancing the robustness of deepfake video detection.
<div id='section'>Paperid: <span id='pid'>138, <a href='https://arxiv.org/pdf/2501.13475.pdf' target='_blank'>https://arxiv.org/pdf/2501.13475.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>JiaXin Chen, Miao Hu, DengYong Zhang, Yun Song, Xin Liao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.13475">LDR-Net: A Novel Framework for AI-generated Image Detection via Localized Discrepancy Representation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the rapid advancement of generative models, the visual quality of generated images has become nearly indistinguishable from the real ones, posing challenges to content authenticity verification. Existing methods for detecting AI-generated images primarily focus on specific forgery clues, which are often tailored to particular generative models like GANs or diffusion models. These approaches struggle to generalize across architectures. Building on the observation that generative images often exhibit local anomalies, such as excessive smoothness, blurred textures, and unnatural pixel variations in small regions, we propose the localized discrepancy representation network (LDR-Net), a novel approach for detecting AI-generated images. LDR-Net captures smoothing artifacts and texture irregularities, which are common but often overlooked. It integrates two complementary modules: local gradient autocorrelation (LGA) which models local smoothing anomalies to detect smoothing anomalies, and local variation pattern (LVP) which captures unnatural regularities by modeling the complexity of image patterns. By merging LGA and LVP features, a comprehensive representation of localized discrepancies can be provided. Extensive experiments demonstrate that our LDR-Net achieves state-of-the-art performance in detecting generated images and exhibits satisfactory generalization across unseen generative models. The code will be released upon acceptance of this paper.
<div id='section'>Paperid: <span id='pid'>139, <a href='https://arxiv.org/pdf/2411.15457.pdf' target='_blank'>https://arxiv.org/pdf/2411.15457.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sukhandeep Kaur, Mubashir Buhari, Naman Khandelwal, Priyansh Tyagi, Kiran Sharma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.15457">Hindi audio-video-Deepfake (HAV-DF): A Hindi language-based Audio-video Deepfake Dataset</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deepfakes offer great potential for innovation and creativity, but they also pose significant risks to privacy, trust, and security. With a vast Hindi-speaking population, India is particularly vulnerable to deepfake-driven misinformation campaigns. Fake videos or speeches in Hindi can have an enormous impact on rural and semi-urban communities, where digital literacy tends to be lower and people are more inclined to trust video content. The development of effective frameworks and detection tools to combat deepfake misuse requires high-quality, diverse, and extensive datasets. The existing popular datasets like FF-DF (FaceForensics++), and DFDC (DeepFake Detection Challenge) are based on English language.. Hence, this paper aims to create a first novel Hindi deep fake dataset, named ``Hindi audio-video-Deepfake'' (HAV-DF). The dataset has been generated using the faceswap, lipsyn and voice cloning methods. This multi-step process allows us to create a rich, varied dataset that captures the nuances of Hindi speech and facial expressions, providing a robust foundation for training and evaluating deepfake detection models in a Hindi language context. It is unique of its kind as all of the previous datasets contain either deepfake videos or synthesized audio. This type of deepfake dataset can be used for training a detector for both deepfake video and audio datasets. Notably, the newly introduced HAV-DF dataset demonstrates lower detection accuracy's across existing detection methods like Headpose, Xception-c40, etc. Compared to other well-known datasets FF-DF, and DFDC. This trend suggests that the HAV-DF dataset presents deeper challenges to detect, possibly due to its focus on Hindi language content and diverse manipulation techniques. The HAV-DF dataset fills the gap in Hindi-specific deepfake datasets, aiding multilingual deepfake detection development.
<div id='section'>Paperid: <span id='pid'>140, <a href='https://arxiv.org/pdf/2408.09371.pdf' target='_blank'>https://arxiv.org/pdf/2408.09371.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Taharim Rahman Anon, Jakaria Islam Emon
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.09371">Detecting the Undetectable: Combining Kolmogorov-Arnold Networks and MLP for AI-Generated Image Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As artificial intelligence progresses, the task of distinguishing between real and AI-generated images is increasingly complicated by sophisticated generative models. This paper presents a novel detection framework adept at robustly identifying images produced by cutting-edge generative AI models, such as DALL-E 3, MidJourney, and Stable Diffusion 3. We introduce a comprehensive dataset, tailored to include images from these advanced generators, which serves as the foundation for extensive evaluation. we propose a classification system that integrates semantic image embeddings with a traditional Multilayer Perceptron (MLP). This baseline system is designed to effectively differentiate between real and AI-generated images under various challenging conditions. Enhancing this approach, we introduce a hybrid architecture that combines Kolmogorov-Arnold Networks (KAN) with the MLP. This hybrid model leverages the adaptive, high-resolution feature transformation capabilities of KAN, enabling our system to capture and analyze complex patterns in AI-generated images that are typically overlooked by conventional models. In out-of-distribution testing, our proposed model consistently outperformed the standard MLP across three out of distribution test datasets, demonstrating superior performance and robustness in classifying real images from AI-generated images with impressive F1 scores.
<div id='section'>Paperid: <span id='pid'>141, <a href='https://arxiv.org/pdf/2406.08758.pdf' target='_blank'>https://arxiv.org/pdf/2406.08758.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Abdel Rahman Alsabbagh, Omar Al-Kadi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.08758">Comparative Analysis of Deep Convolutional Neural Networks for Detecting Medical Image Deepfakes</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generative Adversarial Networks (GANs) have exhibited noteworthy advancements across various applications, including medical imaging. While numerous state-of-the-art Deep Convolutional Neural Network (DCNN) architectures are renowned for their proficient feature extraction, this paper investigates their efficacy in the context of medical image deepfake detection. The primary objective is to effectively distinguish real from tampered or manipulated medical images by employing a comprehensive evaluation of 13 state-of-the-art DCNNs. Performance is assessed across diverse evaluation metrics, encompassing considerations of time efficiency and computational resource requirements. Our findings reveal that ResNet50V2 excels in precision and specificity, whereas DenseNet169 is distinguished by its accuracy, recall, and F1-score. We investigate the specific scenarios in which one model would be more favorable than another. Additionally, MobileNetV3Large offers competitive performance, emerging as the swiftest among the considered DCNN models while maintaining a relatively small parameter count. We also assess the latent space separability quality across the examined DCNNs, showing superiority in both the DenseNet and EfficientNet model families and entailing a higher understanding of medical image deepfakes. The experimental analysis in this research contributes valuable insights to the field of deepfake image detection in the medical imaging domain.
<div id='section'>Paperid: <span id='pid'>142, <a href='https://arxiv.org/pdf/2404.12841.pdf' target='_blank'>https://arxiv.org/pdf/2404.12841.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Gazi Hasin Ishrak, Zalish Mahmud, MD. Zami Al Zunaed Farabe, Tahera Khanom Tinni, Tanzim Reza, Mohammad Zavid Parvez
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.12841">Explainable Deepfake Video Detection using Convolutional Neural Network and CapsuleNet</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deepfake technology, derived from deep learning, seamlessly inserts individuals into digital media, irrespective of their actual participation. Its foundation lies in machine learning and Artificial Intelligence (AI). Initially, deepfakes served research, industry, and entertainment. While the concept has existed for decades, recent advancements render deepfakes nearly indistinguishable from reality. Accessibility has soared, empowering even novices to create convincing deepfakes. However, this accessibility raises security concerns.The primary deepfake creation algorithm, GAN (Generative Adversarial Network), employs machine learning to craft realistic images or videos. Our objective is to utilize CNN (Convolutional Neural Network) and CapsuleNet with LSTM to differentiate between deepfake-generated frames and originals. Furthermore, we aim to elucidate our model's decision-making process through Explainable AI, fostering transparent human-AI relationships and offering practical examples for real-life scenarios.
<div id='section'>Paperid: <span id='pid'>143, <a href='https://arxiv.org/pdf/2403.17608.pdf' target='_blank'>https://arxiv.org/pdf/2403.17608.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Patrick Grommelt, Louis Weiss, Franz-Josef Pfreundt, Janis Keuper
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.17608">Fake or JPEG? Revealing Common Biases in Generated Image Detection Datasets</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The widespread adoption of generative image models has highlighted the urgent need to detect artificial content, which is a crucial step in combating widespread manipulation and misinformation. Consequently, numerous detectors and associated datasets have emerged. However, many of these datasets inadvertently introduce undesirable biases, thereby impacting the effectiveness and evaluation of detectors. In this paper, we emphasize that many datasets for AI-generated image detection contain biases related to JPEG compression and image size. Using the GenImage dataset, we demonstrate that detectors indeed learn from these undesired factors. Furthermore, we show that removing the named biases substantially increases robustness to JPEG compression and significantly alters the cross-generator performance of evaluated detectors. Specifically, it leads to more than 11 percentage points increase in cross-generator performance for ResNet50 and Swin-T detectors on the GenImage dataset, achieving state-of-the-art results.
  We provide the dataset and source codes of this paper on the anonymous website: https://www.unbiased-genimage.org
<div id='section'>Paperid: <span id='pid'>144, <a href='https://arxiv.org/pdf/2307.14273.pdf' target='_blank'>https://arxiv.org/pdf/2307.14273.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Roa'a Al-Emaryeen, Sara Al-Nahhas, Fatima Himour, Waleed Mahafza, Omar Al-Kadi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.14273">Deepfake Image Generation for Improved Brain Tumor Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As the world progresses in technology and health, awareness of disease by revealing asymptomatic signs improves. It is important to detect and treat tumors in early stage as it can be life-threatening. Computer-aided technologies are used to overcome lingering limitations facing disease diagnosis, while brain tumor segmentation remains a difficult process, especially when multi-modality data is involved. This is mainly attributed to ineffective training due to lack of data and corresponding labelling. This work investigates the feasibility of employing deep-fake image generation for effective brain tumor segmentation. To this end, a Generative Adversarial Network was used for image-to-image translation for increasing dataset size, followed by image segmentation using a U-Net-based convolutional neural network trained with deepfake images. Performance of the proposed approach is compared with ground truth of four publicly available datasets. Results show improved performance in terms of image segmentation quality metrics, and could potentially assist when training with limited data.
<div id='section'>Paperid: <span id='pid'>145, <a href='https://arxiv.org/pdf/2505.10664.pdf' target='_blank'>https://arxiv.org/pdf/2505.10664.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ziyang Ou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.10664">CLIP Embeddings for AI-Generated Image Detection: A Few-Shot Study with Lightweight Classifier</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Verifying the authenticity of AI-generated images presents a growing challenge on social media platforms these days. While vision-language models (VLMs) like CLIP outdo in multimodal representation, their capacity for AI-generated image classification is underexplored due to the absence of such labels during the pre-training process. This work investigates whether CLIP embeddings inherently contain information indicative of AI generation. A proposed pipeline extracts visual embeddings using a frozen CLIP model, feeds its embeddings to lightweight networks, and fine-tunes only the final classifier. Experiments on the public CIFAKE benchmark show the performance reaches 95% accuracy without language reasoning. Few-shot adaptation to curated custom with 20% of the data results in performance to 85%. A closed-source baseline (Gemini-2.0) has the best zero-shot accuracy yet fails on specific styles. Notably, some specific image types, such as wide-angle photographs and oil paintings, pose significant challenges to classification. These results indicate previously unexplored difficulties in classifying certain types of AI-generated images, revealing new and more specific questions in this domain that are worth further investigation.
